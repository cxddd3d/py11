{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看FashionMNIST原始数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:03.878724Z",
     "start_time": "2025-07-02T02:02:59.128019Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from wangdao_deeplearning_train import EarlyStopping, ModelSaver,train_classification_model,plot_learning_curves\n",
    "from wangdao_deeplearning_train import evaluate_classification_model as evaluate_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据并处理为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.185109Z",
     "start_time": "2025-07-02T02:03:03.878724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 45000\n",
      "验证集大小: 5000\n"
     ]
    }
   ],
   "source": [
    "# 加载CIFAR-10数据集\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 定义CIFAR-10数据集类\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, img_dir, labels_df, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.labels_df = labels_df\n",
    "        self.img_names = self.labels_df.iloc[:, 0].values.astype(str)  # 第一列是图片名称，确保为字符串类型\n",
    "        \n",
    "        # 类别名称字典，使用字典可以提高查找速度\n",
    "        self.class_names_dict = {'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, \n",
    "                                 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
    "        # 将文本标签转换为数字ID\n",
    "        self.labels = [self.class_names_dict[label] for label in self.labels_df.iloc[:, 1].values]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx] + '.png') #图片路径\n",
    "        image = Image.open(img_path) #打开图片\n",
    "        label = self.labels[idx] #标签\n",
    "        \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image) #转换为张量\n",
    "            \n",
    "        return image_tensor, label\n",
    "\n",
    "# 读取标签文件\n",
    "img_dir = r\"D:\\cifar-10\\train\\train\"\n",
    "labels_file = r\"D:\\cifar-10\\trainLabels.csv\"\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "# 划分数据集\n",
    "train_size = 45000\n",
    "val_size = 5000\n",
    "train_df = labels_df.iloc[:train_size]\n",
    "val_df = labels_df.iloc[train_size:]\n",
    "\n",
    "# 定义训练集数据预处理（包含图像增强）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(40), #随机旋转\n",
    "    transforms.RandomHorizontalFlip(),  #随机水平翻转\n",
    "    transforms.Normalize((0.4917, 0.4823, 0.4467), (0.2024, 0.1995, 0.2010))\n",
    "])\n",
    "\n",
    "# 定义验证集数据预处理（不做图像增强）\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4917, 0.4823, 0.4467), (0.2024, 0.1995, 0.2010))\n",
    "])\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = CIFAR10Dataset(img_dir=img_dir, labels_df=train_df, transform=train_transform)\n",
    "val_dataset = CIFAR10Dataset(img_dir=img_dir, labels_df=val_df, transform=val_transform)\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 查看数据集基本信息\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"验证集大小: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img.mean(dim=(1, 2)) #dim=(1, 2)表示在通道维度上求平均\n",
    "        std += img.std(dim=(1, 2))  #dim=(1, 2)表示在通道维度上求标准差\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "# cal_mean_std(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 把数据集划分为训练集45000和验证集5000，并给DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.187120Z",
     "start_time": "2025-07-02T02:03:04.187120Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True #打乱数据集，每次迭代时，数据集的顺序都会被打乱\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.188247Z",
     "start_time": "2025-07-02T02:03:04.188247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "#理解每个接口的方法，单独写例子\n",
    "import torch.nn as nn\n",
    "m=nn.BatchNorm1d(100)\n",
    "x=torch.randn(20,100)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.189256Z",
     "start_time": "2025-07-02T02:03:04.189256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一组卷积层 - 使用Sequential组织\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), #BatchNorm2d 用于处理四维张量\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 第二组卷积层 - 使用Sequential组织\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 第三组卷积层 - 使用Sequential组织\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 全连接层 - 使用Sequential组织\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"使用 xavier 均匀分布来初始化卷积层和全连接层的权重\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向传播使用Sequential定义的块\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        \n",
    "        # 展平\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 分类器\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.190256Z",
     "start_time": "2025-07-02T02:03:04.190256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批次图像形状: torch.Size([64, 3, 32, 32])\n",
      "批次标签形状: torch.Size([64])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# 从train_loader获取第一个批次的数据\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 查看批次数据的形状\n",
    "print(\"批次图像形状:\", images.shape)\n",
    "print(\"批次标签形状:\", labels.shape)\n",
    "\n",
    "\n",
    "print('-'*100)\n",
    "# 进行前向传播\n",
    "with torch.no_grad():  # 不需要计算梯度\n",
    "    outputs = model(images)\n",
    "    \n",
    "\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.191255Z",
     "start_time": "2025-07-02T02:03:04.191255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要求梯度的参数总量: 12979850\n",
      "模型总参数量: 12979850\n",
      "\n",
      "各层参数量明细:\n",
      "conv_block1.0.weight: 3456 参数\n",
      "conv_block1.0.bias: 128 参数\n",
      "conv_block1.1.weight: 128 参数\n",
      "conv_block1.1.bias: 128 参数\n",
      "conv_block1.3.weight: 147456 参数\n",
      "conv_block1.3.bias: 128 参数\n",
      "conv_block1.4.weight: 128 参数\n",
      "conv_block1.4.bias: 128 参数\n",
      "conv_block2.0.weight: 294912 参数\n",
      "conv_block2.0.bias: 256 参数\n",
      "conv_block2.1.weight: 256 参数\n",
      "conv_block2.1.bias: 256 参数\n",
      "conv_block2.3.weight: 589824 参数\n",
      "conv_block2.3.bias: 256 参数\n",
      "conv_block2.4.weight: 256 参数\n",
      "conv_block2.4.bias: 256 参数\n",
      "conv_block3.0.weight: 1179648 参数\n",
      "conv_block3.0.bias: 512 参数\n",
      "conv_block3.1.weight: 512 参数\n",
      "conv_block3.1.bias: 512 参数\n",
      "conv_block3.3.weight: 2359296 参数\n",
      "conv_block3.3.bias: 512 参数\n",
      "conv_block3.4.weight: 512 参数\n",
      "conv_block3.4.bias: 512 参数\n",
      "classifier.0.weight: 8388608 参数\n",
      "classifier.0.bias: 1024 参数\n",
      "classifier.2.weight: 10240 参数\n",
      "classifier.2.bias: 10 参数\n"
     ]
    }
   ],
   "source": [
    "# 计算模型的总参数量\n",
    "# 统计需要求梯度的参数总量\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"需要求梯度的参数总量: {total_params}\")\n",
    "\n",
    "# 统计所有参数总量\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"模型总参数量: {all_params}\")\n",
    "\n",
    "# 查看每层参数量明细\n",
    "print(\"\\n各层参数量明细:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel()} 参数\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*3*3*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.195758Z",
     "start_time": "2025-07-02T02:03:04.194767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_block1.0.weight',\n",
       "              tensor([[[[-0.0361, -0.0497, -0.0056],\n",
       "                        [-0.0465,  0.0258, -0.0437],\n",
       "                        [ 0.0674, -0.0332,  0.0155]],\n",
       "              \n",
       "                       [[ 0.0527,  0.0423,  0.0329],\n",
       "                        [ 0.0355, -0.0624,  0.0385],\n",
       "                        [ 0.0679,  0.0537, -0.0115]],\n",
       "              \n",
       "                       [[ 0.0533,  0.0504, -0.0355],\n",
       "                        [ 0.0393,  0.0349, -0.0587],\n",
       "                        [-0.0083, -0.0600,  0.0376]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0005, -0.0500,  0.0654],\n",
       "                        [-0.0642,  0.0389,  0.0138],\n",
       "                        [ 0.0567,  0.0325,  0.0399]],\n",
       "              \n",
       "                       [[-0.0271,  0.0378,  0.0650],\n",
       "                        [-0.0653, -0.0702, -0.0525],\n",
       "                        [-0.0579, -0.0233, -0.0118]],\n",
       "              \n",
       "                       [[-0.0613, -0.0157,  0.0604],\n",
       "                        [-0.0127,  0.0338, -0.0501],\n",
       "                        [-0.0201,  0.0278, -0.0623]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0361, -0.0141,  0.0310],\n",
       "                        [-0.0315,  0.0421, -0.0253],\n",
       "                        [-0.0242, -0.0672, -0.0226]],\n",
       "              \n",
       "                       [[-0.0401, -0.0538,  0.0284],\n",
       "                        [ 0.0676, -0.0094, -0.0539],\n",
       "                        [ 0.0215, -0.0490, -0.0089]],\n",
       "              \n",
       "                       [[-0.0427, -0.0062,  0.0624],\n",
       "                        [-0.0086,  0.0035, -0.0404],\n",
       "                        [ 0.0321, -0.0328, -0.0562]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0088,  0.0626, -0.0019],\n",
       "                        [ 0.0391, -0.0191, -0.0168],\n",
       "                        [ 0.0282,  0.0037,  0.0155]],\n",
       "              \n",
       "                       [[-0.0059,  0.0227, -0.0669],\n",
       "                        [-0.0518, -0.0364, -0.0236],\n",
       "                        [ 0.0371, -0.0087,  0.0411]],\n",
       "              \n",
       "                       [[ 0.0381, -0.0062,  0.0112],\n",
       "                        [-0.0361, -0.0370, -0.0086],\n",
       "                        [-0.0405,  0.0653, -0.0543]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0157,  0.0506,  0.0492],\n",
       "                        [-0.0620, -0.0130, -0.0413],\n",
       "                        [ 0.0490, -0.0585, -0.0696]],\n",
       "              \n",
       "                       [[-0.0259,  0.0217, -0.0255],\n",
       "                        [-0.0006, -0.0374,  0.0658],\n",
       "                        [-0.0005,  0.0653, -0.0622]],\n",
       "              \n",
       "                       [[-0.0452,  0.0065,  0.0335],\n",
       "                        [ 0.0059,  0.0002, -0.0060],\n",
       "                        [ 0.0352, -0.0013,  0.0562]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0542, -0.0432,  0.0193],\n",
       "                        [-0.0167, -0.0537,  0.0512],\n",
       "                        [-0.0116, -0.0075, -0.0492]],\n",
       "              \n",
       "                       [[ 0.0163,  0.0333,  0.0036],\n",
       "                        [ 0.0534,  0.0162, -0.0636],\n",
       "                        [ 0.0129, -0.0614,  0.0339]],\n",
       "              \n",
       "                       [[ 0.0302, -0.0116,  0.0047],\n",
       "                        [-0.0538, -0.0495, -0.0617],\n",
       "                        [-0.0367,  0.0230,  0.0175]]]])),\n",
       "             ('conv_block1.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block1.1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('conv_block1.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block1.1.running_mean',\n",
       "              tensor([-0.0075,  0.0075,  0.0130,  0.0002,  0.0035, -0.0043,  0.0139, -0.0015,\n",
       "                      -0.0127,  0.0033,  0.0063,  0.0171, -0.0019,  0.0037, -0.0031,  0.0142,\n",
       "                      -0.0104, -0.0144, -0.0092,  0.0013,  0.0001,  0.0034, -0.0173,  0.0096,\n",
       "                      -0.0003,  0.0003,  0.0049,  0.0082,  0.0036,  0.0152,  0.0053,  0.0002,\n",
       "                       0.0105, -0.0036, -0.0075,  0.0185,  0.0225, -0.0108,  0.0056,  0.0103,\n",
       "                      -0.0045,  0.0072, -0.0082,  0.0011,  0.0025, -0.0061, -0.0061, -0.0029,\n",
       "                       0.0064, -0.0115,  0.0037, -0.0064, -0.0008,  0.0068,  0.0148, -0.0054,\n",
       "                       0.0011,  0.0028, -0.0171, -0.0151,  0.0217, -0.0112, -0.0003,  0.0043,\n",
       "                       0.0153, -0.0062,  0.0140, -0.0078, -0.0054,  0.0032, -0.0027,  0.0063,\n",
       "                      -0.0036, -0.0023,  0.0199, -0.0121,  0.0052,  0.0019,  0.0171, -0.0084,\n",
       "                       0.0109,  0.0095,  0.0007,  0.0037,  0.0073,  0.0089, -0.0006, -0.0039,\n",
       "                       0.0016, -0.0031, -0.0027,  0.0002, -0.0014, -0.0061,  0.0112, -0.0040,\n",
       "                      -0.0022, -0.0121, -0.0075,  0.0041,  0.0045,  0.0113,  0.0017,  0.0029,\n",
       "                       0.0004, -0.0064,  0.0038, -0.0161,  0.0030,  0.0004, -0.0042,  0.0065,\n",
       "                       0.0046,  0.0065, -0.0016, -0.0109, -0.0101,  0.0242, -0.0148, -0.0102,\n",
       "                       0.0049, -0.0101,  0.0008,  0.0043,  0.0003,  0.0028,  0.0010,  0.0103])),\n",
       "             ('conv_block1.1.running_var',\n",
       "              tensor([0.9069, 0.9085, 0.9155, 0.9038, 0.9039, 0.9022, 0.9180, 0.9018, 0.9147,\n",
       "                      0.9029, 0.9055, 0.9249, 0.9050, 0.9040, 0.9013, 0.9193, 0.9113, 0.9195,\n",
       "                      0.9085, 0.9027, 0.9027, 0.9023, 0.9280, 0.9101, 0.9009, 0.9016, 0.9034,\n",
       "                      0.9070, 0.9033, 0.9206, 0.9080, 0.9009, 0.9132, 0.9022, 0.9105, 0.9296,\n",
       "                      0.9431, 0.9112, 0.9049, 0.9108, 0.9055, 0.9056, 0.9072, 0.9012, 0.9026,\n",
       "                      0.9066, 0.9043, 0.9025, 0.9054, 0.9133, 0.9021, 0.9082, 0.9020, 0.9053,\n",
       "                      0.9184, 0.9056, 0.9006, 0.9020, 0.9252, 0.9223, 0.9402, 0.9125, 0.9016,\n",
       "                      0.9045, 0.9206, 0.9047, 0.9177, 0.9080, 0.9042, 0.9038, 0.9031, 0.9044,\n",
       "                      0.9038, 0.9021, 0.9366, 0.9142, 0.9077, 0.9035, 0.9278, 0.9069, 0.9128,\n",
       "                      0.9110, 0.9010, 0.9030, 0.9080, 0.9099, 0.9050, 0.9031, 0.9026, 0.9033,\n",
       "                      0.9022, 0.9029, 0.9018, 0.9057, 0.9116, 0.9031, 0.9022, 0.9137, 0.9068,\n",
       "                      0.9044, 0.9037, 0.9143, 0.9030, 0.9029, 0.9019, 0.9061, 0.9032, 0.9242,\n",
       "                      0.9019, 0.9030, 0.9050, 0.9047, 0.9046, 0.9055, 0.9018, 0.9125, 0.9104,\n",
       "                      0.9500, 0.9184, 0.9104, 0.9034, 0.9102, 0.9023, 0.9035, 0.9034, 0.9020,\n",
       "                      0.9017, 0.9094])),\n",
       "             ('conv_block1.1.num_batches_tracked', tensor(1)),\n",
       "             ('conv_block1.3.weight',\n",
       "              tensor([[[[ 0.0170,  0.0440,  0.0400],\n",
       "                        [-0.0238, -0.0435, -0.0268],\n",
       "                        [-0.0216,  0.0348, -0.0345]],\n",
       "              \n",
       "                       [[-0.0398, -0.0059,  0.0475],\n",
       "                        [-0.0475,  0.0193,  0.0427],\n",
       "                        [-0.0193, -0.0298, -0.0051]],\n",
       "              \n",
       "                       [[ 0.0469,  0.0358, -0.0145],\n",
       "                        [-0.0268, -0.0094, -0.0043],\n",
       "                        [-0.0297,  0.0360, -0.0415]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0231,  0.0059,  0.0470],\n",
       "                        [-0.0238, -0.0477, -0.0300],\n",
       "                        [-0.0032, -0.0008,  0.0384]],\n",
       "              \n",
       "                       [[-0.0425, -0.0189,  0.0353],\n",
       "                        [-0.0437, -0.0035, -0.0399],\n",
       "                        [-0.0129,  0.0412, -0.0388]],\n",
       "              \n",
       "                       [[-0.0072,  0.0257,  0.0003],\n",
       "                        [-0.0259,  0.0283, -0.0244],\n",
       "                        [ 0.0201,  0.0028, -0.0006]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0280,  0.0085, -0.0079],\n",
       "                        [ 0.0486,  0.0403, -0.0191],\n",
       "                        [-0.0297,  0.0481, -0.0313]],\n",
       "              \n",
       "                       [[-0.0362,  0.0440,  0.0322],\n",
       "                        [ 0.0442, -0.0369, -0.0497],\n",
       "                        [ 0.0480, -0.0400,  0.0250]],\n",
       "              \n",
       "                       [[-0.0495, -0.0354, -0.0451],\n",
       "                        [-0.0480, -0.0200,  0.0405],\n",
       "                        [ 0.0149, -0.0464, -0.0406]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0062,  0.0413, -0.0210],\n",
       "                        [-0.0384,  0.0340,  0.0045],\n",
       "                        [ 0.0500,  0.0372,  0.0499]],\n",
       "              \n",
       "                       [[-0.0217,  0.0218,  0.0258],\n",
       "                        [-0.0397,  0.0177,  0.0334],\n",
       "                        [-0.0259,  0.0438,  0.0433]],\n",
       "              \n",
       "                       [[-0.0136,  0.0313,  0.0131],\n",
       "                        [-0.0367, -0.0003,  0.0438],\n",
       "                        [-0.0212, -0.0004,  0.0088]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0102, -0.0301, -0.0215],\n",
       "                        [-0.0132, -0.0509,  0.0275],\n",
       "                        [-0.0462, -0.0217, -0.0454]],\n",
       "              \n",
       "                       [[-0.0296,  0.0447,  0.0215],\n",
       "                        [ 0.0038, -0.0132, -0.0192],\n",
       "                        [ 0.0118,  0.0470, -0.0400]],\n",
       "              \n",
       "                       [[ 0.0505, -0.0284, -0.0318],\n",
       "                        [-0.0349,  0.0503, -0.0417],\n",
       "                        [-0.0121, -0.0185,  0.0282]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0344, -0.0037, -0.0300],\n",
       "                        [ 0.0378, -0.0319,  0.0477],\n",
       "                        [ 0.0354, -0.0285, -0.0478]],\n",
       "              \n",
       "                       [[ 0.0191, -0.0190,  0.0461],\n",
       "                        [ 0.0456, -0.0108, -0.0325],\n",
       "                        [-0.0023, -0.0118,  0.0139]],\n",
       "              \n",
       "                       [[ 0.0486, -0.0380, -0.0467],\n",
       "                        [-0.0170, -0.0037, -0.0275],\n",
       "                        [-0.0225,  0.0060,  0.0039]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0199,  0.0427, -0.0082],\n",
       "                        [-0.0096, -0.0342,  0.0425],\n",
       "                        [-0.0051, -0.0046, -0.0024]],\n",
       "              \n",
       "                       [[ 0.0344,  0.0399, -0.0411],\n",
       "                        [-0.0035,  0.0128,  0.0333],\n",
       "                        [ 0.0338,  0.0104,  0.0471]],\n",
       "              \n",
       "                       [[ 0.0247, -0.0310,  0.0215],\n",
       "                        [-0.0134,  0.0143,  0.0033],\n",
       "                        [ 0.0208, -0.0276, -0.0158]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0036, -0.0152, -0.0288],\n",
       "                        [ 0.0391, -0.0388,  0.0346],\n",
       "                        [ 0.0457, -0.0183, -0.0070]],\n",
       "              \n",
       "                       [[ 0.0414,  0.0104,  0.0253],\n",
       "                        [ 0.0426,  0.0095, -0.0424],\n",
       "                        [ 0.0349,  0.0285, -0.0488]],\n",
       "              \n",
       "                       [[-0.0244,  0.0356, -0.0190],\n",
       "                        [ 0.0281,  0.0027, -0.0314],\n",
       "                        [ 0.0463, -0.0381,  0.0234]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0175, -0.0466, -0.0282],\n",
       "                        [ 0.0124, -0.0496, -0.0077],\n",
       "                        [ 0.0217, -0.0322, -0.0300]],\n",
       "              \n",
       "                       [[ 0.0392,  0.0226,  0.0469],\n",
       "                        [ 0.0039,  0.0189, -0.0215],\n",
       "                        [-0.0028, -0.0366, -0.0464]],\n",
       "              \n",
       "                       [[ 0.0354,  0.0361,  0.0158],\n",
       "                        [ 0.0244, -0.0154,  0.0048],\n",
       "                        [ 0.0118,  0.0162,  0.0197]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0439,  0.0322, -0.0008],\n",
       "                        [-0.0058,  0.0208, -0.0398],\n",
       "                        [-0.0253,  0.0094, -0.0012]],\n",
       "              \n",
       "                       [[ 0.0097,  0.0045,  0.0073],\n",
       "                        [-0.0375, -0.0474,  0.0217],\n",
       "                        [-0.0332, -0.0114, -0.0417]],\n",
       "              \n",
       "                       [[ 0.0128,  0.0135, -0.0038],\n",
       "                        [ 0.0249, -0.0058, -0.0257],\n",
       "                        [ 0.0178, -0.0072,  0.0336]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0369, -0.0470, -0.0019],\n",
       "                        [-0.0236,  0.0375, -0.0138],\n",
       "                        [-0.0135, -0.0128,  0.0076]],\n",
       "              \n",
       "                       [[ 0.0243,  0.0397,  0.0085],\n",
       "                        [-0.0145, -0.0491, -0.0016],\n",
       "                        [ 0.0115,  0.0288,  0.0400]],\n",
       "              \n",
       "                       [[ 0.0327, -0.0302,  0.0228],\n",
       "                        [-0.0330, -0.0248, -0.0104],\n",
       "                        [-0.0328,  0.0073, -0.0013]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0351, -0.0081, -0.0393],\n",
       "                        [-0.0226,  0.0454,  0.0488],\n",
       "                        [-0.0241, -0.0121, -0.0110]],\n",
       "              \n",
       "                       [[ 0.0462,  0.0126, -0.0264],\n",
       "                        [ 0.0228,  0.0293,  0.0436],\n",
       "                        [ 0.0469, -0.0482,  0.0152]],\n",
       "              \n",
       "                       [[ 0.0078,  0.0419,  0.0158],\n",
       "                        [-0.0300,  0.0123,  0.0336],\n",
       "                        [-0.0371,  0.0299, -0.0088]]]])),\n",
       "             ('conv_block1.3.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block1.4.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1.])),\n",
       "             ('conv_block1.4.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block1.4.running_mean',\n",
       "              tensor([ 0.0418, -0.0294,  0.0211, -0.0833, -0.0073,  0.0471,  0.0869, -0.0421,\n",
       "                      -0.0121, -0.0563,  0.0065, -0.0184, -0.0246,  0.0465, -0.0653, -0.0277,\n",
       "                      -0.0282, -0.0261,  0.0049, -0.0377,  0.1062, -0.0293,  0.0174,  0.0429,\n",
       "                       0.0034,  0.0101,  0.0181, -0.0248, -0.0075,  0.0296, -0.0249,  0.0767,\n",
       "                       0.0192,  0.0506, -0.0567,  0.0014, -0.0119,  0.0156, -0.0049, -0.0519,\n",
       "                       0.0238, -0.0488, -0.0009,  0.0573, -0.0161,  0.0070,  0.0063,  0.0029,\n",
       "                      -0.0283, -0.0086, -0.0259, -0.0580,  0.0636,  0.0133, -0.0296, -0.0179,\n",
       "                       0.0678, -0.0355,  0.0457, -0.0005, -0.0125, -0.0192, -0.0166, -0.0435,\n",
       "                      -0.0780, -0.0099,  0.0136,  0.0449,  0.0074,  0.0008,  0.0417, -0.0749,\n",
       "                      -0.0009, -0.0129,  0.0407, -0.0002,  0.0136, -0.0096,  0.0537,  0.0501,\n",
       "                       0.0451, -0.0329,  0.0321, -0.0258,  0.0227, -0.0232,  0.0334, -0.0363,\n",
       "                       0.0578, -0.0171, -0.0715,  0.0475,  0.0012,  0.0814,  0.0397, -0.0559,\n",
       "                       0.0454, -0.0536,  0.0795, -0.0212, -0.0459,  0.0087,  0.0428,  0.0336,\n",
       "                       0.0094,  0.0142,  0.0340,  0.0059, -0.0251, -0.0454,  0.0119,  0.0223,\n",
       "                      -0.0594,  0.0166, -0.0033,  0.0222,  0.0329,  0.0112,  0.0108,  0.0201,\n",
       "                      -0.0254, -0.0155,  0.0025, -0.0540, -0.0273,  0.0090, -0.0083, -0.0166])),\n",
       "             ('conv_block1.4.running_var',\n",
       "              tensor([0.9761, 0.9223, 0.9186, 0.9286, 0.9257, 0.9272, 0.9412, 0.9828, 0.9543,\n",
       "                      0.9477, 0.9460, 0.9275, 0.9182, 0.9216, 0.9280, 0.9260, 0.9273, 0.9244,\n",
       "                      0.9515, 0.9308, 0.9637, 0.9219, 0.9103, 0.9270, 0.9178, 0.9171, 0.9309,\n",
       "                      0.9165, 0.9374, 0.9199, 0.9197, 0.9752, 0.9175, 0.9410, 0.9491, 0.9240,\n",
       "                      0.9147, 0.9227, 0.9265, 0.9295, 0.9483, 0.9235, 0.9236, 0.9144, 0.9207,\n",
       "                      0.9461, 0.9303, 0.9319, 0.9181, 0.9395, 0.9397, 0.9346, 0.9308, 0.9792,\n",
       "                      0.9365, 0.9292, 0.9443, 0.9262, 0.9275, 0.9249, 0.9735, 0.9299, 0.9248,\n",
       "                      0.9256, 0.9699, 0.9273, 0.9293, 0.9285, 0.9232, 0.9338, 0.9505, 0.9356,\n",
       "                      0.9445, 0.9289, 0.9209, 0.9266, 0.9349, 0.9208, 0.9306, 0.9574, 0.9597,\n",
       "                      1.0156, 0.9276, 0.9407, 0.9388, 0.9267, 0.9533, 0.9427, 0.9241, 0.9211,\n",
       "                      0.9356, 1.0115, 0.9275, 0.9509, 0.9204, 0.9176, 0.9358, 0.9367, 0.9279,\n",
       "                      0.9898, 0.9252, 0.9201, 0.9242, 0.9293, 0.9149, 0.9171, 0.9242, 0.9180,\n",
       "                      0.9153, 0.9278, 0.9292, 1.0100, 0.9519, 0.9332, 0.9275, 0.9228, 0.9289,\n",
       "                      0.9149, 0.9277, 0.9240, 0.9372, 0.9237, 0.9197, 0.9251, 0.9247, 0.9335,\n",
       "                      0.9506, 0.9173])),\n",
       "             ('conv_block1.4.num_batches_tracked', tensor(1)),\n",
       "             ('conv_block2.0.weight',\n",
       "              tensor([[[[-5.6740e-03,  2.1645e-02, -1.8886e-02],\n",
       "                        [-4.1285e-02, -2.0296e-02, -3.3992e-02],\n",
       "                        [ 1.3456e-02, -2.2523e-02, -8.4815e-03]],\n",
       "              \n",
       "                       [[ 2.2650e-02,  3.1264e-02, -3.5604e-03],\n",
       "                        [-2.1208e-02, -8.1595e-03,  1.0369e-02],\n",
       "                        [ 8.7335e-03, -6.1623e-03, -3.7842e-02]],\n",
       "              \n",
       "                       [[-3.7443e-02, -2.9840e-02, -1.7784e-02],\n",
       "                        [ 1.8070e-02, -6.7565e-03,  9.2547e-03],\n",
       "                        [-3.4981e-02,  1.0078e-02,  2.1323e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.3101e-02, -1.6605e-02, -2.7713e-02],\n",
       "                        [-1.6375e-02, -1.7742e-02,  3.2321e-03],\n",
       "                        [ 3.8062e-02, -3.6311e-02,  3.4696e-02]],\n",
       "              \n",
       "                       [[-2.8717e-02, -3.1814e-02,  2.8408e-03],\n",
       "                        [-4.1630e-02, -8.6604e-03, -1.3176e-02],\n",
       "                        [-3.0544e-02,  3.8322e-02, -6.7906e-03]],\n",
       "              \n",
       "                       [[ 1.6414e-02, -3.2171e-02, -3.7360e-02],\n",
       "                        [ 4.4400e-03,  2.4497e-02, -3.0932e-02],\n",
       "                        [-2.7928e-02,  4.1240e-02, -3.5248e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.5720e-02, -1.4095e-02, -1.3388e-02],\n",
       "                        [ 1.3881e-02,  1.3946e-02,  2.8456e-02],\n",
       "                        [-3.9684e-02,  1.3216e-03, -1.8222e-02]],\n",
       "              \n",
       "                       [[ 2.5034e-02,  1.2596e-02, -6.6899e-03],\n",
       "                        [ 1.1843e-02,  3.2463e-02, -3.7163e-02],\n",
       "                        [ 2.4361e-02, -2.6977e-02, -3.3954e-02]],\n",
       "              \n",
       "                       [[ 2.9540e-02,  1.9246e-02, -8.6777e-03],\n",
       "                        [ 3.5542e-02,  1.4535e-02, -3.0964e-02],\n",
       "                        [-2.0298e-02, -1.2386e-02,  6.8763e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.3776e-03, -3.3586e-02, -3.8933e-02],\n",
       "                        [ 4.9288e-03,  7.5151e-03,  3.4313e-02],\n",
       "                        [ 2.3882e-02, -1.4004e-02,  7.9537e-03]],\n",
       "              \n",
       "                       [[-2.0275e-02,  3.7737e-02, -4.1042e-03],\n",
       "                        [-3.1467e-02, -3.3101e-02, -1.8555e-02],\n",
       "                        [-2.4879e-02,  6.2864e-03, -2.3963e-02]],\n",
       "              \n",
       "                       [[ 2.4130e-02, -2.5258e-02,  1.3855e-02],\n",
       "                        [ 2.9302e-02, -1.9088e-02, -2.4203e-02],\n",
       "                        [ 9.3923e-03, -1.2946e-02,  2.1784e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5176e-02, -6.2605e-04,  1.3536e-02],\n",
       "                        [-3.6427e-02, -5.4827e-04, -1.0436e-02],\n",
       "                        [ 1.1257e-02, -3.2168e-03, -3.5676e-02]],\n",
       "              \n",
       "                       [[ 3.3034e-02, -2.0784e-02, -3.2630e-02],\n",
       "                        [-2.9390e-02, -1.7438e-02, -3.7954e-02],\n",
       "                        [-3.4909e-02,  2.6384e-02, -3.7416e-03]],\n",
       "              \n",
       "                       [[ 1.3281e-02, -2.7001e-02, -1.9996e-02],\n",
       "                        [-3.9350e-02, -1.9338e-02, -2.1599e-02],\n",
       "                        [ 3.1617e-02, -1.4925e-02, -2.7035e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.9125e-03,  2.3156e-02,  2.0254e-02],\n",
       "                        [-1.3268e-02,  6.9707e-03,  3.4140e-02],\n",
       "                        [-9.3170e-03, -1.3351e-02,  1.5151e-02]],\n",
       "              \n",
       "                       [[ 1.0877e-02,  1.7081e-02, -3.7909e-02],\n",
       "                        [ 1.1727e-02, -3.7357e-02,  1.0625e-02],\n",
       "                        [ 1.3165e-02, -1.8324e-02,  1.9562e-02]],\n",
       "              \n",
       "                       [[ 1.7736e-02,  2.8326e-02,  3.1071e-02],\n",
       "                        [ 4.0972e-02, -2.3309e-02,  4.9455e-03],\n",
       "                        [-2.1693e-02,  1.9974e-02,  1.9679e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.1071e-02,  6.1385e-04, -9.1537e-03],\n",
       "                        [-4.1171e-02, -3.7121e-03,  1.2027e-02],\n",
       "                        [-4.0675e-02, -2.8824e-02,  1.3466e-02]],\n",
       "              \n",
       "                       [[ 1.6335e-02, -2.0515e-02,  2.0811e-02],\n",
       "                        [ 1.1633e-02,  2.8888e-03, -3.5203e-02],\n",
       "                        [ 3.6290e-02, -3.2288e-02, -5.6584e-03]],\n",
       "              \n",
       "                       [[-3.2742e-02,  7.2546e-03, -4.0938e-02],\n",
       "                        [ 2.5857e-02,  3.6146e-02,  1.2991e-02],\n",
       "                        [ 3.0818e-02,  8.0157e-03, -7.3989e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.2370e-02, -2.0188e-02, -1.8166e-02],\n",
       "                        [-2.3884e-02,  4.5484e-03,  3.2627e-02],\n",
       "                        [-9.8299e-05, -1.1267e-02, -3.3084e-02]],\n",
       "              \n",
       "                       [[ 2.3529e-02,  2.5321e-02,  3.8836e-02],\n",
       "                        [ 3.6491e-02, -3.7313e-03, -1.8848e-02],\n",
       "                        [-4.9198e-03, -2.6398e-02, -3.2411e-02]],\n",
       "              \n",
       "                       [[-3.6134e-02, -7.6994e-03,  3.6678e-02],\n",
       "                        [-2.5411e-02,  2.0932e-02,  2.6406e-02],\n",
       "                        [-1.7719e-02,  2.4836e-02, -2.7929e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1771e-02,  2.8793e-05,  2.6350e-02],\n",
       "                        [-1.0550e-02,  2.9018e-03, -2.2128e-02],\n",
       "                        [ 1.8713e-02,  1.4072e-02,  3.2688e-02]],\n",
       "              \n",
       "                       [[-1.0905e-02, -5.1213e-03,  2.8296e-02],\n",
       "                        [ 1.3302e-02,  8.3596e-03,  1.4453e-02],\n",
       "                        [ 3.8186e-02, -2.1869e-02, -2.8489e-02]],\n",
       "              \n",
       "                       [[-4.1518e-02,  9.4322e-04,  1.5388e-02],\n",
       "                        [ 1.0720e-03,  2.5785e-02,  2.2748e-02],\n",
       "                        [-4.0172e-02,  1.0901e-02,  3.1325e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 9.7837e-03,  1.3419e-02,  4.0881e-02],\n",
       "                        [-2.4833e-02,  3.9372e-02, -1.5492e-02],\n",
       "                        [-1.7868e-02, -2.1696e-02,  3.7853e-02]],\n",
       "              \n",
       "                       [[ 2.4345e-02,  1.2529e-03, -2.7202e-02],\n",
       "                        [-2.6120e-02, -2.0738e-02,  1.4055e-02],\n",
       "                        [-1.4906e-02,  1.5516e-02, -3.4887e-02]],\n",
       "              \n",
       "                       [[-2.1105e-02,  3.3073e-02, -5.1802e-03],\n",
       "                        [ 2.8441e-02, -2.4430e-02,  3.0880e-02],\n",
       "                        [-2.6485e-02, -1.3026e-02, -3.6813e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.3701e-03,  3.2182e-02,  2.8930e-02],\n",
       "                        [-2.3628e-03, -1.3116e-02, -2.9064e-02],\n",
       "                        [ 1.4240e-03, -2.1115e-02, -6.4015e-03]],\n",
       "              \n",
       "                       [[-1.8719e-02,  2.0554e-02, -3.7324e-02],\n",
       "                        [-2.3664e-02,  3.6710e-02,  2.6741e-02],\n",
       "                        [ 2.0282e-02, -3.2684e-02,  3.2067e-02]],\n",
       "              \n",
       "                       [[-3.4155e-02,  2.6888e-02,  4.0264e-02],\n",
       "                        [-6.2025e-04,  1.3603e-02,  2.6144e-03],\n",
       "                        [-2.9797e-02,  1.5205e-02,  8.3867e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.0805e-02, -2.9471e-02, -4.4678e-03],\n",
       "                        [-4.4587e-03,  6.2216e-05, -3.8663e-02],\n",
       "                        [-1.3646e-02,  3.0194e-02, -2.8873e-02]],\n",
       "              \n",
       "                       [[ 2.1270e-02, -3.7526e-02,  6.1834e-03],\n",
       "                        [-9.4712e-03, -2.5312e-02,  1.3174e-02],\n",
       "                        [-9.2639e-03, -1.1697e-02, -2.8365e-02]],\n",
       "              \n",
       "                       [[ 3.3277e-02,  6.3595e-03,  1.9489e-02],\n",
       "                        [ 3.2264e-03, -1.3421e-02, -1.8513e-02],\n",
       "                        [ 1.6853e-02,  4.1061e-02, -2.9967e-02]]]])),\n",
       "             ('conv_block2.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block2.1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('conv_block2.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block2.1.running_mean',\n",
       "              tensor([ 0.0700,  0.0102, -0.0582, -0.0086, -0.0551, -0.0353, -0.0083,  0.0273,\n",
       "                      -0.0364, -0.0641,  0.1767,  0.0601,  0.0023, -0.0006, -0.0456,  0.0186,\n",
       "                       0.0470, -0.0466,  0.0032, -0.0536, -0.0069,  0.0815,  0.0531,  0.0223,\n",
       "                       0.0321,  0.0130, -0.0079,  0.0746,  0.0374,  0.0389, -0.0482,  0.0468,\n",
       "                       0.0809,  0.0604,  0.0356, -0.0425,  0.0572, -0.0416,  0.0798,  0.0129,\n",
       "                      -0.0135, -0.0265, -0.0296, -0.0365,  0.0286, -0.0096,  0.0351, -0.0390,\n",
       "                      -0.0062,  0.0786, -0.0276,  0.0032,  0.1170,  0.0038, -0.0027,  0.0266,\n",
       "                      -0.0697, -0.0106,  0.0841,  0.0379, -0.0189,  0.0458, -0.0355,  0.0305,\n",
       "                       0.0107,  0.0060, -0.0699, -0.0207, -0.0602, -0.0660,  0.0054,  0.0758,\n",
       "                       0.0141,  0.0891, -0.0036,  0.0649, -0.0402, -0.0443, -0.0271, -0.0020,\n",
       "                      -0.0626,  0.0226, -0.0943, -0.0344,  0.0252, -0.0950, -0.0292, -0.0445,\n",
       "                      -0.0035, -0.0168, -0.0536,  0.0643, -0.1000,  0.0784,  0.0057, -0.0358,\n",
       "                       0.0348,  0.0286,  0.0042, -0.0991,  0.0040, -0.0143, -0.0184,  0.0366,\n",
       "                      -0.0212, -0.0987,  0.0075, -0.0850, -0.0142,  0.0460,  0.1363,  0.0131,\n",
       "                       0.0188,  0.0291,  0.0336, -0.0631, -0.1005,  0.0017,  0.0596, -0.0727,\n",
       "                      -0.0019, -0.0473,  0.0464,  0.0292,  0.0140, -0.0327,  0.0204,  0.0713,\n",
       "                      -0.0064,  0.0135,  0.0636,  0.0041, -0.0007, -0.0289,  0.0260,  0.0636,\n",
       "                       0.0222,  0.0909, -0.0769, -0.1096,  0.0128,  0.0146,  0.0961,  0.0360,\n",
       "                      -0.0837,  0.0296,  0.1450,  0.0461, -0.0479,  0.0299,  0.0361,  0.1258,\n",
       "                       0.0020, -0.0934, -0.0021,  0.0931,  0.0719, -0.0306,  0.0680, -0.0248,\n",
       "                       0.0534, -0.0248,  0.0790,  0.1104,  0.0848,  0.1177, -0.0771,  0.0389,\n",
       "                       0.0019,  0.0429, -0.0382, -0.0357,  0.0063,  0.0645, -0.0412, -0.0500,\n",
       "                       0.0643, -0.0490, -0.0084, -0.0576, -0.0654,  0.0837, -0.0665,  0.0899,\n",
       "                      -0.0086, -0.0457,  0.0503,  0.0651,  0.0063,  0.0197,  0.0253, -0.0478,\n",
       "                       0.1046,  0.0131, -0.1305, -0.0319, -0.0769, -0.0650,  0.0947,  0.0190,\n",
       "                      -0.0167, -0.0396,  0.0747, -0.0377, -0.0463,  0.0937, -0.0335, -0.0467,\n",
       "                      -0.0039,  0.0871, -0.0366, -0.0234, -0.0148,  0.0401, -0.0740, -0.1491,\n",
       "                       0.0369,  0.0211,  0.0406, -0.0130, -0.0349, -0.0372, -0.0156, -0.0924,\n",
       "                       0.0372,  0.0911,  0.1053,  0.0906,  0.0247, -0.1026, -0.0351,  0.0038,\n",
       "                       0.0546,  0.0587, -0.0136, -0.0726,  0.0005, -0.0130,  0.0107,  0.0390,\n",
       "                      -0.0779, -0.0572,  0.0792,  0.0782,  0.0670,  0.1004,  0.0352, -0.0588,\n",
       "                      -0.0301, -0.1216,  0.0136,  0.0224,  0.0179,  0.0979, -0.0530, -0.0517])),\n",
       "             ('conv_block2.1.running_var',\n",
       "              tensor([0.9473, 0.9363, 0.9311, 0.9293, 0.9226, 0.9330, 0.9316, 0.9592, 0.9268,\n",
       "                      0.9267, 0.9708, 0.9381, 0.9266, 0.9468, 0.9455, 0.9520, 0.9242, 0.9363,\n",
       "                      0.9228, 0.9304, 0.9418, 0.9978, 0.9277, 0.9210, 0.9229, 0.9223, 0.9233,\n",
       "                      0.9344, 0.9359, 0.9722, 0.9374, 0.9330, 0.9400, 0.9228, 0.9225, 0.9267,\n",
       "                      0.9236, 0.9383, 0.9539, 0.9235, 0.9260, 0.9291, 0.9273, 0.9484, 0.9206,\n",
       "                      0.9268, 0.9357, 0.9289, 0.9317, 0.9704, 0.9314, 0.9256, 0.9457, 0.9466,\n",
       "                      0.9167, 0.9497, 0.9450, 0.9325, 0.9396, 0.9373, 0.9416, 0.9253, 0.9452,\n",
       "                      0.9387, 0.9307, 0.9422, 0.9378, 0.9294, 0.9276, 0.9274, 0.9249, 0.9338,\n",
       "                      0.9307, 0.9456, 0.9701, 0.9346, 0.9395, 0.9361, 0.9224, 0.9432, 0.9250,\n",
       "                      0.9256, 0.9360, 0.9304, 0.9233, 0.9399, 0.9578, 0.9229, 0.9240, 0.9334,\n",
       "                      0.9441, 0.9279, 0.9465, 0.9334, 0.9227, 0.9212, 0.9208, 0.9461, 0.9268,\n",
       "                      0.9290, 0.9258, 0.9267, 0.9235, 0.9223, 0.9316, 0.9529, 0.9495, 0.9351,\n",
       "                      0.9746, 0.9362, 0.9504, 0.9269, 0.9427, 0.9317, 0.9337, 0.9360, 0.9342,\n",
       "                      0.9325, 0.9291, 0.9305, 0.9442, 0.9442, 0.9563, 0.9249, 0.9427, 0.9233,\n",
       "                      0.9332, 0.9312, 0.9358, 0.9247, 0.9323, 0.9343, 0.9535, 0.9328, 0.9345,\n",
       "                      0.9458, 0.9296, 0.9455, 0.9306, 0.9438, 0.9309, 0.9233, 0.9356, 0.9207,\n",
       "                      0.9641, 0.9438, 0.9535, 0.9487, 0.9235, 0.9455, 0.9519, 0.9392, 0.9278,\n",
       "                      0.9373, 0.9432, 0.9391, 0.9289, 0.9432, 0.9265, 0.9242, 0.9304, 0.9286,\n",
       "                      0.9347, 0.9493, 0.9573, 0.9471, 0.9344, 0.9259, 0.9347, 0.9274, 0.9204,\n",
       "                      0.9367, 0.9230, 0.9318, 0.9466, 0.9401, 0.9499, 0.9325, 0.9262, 0.9406,\n",
       "                      0.9295, 0.9407, 0.9416, 0.9324, 0.9191, 0.9241, 0.9325, 0.9216, 0.9446,\n",
       "                      0.9371, 0.9497, 0.9415, 0.9527, 0.9301, 0.9470, 0.9270, 0.9332, 0.9382,\n",
       "                      0.9454, 0.9504, 0.9348, 0.9341, 0.9717, 0.9263, 0.9417, 0.9358, 0.9209,\n",
       "                      0.9288, 0.9225, 0.9301, 0.9284, 0.9300, 0.9300, 0.9248, 0.9560, 0.9542,\n",
       "                      0.9349, 0.9198, 0.9241, 0.9215, 0.9314, 0.9368, 0.9217, 0.9399, 0.9235,\n",
       "                      0.9292, 0.9456, 0.9623, 0.9304, 0.9320, 0.9356, 0.9248, 0.9283, 0.9404,\n",
       "                      0.9285, 0.9298, 0.9624, 0.9221, 0.9235, 0.9392, 0.9370, 0.9520, 0.9441,\n",
       "                      0.9214, 0.9253, 0.9498, 0.9273, 0.9267, 0.9193, 0.9663, 0.9219, 0.9252,\n",
       "                      0.9271, 0.9348, 0.9460, 0.9273])),\n",
       "             ('conv_block2.1.num_batches_tracked', tensor(1)),\n",
       "             ('conv_block2.3.weight',\n",
       "              tensor([[[[-1.0800e-02,  2.8441e-02, -1.5634e-02],\n",
       "                        [-4.9554e-03,  3.3945e-02, -3.0899e-02],\n",
       "                        [ 2.3227e-02,  1.1160e-03,  2.6249e-02]],\n",
       "              \n",
       "                       [[-1.5789e-03, -6.4140e-03, -2.6443e-02],\n",
       "                        [-1.8791e-02, -2.8862e-02,  1.4296e-02],\n",
       "                        [ 1.4650e-02,  1.2272e-02,  1.6979e-02]],\n",
       "              \n",
       "                       [[-8.5744e-03, -2.2875e-02,  2.2341e-02],\n",
       "                        [-6.0153e-03, -1.3184e-02,  3.4458e-02],\n",
       "                        [-3.1634e-02,  2.9678e-02,  2.8594e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.1638e-02, -2.2954e-02,  3.3882e-03],\n",
       "                        [-3.3461e-02,  3.3896e-02,  1.9580e-02],\n",
       "                        [ 1.9560e-02, -5.4760e-03,  3.8377e-03]],\n",
       "              \n",
       "                       [[ 1.3352e-02, -1.4474e-02,  2.1145e-02],\n",
       "                        [ 9.5341e-03,  1.3220e-02, -2.6375e-02],\n",
       "                        [-1.2452e-02, -3.8317e-03,  1.6261e-03]],\n",
       "              \n",
       "                       [[ 3.5596e-02, -2.9097e-03,  2.5699e-02],\n",
       "                        [-2.9772e-02,  1.7435e-02, -2.9726e-02],\n",
       "                        [-1.6811e-03,  8.7338e-03,  7.9654e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.7091e-03, -3.1889e-02, -3.0186e-02],\n",
       "                        [ 2.5904e-02,  7.1699e-03, -1.3562e-02],\n",
       "                        [-2.3927e-02,  3.6046e-02,  2.5935e-02]],\n",
       "              \n",
       "                       [[-1.2069e-02,  1.2281e-02,  3.2445e-02],\n",
       "                        [-1.8137e-02, -1.2400e-02, -3.4144e-02],\n",
       "                        [-3.0973e-02,  1.4977e-02, -2.3144e-02]],\n",
       "              \n",
       "                       [[ 2.4617e-02, -1.7694e-03,  1.9364e-02],\n",
       "                        [-3.4806e-02,  2.8190e-02,  1.7977e-02],\n",
       "                        [-3.4866e-03, -1.3797e-02, -2.3360e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.2277e-03, -2.4891e-02,  2.4256e-02],\n",
       "                        [-3.4257e-02,  2.8177e-02,  3.3706e-02],\n",
       "                        [-3.5513e-02,  2.5981e-03, -2.7525e-02]],\n",
       "              \n",
       "                       [[-1.1363e-02, -3.3500e-02,  1.0676e-02],\n",
       "                        [-3.5918e-02, -3.1472e-02, -2.7888e-02],\n",
       "                        [-8.4452e-03, -1.0696e-02, -2.9948e-02]],\n",
       "              \n",
       "                       [[ 1.8807e-02, -3.2484e-03, -6.8382e-03],\n",
       "                        [-7.2554e-03,  7.9012e-03,  1.9472e-02],\n",
       "                        [-7.9658e-03,  3.1724e-02,  1.5929e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.3375e-02,  2.5334e-02,  1.4776e-02],\n",
       "                        [-2.9553e-02,  7.6273e-03, -3.3876e-02],\n",
       "                        [ 2.0103e-03, -3.3482e-02, -1.9005e-02]],\n",
       "              \n",
       "                       [[-2.0830e-02,  3.3249e-03, -1.0084e-02],\n",
       "                        [ 2.9020e-02, -1.5986e-02,  2.1158e-02],\n",
       "                        [ 2.3041e-02, -2.1644e-03,  2.7277e-02]],\n",
       "              \n",
       "                       [[-1.6485e-02, -9.0196e-03,  3.0034e-02],\n",
       "                        [-1.7904e-02, -2.3405e-02, -1.4210e-02],\n",
       "                        [ 3.4747e-02,  2.7163e-03, -3.3470e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.7878e-02,  2.2621e-03,  2.5461e-02],\n",
       "                        [-1.1418e-02, -6.9475e-03,  2.9242e-02],\n",
       "                        [ 1.0386e-02, -2.7592e-02,  1.9732e-02]],\n",
       "              \n",
       "                       [[ 2.1419e-02, -1.2793e-02,  1.0174e-02],\n",
       "                        [ 3.4417e-02, -4.1479e-03, -2.1951e-02],\n",
       "                        [-3.3367e-05, -2.3671e-02, -7.9230e-03]],\n",
       "              \n",
       "                       [[-5.8337e-03,  3.5226e-02, -2.6396e-02],\n",
       "                        [-2.2361e-02,  2.7176e-03, -2.9775e-02],\n",
       "                        [ 1.5623e-02,  2.8391e-02, -1.8647e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.4251e-03,  3.3994e-02, -1.1240e-02],\n",
       "                        [-1.6227e-02, -1.2138e-02,  1.4950e-02],\n",
       "                        [ 7.4957e-03, -7.0344e-03, -1.4961e-02]],\n",
       "              \n",
       "                       [[-5.9995e-03,  3.1097e-02,  2.3008e-02],\n",
       "                        [ 1.4761e-02,  1.7909e-02,  1.4353e-02],\n",
       "                        [ 1.8818e-02,  2.0247e-02, -2.9680e-02]],\n",
       "              \n",
       "                       [[ 1.5507e-02,  1.7292e-02,  2.9316e-02],\n",
       "                        [ 2.0544e-02,  5.0657e-03, -2.2305e-03],\n",
       "                        [ 1.6721e-03,  1.5007e-02, -1.5225e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.6877e-02, -3.1479e-02,  3.1413e-02],\n",
       "                        [ 8.2463e-03,  3.3747e-02,  1.4905e-03],\n",
       "                        [ 3.8731e-03,  3.4510e-02, -2.1768e-02]],\n",
       "              \n",
       "                       [[-1.3145e-02,  2.9654e-02, -3.6741e-03],\n",
       "                        [-1.4363e-02, -3.4932e-02, -9.9273e-03],\n",
       "                        [-8.1692e-03, -2.3132e-02, -1.3801e-02]],\n",
       "              \n",
       "                       [[ 3.2688e-02, -1.6793e-02, -2.4338e-02],\n",
       "                        [-6.6197e-03,  3.6065e-02, -2.5096e-02],\n",
       "                        [ 5.9037e-03, -3.8106e-03, -3.0207e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4324e-02,  1.6962e-02, -2.9253e-02],\n",
       "                        [-7.9226e-04, -7.9832e-03,  2.8478e-02],\n",
       "                        [-1.1710e-02,  2.6482e-02, -2.2141e-02]],\n",
       "              \n",
       "                       [[ 1.0544e-02, -2.7635e-02, -4.2195e-03],\n",
       "                        [-1.5395e-02, -1.9819e-02,  9.0910e-03],\n",
       "                        [-9.4155e-03,  9.2385e-03,  3.5639e-03]],\n",
       "              \n",
       "                       [[ 1.3502e-02, -6.3509e-03,  3.1475e-02],\n",
       "                        [ 1.4613e-02,  1.2443e-02,  1.1727e-03],\n",
       "                        [ 3.5894e-02, -1.9141e-02, -9.9319e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.2973e-02, -4.1287e-03, -2.4206e-02],\n",
       "                        [ 2.2348e-02,  1.2620e-02,  2.3068e-02],\n",
       "                        [ 2.0436e-02, -3.2044e-02,  2.5004e-02]],\n",
       "              \n",
       "                       [[ 7.6298e-05, -9.5828e-03,  4.5001e-03],\n",
       "                        [ 3.4075e-02, -2.7596e-02, -1.8009e-02],\n",
       "                        [ 3.0489e-02,  1.1429e-02,  3.0309e-02]],\n",
       "              \n",
       "                       [[ 9.8494e-03, -9.0176e-03,  2.6487e-03],\n",
       "                        [ 2.1006e-02,  7.4314e-03, -1.7859e-02],\n",
       "                        [ 3.2220e-02,  9.2170e-03, -1.8532e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1880e-02,  1.2943e-02,  2.8610e-03],\n",
       "                        [ 4.7682e-03,  1.5323e-02,  2.5945e-02],\n",
       "                        [-1.8753e-02, -2.8213e-02,  1.3202e-02]],\n",
       "              \n",
       "                       [[-1.3710e-02, -1.5891e-02, -2.4892e-02],\n",
       "                        [-2.0263e-02,  1.5372e-02, -4.5908e-03],\n",
       "                        [ 2.1566e-02,  3.5871e-02,  1.7046e-02]],\n",
       "              \n",
       "                       [[-7.1031e-04,  1.8561e-02,  2.6283e-03],\n",
       "                        [ 8.7622e-03, -7.1285e-03,  5.3996e-03],\n",
       "                        [ 3.0880e-02,  2.0445e-02,  3.2196e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.1708e-02,  1.9281e-02,  1.1989e-02],\n",
       "                        [-3.5705e-02,  4.6764e-03, -2.8763e-03],\n",
       "                        [-7.6047e-03, -3.0930e-02, -2.6709e-02]],\n",
       "              \n",
       "                       [[ 3.2242e-02, -1.6330e-04,  2.4441e-03],\n",
       "                        [ 2.9156e-04,  4.4115e-03, -2.8777e-02],\n",
       "                        [-2.1135e-02, -3.2160e-02,  5.9109e-03]],\n",
       "              \n",
       "                       [[ 2.5079e-02, -2.1080e-02, -1.7081e-02],\n",
       "                        [-2.4042e-02, -1.0590e-02, -1.9705e-02],\n",
       "                        [ 2.5034e-02,  3.3507e-02, -2.1431e-02]]]])),\n",
       "             ('conv_block2.3.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block2.4.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1.])),\n",
       "             ('conv_block2.4.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block2.4.running_mean',\n",
       "              tensor([-3.1197e-02, -2.6119e-02, -4.4932e-02,  3.5574e-02,  1.7360e-02,\n",
       "                      -2.0205e-02, -6.1992e-02, -1.2183e-02,  1.6796e-03, -4.0197e-03,\n",
       "                       2.5929e-02,  3.0134e-02, -2.5014e-02,  2.2660e-02, -7.3405e-03,\n",
       "                       1.2083e-02,  7.6874e-02, -4.0712e-02,  2.9777e-02,  4.8049e-03,\n",
       "                       1.1481e-02, -2.3416e-02,  4.5446e-02,  3.4890e-02, -1.0737e-02,\n",
       "                      -9.2940e-03,  5.0209e-02, -3.0379e-02,  4.1476e-02,  2.6206e-02,\n",
       "                      -2.1267e-02, -2.2580e-02, -1.3255e-02, -3.5989e-02, -1.0951e-02,\n",
       "                      -1.6207e-03,  2.8845e-03,  8.6697e-03, -5.9803e-02,  4.4667e-02,\n",
       "                       1.1302e-02,  1.2383e-02, -3.3393e-02,  1.2185e-02,  1.9181e-02,\n",
       "                       2.0276e-02,  1.1487e-01,  5.3781e-02, -7.0763e-02,  2.2664e-03,\n",
       "                       3.4362e-02, -3.4060e-02, -1.3569e-03, -1.8966e-02, -1.0390e-02,\n",
       "                      -3.9676e-02, -4.3256e-02,  1.7226e-02,  1.7593e-03,  6.6224e-03,\n",
       "                      -7.2671e-03,  2.9446e-02, -4.8069e-02,  1.1329e-02, -6.0335e-02,\n",
       "                      -1.2616e-02, -5.4444e-03,  1.3291e-02,  4.3218e-02,  1.0520e-02,\n",
       "                      -4.6642e-02,  6.7307e-03, -1.7266e-02, -3.9262e-02,  3.4175e-03,\n",
       "                      -9.7291e-03,  1.2845e-03, -4.0915e-02, -5.6759e-03,  3.6509e-02,\n",
       "                      -5.4175e-02, -1.8709e-02,  6.7507e-02,  5.8417e-02,  3.6040e-02,\n",
       "                      -9.2749e-03,  1.3869e-02,  8.1265e-03, -2.6596e-02,  4.9298e-02,\n",
       "                       1.1844e-02, -2.3971e-02, -1.0193e-02,  3.1350e-02,  2.9811e-02,\n",
       "                      -2.2757e-02,  1.6648e-02, -5.1572e-02, -5.1346e-02,  8.8705e-02,\n",
       "                      -6.5413e-02, -1.1207e-02, -4.2893e-02,  2.1665e-02, -3.8129e-02,\n",
       "                       6.2909e-02, -3.2557e-02, -1.5702e-02, -1.0274e-04, -4.7298e-02,\n",
       "                       3.5285e-02, -4.3453e-03,  1.1209e-02, -4.0331e-02,  5.1159e-02,\n",
       "                       5.6400e-03,  8.7154e-03,  4.8441e-03,  5.4353e-02, -3.4699e-02,\n",
       "                      -1.1305e-02,  1.3853e-01, -4.2092e-03,  4.3872e-02,  5.8358e-02,\n",
       "                      -1.8586e-02, -1.4424e-02, -2.8500e-02,  2.1259e-02,  3.6732e-02,\n",
       "                      -1.1942e-04, -2.8104e-02,  1.9620e-02,  7.5049e-02, -3.0248e-02,\n",
       "                      -6.0587e-02, -3.0540e-02, -2.3016e-03, -1.4110e-02,  6.2564e-03,\n",
       "                      -4.9494e-02,  2.1397e-02, -1.9647e-02,  1.6113e-02,  1.3559e-02,\n",
       "                      -6.2209e-02,  7.9649e-03,  3.1675e-02, -1.8297e-02, -1.8168e-03,\n",
       "                      -3.6239e-02, -1.7725e-02, -7.1990e-02, -2.3999e-02, -3.2259e-02,\n",
       "                       3.4072e-02, -8.7841e-03,  3.2154e-02, -3.7643e-02, -1.1473e-02,\n",
       "                      -2.6556e-03, -5.2696e-02,  5.0969e-03, -3.5330e-03,  4.3931e-03,\n",
       "                       3.2651e-02, -1.1321e-02, -3.1489e-02, -6.5869e-02, -6.0021e-02,\n",
       "                      -4.7885e-03,  5.2394e-02, -1.3341e-03,  2.0822e-02, -2.8172e-02,\n",
       "                      -3.6447e-02, -4.2280e-02, -1.0379e-02,  4.0104e-03,  4.3327e-02,\n",
       "                       3.7639e-02, -4.1393e-03, -1.7142e-02, -3.9062e-02,  5.2238e-02,\n",
       "                       7.8677e-02,  3.6333e-02, -2.3261e-02, -4.9237e-02,  5.0322e-02,\n",
       "                      -2.1450e-02,  1.0340e-02,  1.9265e-02,  1.1257e-02, -9.8028e-03,\n",
       "                      -9.5898e-03, -2.7747e-02, -2.4913e-02, -1.1868e-02,  7.4948e-02,\n",
       "                      -5.1673e-03,  2.7454e-02,  7.6680e-03,  3.2872e-02,  3.3120e-02,\n",
       "                      -1.3457e-02, -6.2386e-02, -1.7290e-02,  4.8769e-02, -2.0498e-02,\n",
       "                       8.9666e-05, -5.9187e-02, -1.7638e-02,  3.0872e-02,  2.3577e-02,\n",
       "                      -3.9027e-02, -7.3786e-02,  2.1187e-02,  3.5309e-02,  5.5232e-02,\n",
       "                       1.5501e-02,  5.8661e-03, -2.6591e-02,  1.0033e-02, -2.0287e-02,\n",
       "                       4.4921e-03,  5.2786e-02,  3.0974e-02, -6.8735e-04, -3.7510e-02,\n",
       "                      -2.3815e-02, -8.4310e-03,  2.0130e-02, -4.9057e-02,  4.8384e-02,\n",
       "                      -3.5785e-02,  4.2148e-02, -2.3222e-03,  2.8179e-02,  3.2086e-02,\n",
       "                       3.3059e-02,  2.5284e-02, -1.7492e-02,  2.5972e-02,  1.2275e-02,\n",
       "                       2.1699e-02, -6.0479e-03,  1.8787e-02, -1.8248e-02,  5.5106e-03,\n",
       "                       1.4355e-02,  2.1103e-02, -5.8317e-02, -4.6932e-02,  3.8731e-02,\n",
       "                      -3.1531e-02])),\n",
       "             ('conv_block2.4.running_var',\n",
       "              tensor([0.9586, 0.9272, 0.9394, 0.9414, 0.9282, 0.9220, 0.9479, 0.9283, 0.9291,\n",
       "                      0.9264, 0.9361, 0.9266, 0.9264, 0.9362, 0.9369, 0.9255, 0.9275, 0.9307,\n",
       "                      0.9294, 0.9212, 0.9242, 0.9274, 0.9308, 0.9323, 0.9207, 0.9289, 0.9496,\n",
       "                      0.9401, 0.9343, 0.9241, 0.9316, 0.9374, 0.9287, 0.9424, 0.9433, 0.9277,\n",
       "                      0.9407, 0.9324, 0.9333, 0.9337, 0.9306, 0.9256, 0.9324, 0.9226, 0.9245,\n",
       "                      0.9303, 0.9317, 0.9308, 0.9474, 0.9225, 0.9218, 0.9325, 0.9334, 0.9221,\n",
       "                      0.9270, 0.9433, 0.9261, 0.9294, 0.9280, 0.9350, 0.9295, 0.9371, 0.9250,\n",
       "                      0.9252, 0.9372, 0.9307, 0.9340, 0.9401, 0.9391, 0.9291, 0.9373, 0.9327,\n",
       "                      0.9277, 0.9303, 0.9320, 0.9266, 0.9246, 0.9361, 0.9334, 0.9292, 0.9303,\n",
       "                      0.9377, 0.9451, 0.9334, 0.9367, 0.9289, 0.9339, 0.9357, 0.9373, 0.9314,\n",
       "                      0.9487, 0.9250, 0.9314, 0.9257, 0.9370, 0.9267, 0.9342, 0.9380, 0.9452,\n",
       "                      0.9370, 0.9271, 0.9317, 0.9395, 0.9328, 0.9253, 0.9411, 0.9428, 0.9248,\n",
       "                      0.9348, 0.9288, 0.9395, 0.9322, 0.9361, 0.9419, 0.9517, 0.9390, 0.9310,\n",
       "                      0.9383, 0.9380, 0.9219, 0.9328, 0.9551, 0.9267, 0.9430, 0.9374, 0.9366,\n",
       "                      0.9340, 0.9253, 0.9377, 0.9280, 0.9320, 0.9300, 0.9316, 0.9252, 0.9420,\n",
       "                      0.9279, 0.9349, 0.9410, 0.9325, 0.9325, 0.9388, 0.9365, 0.9260, 0.9313,\n",
       "                      0.9339, 0.9340, 0.9312, 0.9332, 0.9362, 0.9315, 0.9213, 0.9266, 0.9319,\n",
       "                      0.9295, 0.9445, 0.9460, 0.9360, 0.9266, 0.9292, 0.9290, 0.9216, 0.9535,\n",
       "                      0.9323, 0.9305, 0.9259, 0.9290, 0.9219, 0.9433, 0.9563, 0.9254, 0.9255,\n",
       "                      0.9340, 0.9247, 0.9270, 0.9298, 0.9412, 0.9362, 0.9286, 0.9326, 0.9344,\n",
       "                      0.9301, 0.9279, 0.9263, 0.9303, 0.9347, 0.9365, 0.9282, 0.9349, 0.9385,\n",
       "                      0.9265, 0.9406, 0.9374, 0.9376, 0.9377, 0.9250, 0.9308, 0.9350, 0.9490,\n",
       "                      0.9276, 0.9502, 0.9358, 0.9225, 0.9427, 0.9409, 0.9281, 0.9283, 0.9423,\n",
       "                      0.9312, 0.9364, 0.9275, 0.9294, 0.9278, 0.9287, 0.9296, 0.9366, 0.9261,\n",
       "                      0.9407, 0.9267, 0.9246, 0.9223, 0.9463, 0.9425, 0.9274, 0.9264, 0.9286,\n",
       "                      0.9338, 0.9353, 0.9320, 0.9381, 0.9284, 0.9313, 0.9321, 0.9234, 0.9321,\n",
       "                      0.9358, 0.9258, 0.9395, 0.9274, 0.9302, 0.9386, 0.9383, 0.9279, 0.9249,\n",
       "                      0.9274, 0.9316, 0.9334, 0.9409, 0.9267, 0.9298, 0.9267, 0.9240, 0.9348,\n",
       "                      0.9294, 0.9252, 0.9259, 0.9283])),\n",
       "             ('conv_block2.4.num_batches_tracked', tensor(1)),\n",
       "             ('conv_block3.0.weight',\n",
       "              tensor([[[[ 0.0005,  0.0171, -0.0191],\n",
       "                        [-0.0135,  0.0220, -0.0156],\n",
       "                        [ 0.0174,  0.0203,  0.0213]],\n",
       "              \n",
       "                       [[-0.0266, -0.0049, -0.0135],\n",
       "                        [ 0.0040, -0.0241, -0.0284],\n",
       "                        [-0.0091, -0.0228,  0.0035]],\n",
       "              \n",
       "                       [[-0.0097,  0.0090, -0.0273],\n",
       "                        [ 0.0171, -0.0267, -0.0263],\n",
       "                        [ 0.0182,  0.0171, -0.0048]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0059,  0.0159,  0.0019],\n",
       "                        [ 0.0104, -0.0214, -0.0163],\n",
       "                        [-0.0172,  0.0171,  0.0294]],\n",
       "              \n",
       "                       [[ 0.0033, -0.0263,  0.0198],\n",
       "                        [-0.0177,  0.0247, -0.0030],\n",
       "                        [ 0.0236, -0.0095, -0.0024]],\n",
       "              \n",
       "                       [[-0.0098,  0.0246, -0.0268],\n",
       "                        [-0.0240, -0.0240,  0.0108],\n",
       "                        [ 0.0242,  0.0095,  0.0190]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0275, -0.0274,  0.0185],\n",
       "                        [-0.0228, -0.0140, -0.0269],\n",
       "                        [-0.0172,  0.0229,  0.0075]],\n",
       "              \n",
       "                       [[ 0.0167, -0.0207, -0.0266],\n",
       "                        [-0.0230, -0.0041,  0.0201],\n",
       "                        [-0.0285,  0.0184,  0.0118]],\n",
       "              \n",
       "                       [[-0.0167, -0.0245, -0.0137],\n",
       "                        [-0.0020,  0.0195,  0.0115],\n",
       "                        [-0.0238,  0.0053, -0.0194]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0043,  0.0014, -0.0068],\n",
       "                        [-0.0255, -0.0001, -0.0144],\n",
       "                        [-0.0168,  0.0108, -0.0155]],\n",
       "              \n",
       "                       [[ 0.0035, -0.0272,  0.0283],\n",
       "                        [ 0.0087, -0.0022,  0.0239],\n",
       "                        [-0.0002,  0.0277, -0.0196]],\n",
       "              \n",
       "                       [[ 0.0002, -0.0027, -0.0074],\n",
       "                        [ 0.0112,  0.0240, -0.0264],\n",
       "                        [-0.0258,  0.0048, -0.0091]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0211, -0.0135, -0.0160],\n",
       "                        [-0.0237, -0.0041,  0.0015],\n",
       "                        [ 0.0097,  0.0013, -0.0274]],\n",
       "              \n",
       "                       [[ 0.0088,  0.0050, -0.0156],\n",
       "                        [ 0.0256, -0.0203, -0.0064],\n",
       "                        [-0.0268, -0.0170,  0.0094]],\n",
       "              \n",
       "                       [[-0.0014,  0.0259, -0.0271],\n",
       "                        [ 0.0062, -0.0204,  0.0225],\n",
       "                        [-0.0244, -0.0122,  0.0170]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0251,  0.0125, -0.0202],\n",
       "                        [-0.0289,  0.0275,  0.0017],\n",
       "                        [-0.0191,  0.0063, -0.0186]],\n",
       "              \n",
       "                       [[ 0.0101, -0.0120, -0.0143],\n",
       "                        [-0.0181, -0.0289, -0.0012],\n",
       "                        [ 0.0218, -0.0229, -0.0230]],\n",
       "              \n",
       "                       [[-0.0154, -0.0176, -0.0050],\n",
       "                        [-0.0058,  0.0063, -0.0178],\n",
       "                        [-0.0063,  0.0137,  0.0285]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0045, -0.0214,  0.0203],\n",
       "                        [ 0.0066,  0.0022,  0.0078],\n",
       "                        [ 0.0070, -0.0104,  0.0165]],\n",
       "              \n",
       "                       [[-0.0140,  0.0129,  0.0200],\n",
       "                        [ 0.0239, -0.0042,  0.0260],\n",
       "                        [-0.0244, -0.0030, -0.0157]],\n",
       "              \n",
       "                       [[ 0.0249, -0.0117, -0.0102],\n",
       "                        [ 0.0082,  0.0168,  0.0086],\n",
       "                        [-0.0251, -0.0058,  0.0165]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0088, -0.0235, -0.0292],\n",
       "                        [ 0.0177, -0.0088, -0.0255],\n",
       "                        [ 0.0130, -0.0208,  0.0119]],\n",
       "              \n",
       "                       [[ 0.0039,  0.0033,  0.0194],\n",
       "                        [ 0.0032, -0.0277,  0.0015],\n",
       "                        [ 0.0219,  0.0063, -0.0195]],\n",
       "              \n",
       "                       [[ 0.0062, -0.0013,  0.0171],\n",
       "                        [-0.0115,  0.0250, -0.0201],\n",
       "                        [ 0.0200, -0.0064, -0.0054]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0228,  0.0273, -0.0083],\n",
       "                        [ 0.0174,  0.0260,  0.0070],\n",
       "                        [-0.0077,  0.0085, -0.0017]],\n",
       "              \n",
       "                       [[ 0.0199,  0.0279, -0.0069],\n",
       "                        [ 0.0022, -0.0075, -0.0248],\n",
       "                        [-0.0005, -0.0293,  0.0110]],\n",
       "              \n",
       "                       [[-0.0106, -0.0192,  0.0004],\n",
       "                        [ 0.0190,  0.0066, -0.0168],\n",
       "                        [-0.0161, -0.0125,  0.0087]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0063, -0.0285,  0.0095],\n",
       "                        [ 0.0183,  0.0265,  0.0166],\n",
       "                        [ 0.0222, -0.0149,  0.0176]],\n",
       "              \n",
       "                       [[-0.0123,  0.0023,  0.0115],\n",
       "                        [ 0.0253, -0.0018, -0.0139],\n",
       "                        [-0.0023,  0.0100,  0.0168]],\n",
       "              \n",
       "                       [[-0.0210, -0.0039,  0.0275],\n",
       "                        [-0.0126, -0.0200, -0.0093],\n",
       "                        [-0.0073, -0.0092, -0.0281]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0039, -0.0092,  0.0228],\n",
       "                        [-0.0014,  0.0077,  0.0160],\n",
       "                        [-0.0215, -0.0173, -0.0046]],\n",
       "              \n",
       "                       [[-0.0098,  0.0048, -0.0136],\n",
       "                        [ 0.0161,  0.0137, -0.0156],\n",
       "                        [ 0.0043,  0.0233, -0.0185]],\n",
       "              \n",
       "                       [[ 0.0255, -0.0231, -0.0251],\n",
       "                        [-0.0227,  0.0107, -0.0262],\n",
       "                        [ 0.0262,  0.0021, -0.0043]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0219, -0.0061,  0.0012],\n",
       "                        [-0.0027,  0.0175, -0.0147],\n",
       "                        [-0.0178,  0.0083,  0.0291]],\n",
       "              \n",
       "                       [[ 0.0081,  0.0231, -0.0086],\n",
       "                        [-0.0013, -0.0031, -0.0276],\n",
       "                        [-0.0047, -0.0202, -0.0251]],\n",
       "              \n",
       "                       [[ 0.0180, -0.0250, -0.0085],\n",
       "                        [ 0.0250,  0.0133, -0.0097],\n",
       "                        [ 0.0128,  0.0264, -0.0112]]]])),\n",
       "             ('conv_block3.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block3.1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('conv_block3.1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block3.1.running_mean',\n",
       "              tensor([-9.0418e-02, -1.0789e-02, -5.4438e-02, -3.0881e-02, -4.9598e-04,\n",
       "                      -8.2463e-02, -2.8908e-02,  6.8692e-02, -4.5623e-02,  3.2563e-02,\n",
       "                       2.1955e-02,  1.3665e-01, -1.6211e-01, -1.9183e-02,  3.5437e-02,\n",
       "                       3.0037e-02,  2.4388e-02,  9.8985e-02,  5.8404e-03,  1.0642e-02,\n",
       "                       3.0924e-02,  9.8467e-03,  4.2738e-03,  3.9332e-02, -8.0433e-02,\n",
       "                       8.6968e-02, -4.2873e-02,  3.9443e-02,  1.4509e-02,  7.0000e-02,\n",
       "                      -3.0664e-02,  6.0663e-02, -2.5116e-03,  2.5469e-02, -1.5021e-02,\n",
       "                       2.5726e-02, -1.1154e-02, -2.7483e-02, -5.9390e-02, -2.5701e-02,\n",
       "                       4.6066e-02, -2.2013e-02, -6.9823e-03, -1.7310e-02,  8.0383e-02,\n",
       "                      -5.0754e-03, -6.8171e-02, -1.4431e-03, -2.6031e-02,  2.6486e-02,\n",
       "                      -1.1300e-01,  1.1827e-03,  1.9099e-02, -1.1877e-02, -4.0671e-02,\n",
       "                       3.4439e-03, -7.3636e-03, -3.2769e-02, -3.8874e-02,  1.8500e-02,\n",
       "                       2.7046e-02,  9.5263e-02, -3.5419e-02,  7.5251e-02,  2.4120e-02,\n",
       "                       1.8641e-02, -5.7723e-02,  2.1980e-02, -1.1691e-02, -1.9361e-02,\n",
       "                       7.5492e-02, -1.4590e-02,  6.7499e-03, -1.7336e-02, -6.3552e-02,\n",
       "                       6.4143e-02, -2.7808e-02,  4.9274e-02, -2.2372e-02,  3.9619e-02,\n",
       "                      -1.0048e-01, -3.5571e-02,  9.0804e-02, -7.9496e-03,  8.9462e-02,\n",
       "                       9.6575e-02,  3.8262e-04,  5.1532e-02,  1.0355e-03,  2.0458e-02,\n",
       "                      -6.8688e-02,  3.3772e-02, -3.7442e-02, -1.8459e-03,  1.3240e-02,\n",
       "                       5.4589e-02, -2.6018e-02, -4.4384e-02, -5.3009e-02,  1.4690e-01,\n",
       "                      -1.4745e-02,  6.6375e-03, -9.5860e-02, -1.5272e-02, -3.9606e-02,\n",
       "                       2.4321e-02,  5.9694e-04, -2.0058e-02,  2.6378e-02, -4.0922e-02,\n",
       "                       6.0270e-02, -7.4680e-04,  5.4724e-02,  8.5106e-03, -1.0599e-02,\n",
       "                      -6.4936e-02,  1.0646e-02, -5.0296e-02, -4.3720e-03, -4.5965e-02,\n",
       "                      -3.4827e-02, -5.8784e-02,  2.5315e-02,  1.4615e-01, -4.9067e-02,\n",
       "                       3.9693e-02, -3.7826e-02, -1.1036e-02,  1.4219e-02, -2.3983e-02,\n",
       "                      -2.9497e-02,  1.1470e-01,  1.1680e-02,  9.3232e-03, -1.2327e-01,\n",
       "                      -9.5786e-02, -9.1521e-03,  3.1822e-02, -7.2558e-03,  3.7958e-03,\n",
       "                       2.0291e-02, -6.5970e-02, -8.5534e-02, -5.3676e-02, -5.2105e-02,\n",
       "                      -4.9168e-02,  7.8437e-02,  4.0240e-02,  1.3016e-02,  8.4561e-02,\n",
       "                       8.6089e-02,  1.8335e-02,  3.4850e-02,  3.4524e-02,  3.7297e-02,\n",
       "                       3.7220e-02,  2.4603e-02, -1.8429e-02, -1.1265e-01,  1.9199e-03,\n",
       "                      -7.7513e-03,  1.3760e-01, -7.9172e-02,  9.0542e-02,  3.0367e-02,\n",
       "                       1.0213e-01, -7.0266e-02,  4.0023e-02,  6.1289e-04, -1.2036e-03,\n",
       "                      -1.3321e-01, -3.5608e-03, -1.1203e-01,  6.0395e-02, -3.0520e-03,\n",
       "                      -2.7135e-02,  2.3166e-02, -4.9101e-03, -3.1408e-02,  2.1416e-02,\n",
       "                      -6.9709e-02,  1.2462e-03,  6.9826e-02, -5.0851e-02, -5.5659e-02,\n",
       "                       1.8419e-02,  3.6050e-02,  8.8565e-03,  1.1392e-01,  3.5968e-02,\n",
       "                       5.3501e-02, -6.4714e-02, -2.2199e-02, -5.6741e-02, -9.5212e-02,\n",
       "                      -1.0087e-01, -4.9137e-02,  5.9482e-03,  4.6883e-02,  8.8964e-03,\n",
       "                      -1.7258e-02,  8.4345e-02, -6.2575e-02, -5.0627e-02, -1.7991e-02,\n",
       "                      -2.0232e-02,  5.4672e-02,  1.7254e-02,  6.0345e-02, -6.7387e-02,\n",
       "                       6.6348e-02,  4.8026e-02, -2.3954e-02,  1.7198e-02, -5.0763e-02,\n",
       "                      -1.8515e-02, -7.5040e-02,  5.2082e-03,  2.3760e-02,  5.0684e-02,\n",
       "                       2.3683e-03,  4.5356e-03, -6.4841e-02,  2.4569e-02, -8.9560e-02,\n",
       "                      -2.7866e-02, -3.6262e-02,  5.5932e-02,  5.1655e-02,  6.4774e-03,\n",
       "                       6.6490e-02,  6.2169e-02,  4.0542e-02, -9.9143e-02, -6.7559e-02,\n",
       "                       4.3968e-02,  5.6861e-05,  2.7900e-02, -3.4629e-02,  9.6015e-04,\n",
       "                      -7.0457e-03,  1.0089e-01, -8.7875e-02,  1.1711e-01,  5.6445e-02,\n",
       "                       4.2131e-03,  1.2654e-01,  9.0074e-03, -4.7305e-02, -6.4245e-02,\n",
       "                      -3.2943e-02,  4.0244e-02,  1.2185e-02,  7.2698e-02, -5.6371e-02,\n",
       "                      -4.8418e-02, -1.4926e-02, -6.7476e-02,  1.6671e-02,  1.7960e-02,\n",
       "                      -2.2405e-02,  5.5375e-02,  6.9045e-02, -6.2535e-02, -1.8287e-02,\n",
       "                       7.8253e-02,  5.8781e-02,  7.7318e-02,  4.1921e-02, -1.2465e-02,\n",
       "                      -6.8462e-02, -5.4107e-02,  9.2702e-02, -3.5738e-02,  4.4207e-02,\n",
       "                       3.6089e-02, -1.8566e-02,  4.5331e-02, -6.1972e-02, -3.8906e-02,\n",
       "                      -4.7487e-02,  7.5963e-02,  2.0383e-02,  1.1898e-03,  1.0431e-02,\n",
       "                      -4.1450e-02, -6.7105e-02, -2.7082e-02, -4.0901e-02, -4.7489e-04,\n",
       "                       5.2368e-03, -1.4495e-02, -3.3100e-04, -2.0975e-02, -7.3053e-02,\n",
       "                      -4.0053e-02,  2.2289e-02,  2.5320e-02, -4.2124e-02,  4.2247e-02,\n",
       "                       5.0942e-02,  6.0503e-02, -1.3711e-02,  1.5052e-01, -3.8226e-02,\n",
       "                      -1.5183e-02,  7.6567e-02, -3.6086e-02, -5.8497e-02, -6.0864e-02,\n",
       "                       2.2592e-02, -3.1093e-02, -8.9637e-02,  2.7584e-02, -3.0722e-02,\n",
       "                      -2.8157e-02,  8.8965e-02, -6.7583e-02, -3.7733e-02, -1.5745e-02,\n",
       "                       3.1404e-02, -1.2754e-01, -2.3442e-02,  6.5672e-02, -7.7474e-02,\n",
       "                       7.7420e-02,  5.8431e-02, -4.2660e-02, -6.6920e-02, -9.8136e-03,\n",
       "                       5.4917e-02, -4.1880e-02,  9.7892e-02, -4.9817e-02,  3.5923e-04,\n",
       "                       6.0184e-02,  1.1256e-01,  9.7677e-02, -9.4613e-02,  4.5432e-02,\n",
       "                      -9.8189e-02, -2.0996e-02,  2.4297e-02, -5.0257e-02,  9.8313e-03,\n",
       "                       1.2851e-02, -7.7470e-02,  1.2800e-02, -5.5537e-02,  3.2303e-02,\n",
       "                      -4.9432e-03,  7.0429e-03,  3.7602e-02,  7.5769e-02, -6.9561e-02,\n",
       "                      -1.3790e-03, -1.6821e-02,  2.3750e-02,  7.2568e-03,  1.1172e-01,\n",
       "                       1.0672e-01, -7.5818e-02,  1.1360e-01, -1.8885e-02,  1.5382e-01,\n",
       "                       6.3101e-02,  1.2093e-02, -4.0168e-02, -3.7729e-02, -1.3518e-03,\n",
       "                      -2.5986e-02, -1.3625e-02,  1.7110e-01,  7.7093e-02,  8.0719e-03,\n",
       "                      -2.7365e-02,  1.8279e-02, -4.5387e-03,  3.1781e-05, -7.4852e-02,\n",
       "                       3.1039e-02,  3.2229e-02, -3.4815e-02,  6.6443e-03,  3.5057e-02,\n",
       "                      -4.0355e-02, -2.5752e-02, -3.4059e-02,  1.2554e-02, -7.4128e-02,\n",
       "                       9.4177e-02,  9.0810e-03,  1.1675e-01, -5.8223e-02, -7.6235e-03,\n",
       "                       4.8879e-02,  8.1060e-02, -3.2426e-02, -1.0751e-02, -2.4328e-02,\n",
       "                       2.8137e-02, -1.0681e-01, -1.0111e-01, -3.6167e-02, -1.0900e-02,\n",
       "                       5.4406e-02, -4.1735e-02,  3.7894e-02,  4.3302e-02, -1.0276e-02,\n",
       "                      -9.7159e-03, -5.7892e-02, -3.5313e-02, -2.5802e-03, -2.5902e-02,\n",
       "                      -9.2105e-02, -4.9164e-02, -1.1964e-02,  3.7643e-02,  3.2209e-02,\n",
       "                       4.0281e-02,  8.5187e-02, -2.9170e-02,  2.5102e-02, -3.3128e-02,\n",
       "                      -7.6627e-02,  7.3461e-02, -1.8095e-02, -2.3518e-02, -7.4959e-02,\n",
       "                      -3.4822e-02, -3.4716e-02,  6.2867e-02,  4.3966e-02,  7.0885e-03,\n",
       "                       2.9092e-02,  3.8266e-02,  5.0820e-02,  2.7064e-02, -1.0591e-02,\n",
       "                       3.7816e-02,  7.2242e-02, -7.5215e-03, -2.7945e-02, -9.2133e-02,\n",
       "                       2.0583e-02, -8.3233e-02, -1.0183e-01, -8.5789e-02,  8.1461e-02,\n",
       "                       8.0156e-02, -7.3566e-02,  2.3998e-02,  4.4110e-02, -6.6786e-02,\n",
       "                       2.3307e-02, -3.8326e-02, -4.6991e-02,  2.4921e-03,  5.7356e-02,\n",
       "                      -5.1534e-02, -1.0192e-02,  3.8686e-02, -3.6986e-02, -6.0942e-02,\n",
       "                       1.5323e-02, -2.5099e-02, -1.1935e-01,  6.4973e-02, -5.2413e-02,\n",
       "                       3.3174e-02, -1.3734e-01, -4.5661e-03,  1.6435e-02,  5.7718e-02,\n",
       "                      -6.1374e-02,  6.5866e-02,  2.7303e-02,  5.5380e-02, -1.5549e-02,\n",
       "                       6.6271e-02, -4.8756e-02,  3.1015e-02,  1.7688e-02,  6.0136e-03,\n",
       "                       1.8561e-02,  2.0788e-02, -1.6518e-03, -3.8546e-02,  1.9385e-01,\n",
       "                      -1.7171e-02,  3.5158e-02,  4.1663e-02,  1.2251e-01,  3.3227e-02,\n",
       "                       7.2707e-02, -7.5099e-02,  8.2076e-02, -8.6907e-02,  8.3963e-02,\n",
       "                       6.5350e-02, -2.5225e-02, -4.8349e-02,  3.5969e-02, -6.5810e-02,\n",
       "                       4.9832e-02, -1.7676e-02,  7.6724e-02, -2.1050e-02,  9.0036e-02,\n",
       "                       4.1416e-02,  2.7244e-02])),\n",
       "             ('conv_block3.1.running_var',\n",
       "              tensor([0.9397, 0.9339, 0.9420, 0.9365, 0.9446, 0.9586, 0.9382, 0.9422, 0.9373,\n",
       "                      0.9322, 0.9344, 0.9394, 0.9590, 0.9361, 0.9313, 0.9290, 0.9330, 0.9429,\n",
       "                      0.9322, 0.9253, 0.9294, 0.9412, 0.9377, 0.9381, 0.9375, 0.9333, 0.9289,\n",
       "                      0.9310, 0.9292, 0.9331, 0.9469, 0.9227, 0.9294, 0.9347, 0.9241, 0.9372,\n",
       "                      0.9305, 0.9352, 0.9300, 0.9272, 0.9263, 0.9319, 0.9398, 0.9511, 0.9529,\n",
       "                      0.9483, 0.9313, 0.9293, 0.9298, 0.9354, 0.9373, 0.9276, 0.9243, 0.9297,\n",
       "                      0.9685, 0.9225, 0.9393, 0.9285, 0.9409, 0.9372, 0.9298, 0.9376, 0.9329,\n",
       "                      0.9310, 0.9264, 0.9339, 0.9371, 0.9360, 0.9293, 0.9313, 0.9336, 0.9355,\n",
       "                      0.9236, 0.9305, 0.9302, 0.9389, 0.9349, 0.9367, 0.9371, 0.9283, 0.9394,\n",
       "                      0.9309, 0.9263, 0.9303, 0.9418, 0.9444, 0.9303, 0.9303, 0.9501, 0.9280,\n",
       "                      0.9320, 0.9308, 0.9294, 0.9353, 0.9315, 0.9291, 0.9307, 0.9251, 0.9372,\n",
       "                      0.9363, 0.9318, 0.9243, 0.9552, 0.9425, 0.9252, 0.9373, 0.9313, 0.9390,\n",
       "                      0.9313, 0.9365, 0.9479, 0.9472, 0.9383, 0.9403, 0.9289, 0.9495, 0.9332,\n",
       "                      0.9614, 0.9344, 0.9397, 0.9373, 0.9370, 0.9323, 0.9450, 0.9266, 0.9256,\n",
       "                      0.9298, 0.9464, 0.9283, 0.9679, 0.9525, 0.9427, 0.9384, 0.9291, 0.9426,\n",
       "                      0.9369, 0.9327, 0.9338, 0.9391, 0.9412, 0.9356, 0.9343, 0.9270, 0.9318,\n",
       "                      0.9287, 0.9301, 0.9344, 0.9320, 0.9322, 0.9453, 0.9375, 0.9357, 0.9475,\n",
       "                      0.9318, 0.9404, 0.9312, 0.9497, 0.9361, 0.9400, 0.9309, 0.9334, 0.9479,\n",
       "                      0.9330, 0.9556, 0.9294, 0.9300, 0.9347, 0.9381, 0.9307, 0.9350, 0.9510,\n",
       "                      0.9316, 0.9404, 0.9383, 0.9271, 0.9464, 0.9262, 0.9340, 0.9486, 0.9246,\n",
       "                      0.9348, 0.9242, 0.9391, 0.9401, 0.9313, 0.9320, 0.9332, 0.9222, 0.9468,\n",
       "                      0.9334, 0.9318, 0.9314, 0.9402, 0.9261, 0.9363, 0.9349, 0.9381, 0.9471,\n",
       "                      0.9277, 0.9377, 0.9383, 0.9290, 0.9375, 0.9303, 0.9373, 0.9304, 0.9477,\n",
       "                      0.9379, 0.9308, 0.9564, 0.9305, 0.9289, 0.9338, 0.9334, 0.9358, 0.9344,\n",
       "                      0.9365, 0.9255, 0.9388, 0.9331, 0.9291, 0.9356, 0.9302, 0.9342, 0.9384,\n",
       "                      0.9296, 0.9355, 0.9391, 0.9468, 0.9246, 0.9303, 0.9437, 0.9333, 0.9373,\n",
       "                      0.9317, 0.9330, 0.9335, 0.9297, 0.9333, 0.9371, 0.9251, 0.9366, 0.9466,\n",
       "                      0.9467, 0.9380, 0.9224, 0.9567, 0.9390, 0.9321, 0.9373, 0.9394, 0.9369,\n",
       "                      0.9302, 0.9301, 0.9286, 0.9475, 0.9305, 0.9458, 0.9241, 0.9425, 0.9287,\n",
       "                      0.9328, 0.9306, 0.9367, 0.9478, 0.9297, 0.9334, 0.9360, 0.9347, 0.9538,\n",
       "                      0.9349, 0.9365, 0.9246, 0.9291, 0.9403, 0.9319, 0.9288, 0.9420, 0.9388,\n",
       "                      0.9301, 0.9300, 0.9387, 0.9324, 0.9291, 0.9550, 0.9340, 0.9269, 0.9290,\n",
       "                      0.9326, 0.9292, 0.9302, 0.9370, 0.9351, 0.9493, 0.9397, 0.9223, 0.9281,\n",
       "                      0.9371, 0.9372, 0.9351, 0.9332, 0.9306, 0.9329, 0.9353, 0.9490, 0.9273,\n",
       "                      0.9340, 0.9284, 0.9296, 0.9378, 0.9401, 0.9314, 0.9350, 0.9435, 0.9423,\n",
       "                      0.9197, 0.9441, 0.9341, 0.9314, 0.9274, 0.9301, 0.9665, 0.9245, 0.9381,\n",
       "                      0.9314, 0.9669, 0.9346, 0.9474, 0.9469, 0.9384, 0.9552, 0.9265, 0.9402,\n",
       "                      0.9304, 0.9260, 0.9297, 0.9412, 0.9326, 0.9566, 0.9353, 0.9518, 0.9383,\n",
       "                      0.9257, 0.9391, 0.9317, 0.9300, 0.9410, 0.9335, 0.9390, 0.9407, 0.9332,\n",
       "                      0.9396, 0.9399, 0.9298, 0.9429, 0.9284, 0.9368, 0.9238, 0.9402, 0.9521,\n",
       "                      0.9454, 0.9501, 0.9663, 0.9339, 0.9380, 0.9261, 0.9263, 0.9312, 0.9417,\n",
       "                      0.9278, 0.9267, 0.9448, 0.9524, 0.9355, 0.9360, 0.9229, 0.9325, 0.9354,\n",
       "                      0.9342, 0.9354, 0.9286, 0.9351, 0.9270, 0.9416, 0.9326, 0.9451, 0.9423,\n",
       "                      0.9267, 0.9360, 0.9334, 0.9331, 0.9279, 0.9421, 0.9275, 0.9334, 0.9276,\n",
       "                      0.9397, 0.9279, 0.9423, 0.9263, 0.9364, 0.9360, 0.9477, 0.9369, 0.9345,\n",
       "                      0.9321, 0.9438, 0.9379, 0.9349, 0.9266, 0.9536, 0.9432, 0.9385, 0.9301,\n",
       "                      0.9457, 0.9477, 0.9324, 0.9352, 0.9368, 0.9279, 0.9291, 0.9553, 0.9416,\n",
       "                      0.9460, 0.9278, 0.9385, 0.9423, 0.9364, 0.9416, 0.9284, 0.9463, 0.9243,\n",
       "                      0.9442, 0.9260, 0.9542, 0.9295, 0.9408, 0.9463, 0.9394, 0.9419, 0.9459,\n",
       "                      0.9429, 0.9306, 0.9393, 0.9338, 0.9438, 0.9349, 0.9468, 0.9311, 0.9395,\n",
       "                      0.9537, 0.9512, 0.9291, 0.9318, 0.9314, 0.9295, 0.9614, 0.9338, 0.9368,\n",
       "                      0.9336, 0.9385, 0.9309, 0.9340, 0.9280, 0.9357, 0.9287, 0.9303, 0.9301,\n",
       "                      0.9417, 0.9272, 0.9295, 0.9364, 0.9348, 0.9511, 0.9312, 0.9306, 0.9292,\n",
       "                      0.9452, 0.9404, 0.9290, 0.9361, 0.9307, 0.9393, 0.9314, 0.9288, 0.9389,\n",
       "                      0.9360, 0.9448, 0.9354, 0.9721, 0.9358, 0.9348, 0.9347, 0.9585, 0.9242,\n",
       "                      0.9365, 0.9365, 0.9410, 0.9460, 0.9273, 0.9356, 0.9272, 0.9278, 0.9275,\n",
       "                      0.9337, 0.9260, 0.9478, 0.9384, 0.9283, 0.9421, 0.9371, 0.9345])),\n",
       "             ('conv_block3.1.num_batches_tracked', tensor(1)),\n",
       "             ('conv_block3.3.weight',\n",
       "              tensor([[[[ 0.0194,  0.0119, -0.0018],\n",
       "                        [ 0.0052, -0.0173, -0.0197],\n",
       "                        [ 0.0181,  0.0105, -0.0092]],\n",
       "              \n",
       "                       [[ 0.0171,  0.0045, -0.0175],\n",
       "                        [ 0.0208, -0.0017, -0.0100],\n",
       "                        [-0.0013, -0.0091,  0.0197]],\n",
       "              \n",
       "                       [[ 0.0011, -0.0042,  0.0051],\n",
       "                        [ 0.0077,  0.0145, -0.0043],\n",
       "                        [ 0.0042,  0.0048, -0.0136]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0233,  0.0236, -0.0137],\n",
       "                        [-0.0223, -0.0220, -0.0019],\n",
       "                        [-0.0190,  0.0213, -0.0214]],\n",
       "              \n",
       "                       [[ 0.0201, -0.0060, -0.0196],\n",
       "                        [ 0.0129,  0.0235, -0.0036],\n",
       "                        [ 0.0111,  0.0177, -0.0009]],\n",
       "              \n",
       "                       [[ 0.0182,  0.0073,  0.0108],\n",
       "                        [ 0.0091, -0.0152,  0.0217],\n",
       "                        [ 0.0135,  0.0132,  0.0186]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0238, -0.0126,  0.0145],\n",
       "                        [ 0.0252, -0.0122,  0.0129],\n",
       "                        [-0.0034, -0.0226,  0.0222]],\n",
       "              \n",
       "                       [[ 0.0103,  0.0071, -0.0128],\n",
       "                        [-0.0133, -0.0124,  0.0071],\n",
       "                        [ 0.0167,  0.0027,  0.0161]],\n",
       "              \n",
       "                       [[-0.0111,  0.0123,  0.0176],\n",
       "                        [-0.0151, -0.0078,  0.0039],\n",
       "                        [-0.0054, -0.0147,  0.0239]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0175,  0.0126, -0.0019],\n",
       "                        [-0.0186,  0.0065, -0.0107],\n",
       "                        [-0.0094, -0.0212,  0.0134]],\n",
       "              \n",
       "                       [[-0.0134, -0.0232,  0.0162],\n",
       "                        [ 0.0232, -0.0033,  0.0201],\n",
       "                        [-0.0170, -0.0027, -0.0001]],\n",
       "              \n",
       "                       [[ 0.0188, -0.0128,  0.0213],\n",
       "                        [ 0.0177, -0.0103,  0.0083],\n",
       "                        [ 0.0200,  0.0023,  0.0002]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0227, -0.0251,  0.0251],\n",
       "                        [-0.0117, -0.0141,  0.0244],\n",
       "                        [-0.0151,  0.0154,  0.0210]],\n",
       "              \n",
       "                       [[ 0.0020,  0.0074,  0.0108],\n",
       "                        [ 0.0123,  0.0062, -0.0151],\n",
       "                        [ 0.0084, -0.0210, -0.0135]],\n",
       "              \n",
       "                       [[ 0.0106, -0.0225, -0.0035],\n",
       "                        [-0.0085, -0.0244,  0.0073],\n",
       "                        [ 0.0096,  0.0174,  0.0055]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0223, -0.0222,  0.0168],\n",
       "                        [ 0.0050,  0.0215,  0.0001],\n",
       "                        [-0.0104, -0.0012,  0.0137]],\n",
       "              \n",
       "                       [[-0.0234, -0.0112, -0.0144],\n",
       "                        [-0.0130,  0.0036, -0.0079],\n",
       "                        [ 0.0215,  0.0008,  0.0035]],\n",
       "              \n",
       "                       [[-0.0163,  0.0087,  0.0026],\n",
       "                        [ 0.0091,  0.0014, -0.0137],\n",
       "                        [-0.0019,  0.0175, -0.0177]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0058, -0.0184,  0.0190],\n",
       "                        [-0.0142,  0.0113, -0.0139],\n",
       "                        [ 0.0132,  0.0055,  0.0163]],\n",
       "              \n",
       "                       [[-0.0087,  0.0225, -0.0156],\n",
       "                        [-0.0209, -0.0098, -0.0059],\n",
       "                        [-0.0213,  0.0175, -0.0050]],\n",
       "              \n",
       "                       [[-0.0248, -0.0220, -0.0079],\n",
       "                        [ 0.0160, -0.0064,  0.0031],\n",
       "                        [ 0.0182,  0.0153,  0.0085]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0254, -0.0211,  0.0024],\n",
       "                        [-0.0046,  0.0031,  0.0139],\n",
       "                        [-0.0050,  0.0036, -0.0035]],\n",
       "              \n",
       "                       [[ 0.0035, -0.0196,  0.0222],\n",
       "                        [-0.0118,  0.0199, -0.0249],\n",
       "                        [ 0.0053,  0.0154,  0.0212]],\n",
       "              \n",
       "                       [[-0.0243,  0.0205,  0.0098],\n",
       "                        [ 0.0077, -0.0106,  0.0021],\n",
       "                        [-0.0022, -0.0238, -0.0189]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0198, -0.0038,  0.0117],\n",
       "                        [ 0.0071, -0.0072, -0.0231],\n",
       "                        [-0.0230,  0.0086, -0.0130]],\n",
       "              \n",
       "                       [[-0.0219,  0.0236,  0.0114],\n",
       "                        [ 0.0237,  0.0007,  0.0135],\n",
       "                        [ 0.0245, -0.0151,  0.0057]],\n",
       "              \n",
       "                       [[-0.0170, -0.0229,  0.0113],\n",
       "                        [-0.0100, -0.0172, -0.0113],\n",
       "                        [-0.0038,  0.0099,  0.0080]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0191, -0.0239,  0.0199],\n",
       "                        [ 0.0035, -0.0083, -0.0252],\n",
       "                        [ 0.0185,  0.0158,  0.0252]],\n",
       "              \n",
       "                       [[-0.0111,  0.0047,  0.0042],\n",
       "                        [-0.0141,  0.0126,  0.0120],\n",
       "                        [ 0.0052, -0.0051,  0.0094]],\n",
       "              \n",
       "                       [[-0.0077,  0.0215,  0.0034],\n",
       "                        [ 0.0057,  0.0245,  0.0069],\n",
       "                        [ 0.0075, -0.0043,  0.0119]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0223,  0.0118, -0.0153],\n",
       "                        [ 0.0096, -0.0205, -0.0041],\n",
       "                        [-0.0242,  0.0070,  0.0191]],\n",
       "              \n",
       "                       [[-0.0102, -0.0120, -0.0016],\n",
       "                        [-0.0109,  0.0210, -0.0155],\n",
       "                        [-0.0165, -0.0028, -0.0053]],\n",
       "              \n",
       "                       [[-0.0058,  0.0067, -0.0224],\n",
       "                        [-0.0240, -0.0045, -0.0156],\n",
       "                        [-0.0017, -0.0209,  0.0113]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0155, -0.0036,  0.0101],\n",
       "                        [-0.0135,  0.0218,  0.0215],\n",
       "                        [ 0.0180, -0.0002,  0.0044]],\n",
       "              \n",
       "                       [[-0.0004,  0.0169, -0.0184],\n",
       "                        [-0.0242, -0.0015, -0.0047],\n",
       "                        [-0.0044, -0.0118, -0.0080]],\n",
       "              \n",
       "                       [[-0.0157,  0.0104, -0.0045],\n",
       "                        [ 0.0035, -0.0244,  0.0067],\n",
       "                        [ 0.0037, -0.0235, -0.0039]]]])),\n",
       "             ('conv_block3.3.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block3.4.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('conv_block3.4.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('conv_block3.4.running_mean',\n",
       "              tensor([ 6.7479e-02,  4.3253e-03,  4.3642e-03, -1.9937e-02, -3.6028e-02,\n",
       "                       6.1359e-02,  4.9363e-02,  2.1815e-02, -2.1159e-02,  4.0847e-03,\n",
       "                      -3.6798e-02,  3.4254e-02, -3.8743e-03,  5.6435e-02, -5.0198e-04,\n",
       "                       1.0645e-02, -3.2735e-02,  2.1073e-02, -1.4160e-02,  8.0055e-03,\n",
       "                      -5.9940e-02, -1.3768e-02, -3.4583e-02,  3.1113e-02, -3.4697e-02,\n",
       "                       5.2303e-02,  2.6148e-02, -3.0724e-02, -1.6972e-02, -1.4889e-02,\n",
       "                      -5.3096e-02,  1.4524e-02,  1.3709e-02,  1.3141e-02, -1.8265e-02,\n",
       "                      -1.4523e-02, -4.7122e-02, -1.3278e-02,  1.0194e-02,  1.6305e-02,\n",
       "                      -3.9035e-02, -4.0938e-02,  3.3350e-02,  5.5068e-02, -3.4172e-02,\n",
       "                       5.3667e-02, -1.4069e-02,  2.1243e-02, -4.2283e-02, -2.3245e-02,\n",
       "                       6.1839e-04, -2.5608e-02,  2.6294e-02, -4.0833e-02, -2.2004e-02,\n",
       "                       2.7674e-02,  1.7502e-02, -2.2983e-02, -5.3803e-03,  6.1954e-02,\n",
       "                       6.5291e-02, -1.5098e-03, -7.1275e-02,  1.4921e-02, -3.4702e-02,\n",
       "                       5.0987e-02, -1.8667e-02, -3.9465e-02, -4.6615e-02,  2.8761e-02,\n",
       "                      -5.0829e-03, -3.7690e-02,  3.3944e-02, -4.0571e-02, -5.0623e-02,\n",
       "                       4.7012e-02,  4.0329e-02,  1.8934e-02,  8.8048e-03,  3.1787e-03,\n",
       "                       9.1712e-03,  6.1377e-02,  4.5843e-02,  3.8536e-02,  6.0226e-02,\n",
       "                      -2.8343e-02, -4.3105e-02,  7.5321e-03, -4.0039e-05, -1.7845e-02,\n",
       "                      -1.1303e-02, -2.0460e-02, -2.6206e-02,  4.8477e-02, -1.3212e-02,\n",
       "                       2.4998e-02,  6.0610e-02, -1.0464e-02,  5.8428e-02, -4.1956e-02,\n",
       "                       1.2559e-02,  3.8655e-02, -3.0295e-02, -5.3792e-02, -2.8173e-02,\n",
       "                      -2.5982e-02,  1.2649e-02,  1.2321e-02,  4.7971e-02, -2.3430e-02,\n",
       "                      -2.6435e-03, -5.6764e-02, -1.4331e-02, -2.0840e-02, -2.3836e-02,\n",
       "                       3.0214e-02,  1.1683e-02,  4.2940e-03,  6.7364e-03,  1.0044e-02,\n",
       "                       7.5654e-03,  2.1751e-03, -5.0715e-02, -1.2366e-02,  2.0197e-03,\n",
       "                      -1.5137e-02,  5.9588e-02, -2.0900e-02, -2.7827e-02, -5.0973e-03,\n",
       "                       1.8659e-02,  4.4987e-02,  1.2933e-02,  3.9569e-02,  2.0989e-02,\n",
       "                       3.9862e-02,  1.9073e-02,  2.3369e-02, -7.2276e-02, -2.4558e-03,\n",
       "                      -2.5120e-02, -1.3759e-02, -4.0529e-02,  6.5526e-02,  7.3967e-02,\n",
       "                       3.0491e-02,  3.7314e-02,  3.5401e-02, -4.2610e-03, -2.2009e-02,\n",
       "                      -3.2816e-02, -2.0771e-02, -2.4223e-02,  9.8766e-03,  2.0353e-02,\n",
       "                       2.5103e-02, -1.4766e-03, -2.9360e-02, -7.8064e-02,  2.8681e-02,\n",
       "                       2.1111e-03,  1.2418e-02,  3.3241e-03,  5.7731e-02,  2.9570e-02,\n",
       "                      -6.0675e-02, -5.0230e-03, -2.1528e-02, -7.2881e-02,  3.1617e-02,\n",
       "                      -3.2931e-03, -1.6129e-02,  3.4285e-02, -2.6264e-02, -3.4751e-02,\n",
       "                       5.3258e-02,  3.6366e-02, -1.3785e-02,  5.1198e-02, -4.0387e-02,\n",
       "                       1.9687e-02, -4.1908e-02,  1.5296e-02,  1.3458e-02,  3.9578e-02,\n",
       "                      -5.5980e-02,  7.9245e-03, -4.5870e-02, -1.4207e-02,  1.8944e-02,\n",
       "                      -2.5526e-03, -6.2869e-03, -6.4824e-03,  3.4039e-02, -2.9498e-02,\n",
       "                      -1.2918e-02,  1.5223e-02, -2.3140e-02, -4.7806e-03,  3.1259e-02,\n",
       "                      -2.0347e-02,  1.0092e-02,  2.7065e-02,  7.7469e-02, -4.1460e-03,\n",
       "                      -4.0177e-02,  3.0908e-02, -1.8132e-02, -2.9018e-02, -1.2867e-02,\n",
       "                       1.5933e-02, -3.2416e-02, -6.5614e-03,  4.0181e-02, -1.3049e-02,\n",
       "                      -2.9898e-02, -3.5790e-02,  1.4916e-03, -2.8622e-02,  1.6486e-02,\n",
       "                      -5.7252e-02,  1.1283e-02,  4.2590e-02,  2.4617e-02, -2.1710e-02,\n",
       "                       2.0390e-02,  7.0003e-03,  3.5036e-02, -3.6803e-03, -5.4104e-03,\n",
       "                       2.4368e-02, -2.4067e-02, -4.3185e-03, -4.2975e-02, -1.6926e-02,\n",
       "                       1.6371e-02, -2.0229e-02, -6.4415e-02, -1.9064e-02,  2.5531e-02,\n",
       "                      -8.6884e-02,  1.2830e-02,  6.3555e-03, -4.7564e-02,  7.2424e-03,\n",
       "                       9.0776e-03,  6.4953e-03,  1.7193e-03,  1.2446e-02,  2.2816e-02,\n",
       "                      -1.2712e-02,  2.9133e-02,  3.0667e-03, -1.9530e-02,  1.0389e-02,\n",
       "                      -4.9900e-03,  1.6555e-03,  1.4034e-02,  5.2983e-02,  7.3766e-03,\n",
       "                      -6.1507e-02, -9.9080e-03,  3.4700e-03, -3.0653e-02,  2.4367e-02,\n",
       "                      -5.4365e-03, -3.8415e-03,  5.9057e-03, -9.6889e-03, -2.1049e-02,\n",
       "                       1.7675e-02,  4.4832e-02,  5.1081e-02, -2.4507e-03, -4.9793e-02,\n",
       "                       3.5733e-02, -6.6816e-02,  1.9370e-02,  6.7938e-02, -5.1219e-02,\n",
       "                      -2.7755e-02,  1.9392e-02, -3.1333e-02, -3.1920e-02, -1.8785e-02,\n",
       "                      -2.8661e-03,  7.0196e-02, -2.1495e-03, -3.0004e-02, -6.9826e-02,\n",
       "                       3.3017e-02,  1.0741e-02, -1.3396e-02, -3.2965e-02,  2.2256e-02,\n",
       "                      -1.8285e-02,  2.1792e-02,  2.0927e-02,  4.2014e-03, -9.9974e-02,\n",
       "                      -1.6942e-02,  2.7972e-02,  3.9927e-02,  3.8685e-03,  2.9443e-03,\n",
       "                      -3.4205e-03,  1.5186e-02,  1.2825e-02, -5.7825e-03,  1.2866e-02,\n",
       "                      -1.3534e-02,  2.5470e-02, -1.9809e-02,  3.0457e-03, -3.9933e-02,\n",
       "                       6.7709e-03, -1.2444e-02, -1.1413e-02, -1.2643e-02,  2.3777e-02,\n",
       "                      -2.4496e-03,  1.7169e-02, -3.5601e-02,  7.0511e-03,  4.9559e-02,\n",
       "                      -1.7218e-02, -1.1951e-02,  2.7957e-02, -8.0274e-03,  2.7830e-02,\n",
       "                       2.5684e-02, -2.4259e-03, -2.4672e-02,  2.8480e-02,  4.8312e-02,\n",
       "                       1.1831e-02, -1.2698e-02,  2.4304e-02, -6.2647e-03, -1.8276e-02,\n",
       "                       1.5040e-03,  1.6223e-02, -7.7650e-02,  9.6972e-03,  3.0820e-02,\n",
       "                       2.1328e-02, -2.4373e-02, -1.6803e-03,  4.8775e-02,  2.9067e-02,\n",
       "                      -2.8909e-02, -1.8935e-02, -3.9398e-02,  9.8579e-04, -2.8660e-02,\n",
       "                      -3.7138e-03, -2.5025e-02,  1.1477e-03,  1.8338e-02, -1.1418e-02,\n",
       "                       1.2145e-02,  3.1035e-02,  1.2180e-02, -1.1923e-02,  2.4594e-02,\n",
       "                       2.6618e-02, -7.5123e-02, -2.3873e-03, -5.5652e-02, -3.5533e-03,\n",
       "                       2.4659e-02,  3.2066e-02, -5.1133e-02,  1.8728e-02, -3.9648e-02,\n",
       "                      -6.1589e-02,  3.8383e-02,  7.3284e-02, -1.0511e-02, -1.1361e-02,\n",
       "                      -1.0157e-02, -2.2472e-02,  6.2940e-03, -2.9397e-02, -3.1961e-02,\n",
       "                       6.6887e-03,  4.5227e-03, -3.2983e-02,  5.5421e-03, -1.1234e-02,\n",
       "                       2.7295e-02,  1.7225e-02, -4.9920e-04,  1.7052e-02, -1.8765e-02,\n",
       "                      -2.1195e-03, -9.7311e-03, -1.7176e-03,  2.9275e-03, -2.0446e-02,\n",
       "                       2.5471e-02, -3.6105e-03,  3.4351e-02,  5.3430e-02,  3.4859e-03,\n",
       "                       1.8181e-02,  1.2453e-02, -3.7204e-02,  2.8240e-02, -2.2407e-02,\n",
       "                      -5.3398e-02,  5.1151e-03, -6.8713e-02,  6.3246e-03, -4.2876e-03,\n",
       "                       1.8113e-02, -4.4998e-02,  1.1687e-02,  2.2759e-02,  7.7121e-04,\n",
       "                      -1.0911e-02, -2.6160e-02, -1.6698e-02, -4.4922e-02, -5.4182e-02,\n",
       "                      -4.0014e-02,  4.6571e-02, -7.2693e-03,  1.2666e-02, -2.0414e-02,\n",
       "                       1.9609e-02, -1.8525e-02,  2.9083e-02, -1.9180e-02,  4.7024e-02,\n",
       "                      -1.0999e-02, -2.4071e-02, -4.1538e-02,  1.6103e-02,  3.8004e-02,\n",
       "                      -3.0760e-02,  3.8923e-02,  1.5476e-02, -1.4551e-02,  9.6958e-03,\n",
       "                       9.8118e-04, -2.8149e-02, -3.9302e-02,  3.0271e-02,  1.6636e-02,\n",
       "                      -9.5048e-03, -4.0173e-02,  1.7971e-02,  5.8497e-02, -1.1685e-02,\n",
       "                      -1.6178e-02,  1.5917e-02, -7.6087e-03,  1.2646e-02,  2.6113e-02,\n",
       "                      -1.8686e-02,  8.2270e-03,  1.3516e-02, -4.1341e-04, -4.8665e-02,\n",
       "                       1.0586e-03,  5.0478e-02,  4.0017e-03, -1.4690e-03, -3.7774e-02,\n",
       "                       7.1633e-03, -4.7128e-02, -1.9298e-02, -3.1306e-02,  2.0303e-02,\n",
       "                       1.8207e-02, -3.6612e-02, -7.9774e-03,  3.0448e-03,  3.7266e-02,\n",
       "                      -1.6963e-03,  2.3917e-02,  5.0908e-02,  4.4671e-02, -4.1033e-02,\n",
       "                      -3.3956e-02, -2.6438e-02, -4.6790e-02,  7.8890e-03,  1.5388e-03,\n",
       "                      -4.3977e-02, -2.3844e-02, -5.3614e-02, -3.3281e-02, -8.1227e-03,\n",
       "                      -2.1288e-02, -1.8632e-02,  1.8913e-02, -3.6352e-03,  1.3918e-02,\n",
       "                      -1.8535e-02,  3.8229e-02, -1.1787e-02, -2.3344e-02,  2.2921e-02,\n",
       "                       7.5393e-02, -1.6639e-02, -5.5944e-02,  2.8509e-02, -4.1195e-02,\n",
       "                       7.7461e-02,  1.7137e-02])),\n",
       "             ('conv_block3.4.running_var',\n",
       "              tensor([0.9336, 0.9301, 0.9323, 0.9284, 0.9417, 0.9299, 0.9304, 0.9253, 0.9284,\n",
       "                      0.9302, 0.9284, 0.9376, 0.9316, 0.9290, 0.9248, 0.9324, 0.9450, 0.9307,\n",
       "                      0.9297, 0.9274, 0.9342, 0.9296, 0.9351, 0.9309, 0.9299, 0.9318, 0.9350,\n",
       "                      0.9296, 0.9330, 0.9297, 0.9302, 0.9257, 0.9257, 0.9298, 0.9284, 0.9319,\n",
       "                      0.9316, 0.9369, 0.9294, 0.9259, 0.9263, 0.9265, 0.9249, 0.9300, 0.9410,\n",
       "                      0.9321, 0.9279, 0.9270, 0.9289, 0.9311, 0.9329, 0.9292, 0.9311, 0.9416,\n",
       "                      0.9286, 0.9329, 0.9293, 0.9305, 0.9252, 0.9256, 0.9347, 0.9280, 0.9273,\n",
       "                      0.9326, 0.9288, 0.9309, 0.9315, 0.9254, 0.9356, 0.9356, 0.9266, 0.9272,\n",
       "                      0.9358, 0.9299, 0.9251, 0.9282, 0.9316, 0.9342, 0.9250, 0.9245, 0.9369,\n",
       "                      0.9250, 0.9409, 0.9307, 0.9264, 0.9271, 0.9260, 0.9252, 0.9350, 0.9253,\n",
       "                      0.9389, 0.9284, 0.9338, 0.9238, 0.9288, 0.9297, 0.9287, 0.9307, 0.9322,\n",
       "                      0.9362, 0.9243, 0.9317, 0.9279, 0.9259, 0.9314, 0.9265, 0.9266, 0.9285,\n",
       "                      0.9456, 0.9340, 0.9312, 0.9298, 0.9520, 0.9253, 0.9322, 0.9352, 0.9317,\n",
       "                      0.9323, 0.9309, 0.9287, 0.9260, 0.9319, 0.9399, 0.9289, 0.9250, 0.9249,\n",
       "                      0.9339, 0.9259, 0.9307, 0.9292, 0.9282, 0.9254, 0.9434, 0.9306, 0.9322,\n",
       "                      0.9306, 0.9312, 0.9266, 0.9386, 0.9350, 0.9300, 0.9298, 0.9402, 0.9276,\n",
       "                      0.9285, 0.9338, 0.9379, 0.9252, 0.9272, 0.9296, 0.9367, 0.9316, 0.9380,\n",
       "                      0.9259, 0.9307, 0.9275, 0.9390, 0.9327, 0.9316, 0.9277, 0.9253, 0.9293,\n",
       "                      0.9262, 0.9328, 0.9293, 0.9350, 0.9298, 0.9266, 0.9301, 0.9258, 0.9262,\n",
       "                      0.9277, 0.9310, 0.9300, 0.9423, 0.9312, 0.9296, 0.9280, 0.9418, 0.9351,\n",
       "                      0.9263, 0.9241, 0.9414, 0.9318, 0.9239, 0.9429, 0.9280, 0.9332, 0.9384,\n",
       "                      0.9324, 0.9297, 0.9358, 0.9351, 0.9316, 0.9333, 0.9236, 0.9284, 0.9261,\n",
       "                      0.9277, 0.9331, 0.9351, 0.9417, 0.9280, 0.9412, 0.9252, 0.9240, 0.9388,\n",
       "                      0.9290, 0.9338, 0.9267, 0.9362, 0.9310, 0.9297, 0.9287, 0.9298, 0.9287,\n",
       "                      0.9264, 0.9287, 0.9245, 0.9282, 0.9299, 0.9275, 0.9290, 0.9292, 0.9349,\n",
       "                      0.9325, 0.9281, 0.9370, 0.9330, 0.9355, 0.9265, 0.9280, 0.9278, 0.9322,\n",
       "                      0.9380, 0.9329, 0.9319, 0.9320, 0.9321, 0.9254, 0.9437, 0.9279, 0.9360,\n",
       "                      0.9334, 0.9284, 0.9308, 0.9329, 0.9238, 0.9327, 0.9393, 0.9269, 0.9327,\n",
       "                      0.9370, 0.9285, 0.9289, 0.9393, 0.9280, 0.9257, 0.9271, 0.9269, 0.9528,\n",
       "                      0.9273, 0.9281, 0.9241, 0.9266, 0.9347, 0.9254, 0.9361, 0.9351, 0.9390,\n",
       "                      0.9266, 0.9244, 0.9395, 0.9245, 0.9350, 0.9305, 0.9301, 0.9318, 0.9431,\n",
       "                      0.9349, 0.9322, 0.9325, 0.9289, 0.9216, 0.9343, 0.9308, 0.9320, 0.9423,\n",
       "                      0.9336, 0.9318, 0.9267, 0.9282, 0.9288, 0.9292, 0.9274, 0.9269, 0.9269,\n",
       "                      0.9255, 0.9309, 0.9437, 0.9283, 0.9360, 0.9266, 0.9301, 0.9274, 0.9313,\n",
       "                      0.9279, 0.9317, 0.9279, 0.9271, 0.9282, 0.9274, 0.9354, 0.9264, 0.9250,\n",
       "                      0.9337, 0.9283, 0.9292, 0.9251, 0.9289, 0.9233, 0.9292, 0.9323, 0.9279,\n",
       "                      0.9273, 0.9293, 0.9292, 0.9305, 0.9285, 0.9344, 0.9272, 0.9327, 0.9271,\n",
       "                      0.9268, 0.9229, 0.9314, 0.9340, 0.9299, 0.9241, 0.9264, 0.9249, 0.9263,\n",
       "                      0.9298, 0.9287, 0.9271, 0.9310, 0.9386, 0.9273, 0.9319, 0.9335, 0.9325,\n",
       "                      0.9254, 0.9266, 0.9258, 0.9352, 0.9266, 0.9282, 0.9310, 0.9238, 0.9257,\n",
       "                      0.9371, 0.9291, 0.9331, 0.9298, 0.9305, 0.9322, 0.9371, 0.9335, 0.9312,\n",
       "                      0.9338, 0.9281, 0.9290, 0.9297, 0.9263, 0.9268, 0.9304, 0.9345, 0.9471,\n",
       "                      0.9291, 0.9278, 0.9241, 0.9277, 0.9289, 0.9323, 0.9307, 0.9240, 0.9275,\n",
       "                      0.9350, 0.9360, 0.9247, 0.9276, 0.9330, 0.9305, 0.9262, 0.9310, 0.9330,\n",
       "                      0.9314, 0.9351, 0.9239, 0.9256, 0.9316, 0.9284, 0.9321, 0.9297, 0.9279,\n",
       "                      0.9340, 0.9335, 0.9376, 0.9345, 0.9317, 0.9365, 0.9278, 0.9274, 0.9278,\n",
       "                      0.9504, 0.9302, 0.9302, 0.9289, 0.9317, 0.9233, 0.9317, 0.9266, 0.9292,\n",
       "                      0.9321, 0.9399, 0.9301, 0.9317, 0.9280, 0.9278, 0.9261, 0.9319, 0.9371,\n",
       "                      0.9284, 0.9332, 0.9277, 0.9318, 0.9317, 0.9276, 0.9257, 0.9290, 0.9338,\n",
       "                      0.9323, 0.9316, 0.9259, 0.9354, 0.9361, 0.9331, 0.9444, 0.9306, 0.9358,\n",
       "                      0.9256, 0.9301, 0.9265, 0.9281, 0.9275, 0.9325, 0.9279, 0.9273, 0.9380,\n",
       "                      0.9393, 0.9251, 0.9313, 0.9361, 0.9372, 0.9312, 0.9382, 0.9302, 0.9250,\n",
       "                      0.9261, 0.9279, 0.9246, 0.9408, 0.9292, 0.9304, 0.9265, 0.9288, 0.9237,\n",
       "                      0.9281, 0.9293, 0.9269, 0.9290, 0.9237, 0.9335, 0.9258, 0.9364, 0.9244,\n",
       "                      0.9302, 0.9283, 0.9328, 0.9251, 0.9244, 0.9245, 0.9274, 0.9295, 0.9296,\n",
       "                      0.9293, 0.9269, 0.9290, 0.9316, 0.9324, 0.9280, 0.9260, 0.9284, 0.9262,\n",
       "                      0.9294, 0.9368, 0.9253, 0.9309, 0.9319, 0.9277, 0.9348, 0.9280])),\n",
       "             ('conv_block3.4.num_batches_tracked', tensor(1)),\n",
       "             ('classifier.0.weight',\n",
       "              tensor([[-0.0078,  0.0036,  0.0122,  ..., -0.0045, -0.0057,  0.0132],\n",
       "                      [-0.0131, -0.0080,  0.0155,  ...,  0.0036, -0.0242, -0.0128],\n",
       "                      [ 0.0050, -0.0035, -0.0214,  ..., -0.0222,  0.0043,  0.0009],\n",
       "                      ...,\n",
       "                      [-0.0159,  0.0097,  0.0063,  ..., -0.0202,  0.0051, -0.0045],\n",
       "                      [ 0.0201, -0.0168, -0.0192,  ..., -0.0222,  0.0126, -0.0210],\n",
       "                      [-0.0214,  0.0194,  0.0080,  ...,  0.0136,  0.0083, -0.0239]])),\n",
       "             ('classifier.0.bias', tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('classifier.2.weight',\n",
       "              tensor([[-0.0641,  0.0063,  0.0385,  ...,  0.0191, -0.0405, -0.0663],\n",
       "                      [-0.0578, -0.0053, -0.0288,  ...,  0.0571, -0.0315,  0.0274],\n",
       "                      [-0.0633,  0.0719,  0.0012,  ...,  0.0556,  0.0004,  0.0137],\n",
       "                      ...,\n",
       "                      [-0.0715, -0.0145, -0.0610,  ..., -0.0315,  0.0336,  0.0422],\n",
       "                      [-0.0198, -0.0104,  0.0453,  ...,  0.0586, -0.0522,  0.0330],\n",
       "                      [ 0.0734,  0.0757,  0.0223,  ..., -0.0410, -0.0170, -0.0543]])),\n",
       "             ('classifier.2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置交叉熵损失函数，SGD优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数: CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()  # 交叉熵损失函数，适用于多分类问题，里边会做softmax，还有会把0-9标签转换成one-hot编码\n",
    "\n",
    "print(\"损失函数:\", loss_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.197757Z",
     "start_time": "2025-07-02T02:03:04.197757Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # SGD优化器，学习率为0.01，动量为0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.198757Z",
     "start_time": "2025-07-02T02:03:04.198757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "训练开始，共35200步\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f220ed210d48c68b6cd79873789f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      4\u001B[39m early_stopping=EarlyStopping(patience=\u001B[32m5\u001B[39m, delta=\u001B[32m0.001\u001B[39m)\n\u001B[32m      5\u001B[39m model_saver=ModelSaver(save_dir=\u001B[33m'\u001B[39m\u001B[33mmodel_weights\u001B[39m\u001B[33m'\u001B[39m, save_best_only=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m model, history = \u001B[43mtrain_classification_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_saver\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_saver\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensorboard_logger\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32md:\\python11\\day31\\chapter6-AI\\wangdao_deeplearning_train.py:210\u001B[39m, in \u001B[36mtrain_classification_model\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, device, num_epochs, tensorboard_logger, model_saver, early_stopping, eval_step)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;66;03m# 评估\u001B[39;00m\n\u001B[32m    209\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m global_step % eval_step == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m     val_acc, val_loss = \u001B[43mevaluate_classification_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    211\u001B[39m     record_dict[\u001B[33m\"\u001B[39m\u001B[33mval\u001B[39m\u001B[33m\"\u001B[39m].append({\n\u001B[32m    212\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m\"\u001B[39m: val_loss, \u001B[33m\"\u001B[39m\u001B[33macc\u001B[39m\u001B[33m\"\u001B[39m: val_acc, \u001B[33m\"\u001B[39m\u001B[33mstep\u001B[39m\u001B[33m\"\u001B[39m: global_step\n\u001B[32m    213\u001B[39m     })\n\u001B[32m    214\u001B[39m     model.train()  \u001B[38;5;66;03m# 切换回训练集模式\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32md:\\python11\\day31\\chapter6-AI\\wangdao_deeplearning_train.py:116\u001B[39m, in \u001B[36mevaluate_classification_model\u001B[39m\u001B[34m(model, data_loader, device, criterion)\u001B[39m\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m data_loader:\n\u001B[32m    115\u001B[39m     images, labels = images.to(device), labels.to(device)\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    117\u001B[39m     _, predicted = torch.max(outputs.data, \u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# torch.max(outputs.data, 1)返回两个值，第一个是最大值，第二个是最大值的索引\u001B[39;00m\n\u001B[32m    118\u001B[39m     total += labels.size(\u001B[32m0\u001B[39m)  \u001B[38;5;66;03m# labels.size(0)返回标签的维度，这里返回的是batch_size，因为每个批次有batch_size个标签\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 61\u001B[39m, in \u001B[36mNeuralNetwork.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     60\u001B[39m     \u001B[38;5;66;03m# 前向传播使用Sequential定义的块\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconv_block1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     62\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.conv_block2(x)\n\u001B[32m     63\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.conv_block3(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[32m    239\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m         \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "model = model.to(device) #将模型移动到GPU\n",
    "early_stopping=EarlyStopping(patience=5, delta=0.001)\n",
    "model_saver=ModelSaver(save_dir='model_weights', save_best_only=True)\n",
    "\n",
    "\n",
    "model, history = train_classification_model(model, train_loader, val_loader, loss_fn, optimizer, device, num_epochs=50, early_stopping=early_stopping, model_saver=model_saver, tensorboard_logger=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['train'][-100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['val'][-1000:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制损失曲线和准确率曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.202021Z",
     "start_time": "2025-07-02T02:03:04.202021Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_learning_curves(history, sample_step=500)  #横坐标是 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:03:04.204035Z",
     "start_time": "2025-07-02T02:03:04.204035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在预测测试集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "预测进度:   0%|          | 0/2344 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "\n",
    "# 定义测试数据集类\n",
    "class CIFAR10TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        初始化测试数据集\n",
    "        \n",
    "        参数:\n",
    "            img_dir: 测试图片目录\n",
    "            transform: 图像预处理变换\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # 提取图像ID（文件名去掉扩展名）\n",
    "        img_id = int(os.path.splitext(self.img_files[idx])[0])\n",
    "        \n",
    "        return image, img_id\n",
    "\n",
    "# 定义预测函数\n",
    "def predict_test_set(model, img_dir, labels_file, device, batch_size=64):\n",
    "    \"\"\"\n",
    "    预测测试集并生成提交文件\n",
    "    \n",
    "    参数:\n",
    "        model: 训练好的模型\n",
    "        img_dir: 测试图片目录\n",
    "        labels_file: 提交模板文件路径\n",
    "        device: 计算设备\n",
    "        batch_size: 批处理大小\n",
    "    \"\"\"\n",
    "    # 图像预处理变换（与训练集相同）\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4917, 0.4823, 0.4467), (0.2024, 0.1995, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_dataset = CIFAR10TestDataset(img_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 读取提交模板\n",
    "    submission_df = pd.read_csv(labels_file)\n",
    "    predictions = {}\n",
    "    \n",
    "    # 使用tqdm显示进度条\n",
    "    print(\"正在预测测试集...\")\n",
    "    with torch.no_grad():\n",
    "        for images, img_ids in tqdm.tqdm(test_loader, desc=\"预测进度\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1) #取最大的索引，作为预测结果 \n",
    "            \n",
    "            # 记录每个图像的预测结果\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                predictions[img_id.item()] = predicted[i].item() #因为一个批次有多个图像，所以需要predicted[i]\n",
    "    \n",
    "    # 定义类别名称\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # 将数值标签转换为类别名称\n",
    "    labeled_predictions = {img_id: class_names[pred] for img_id, pred in predictions.items()}\n",
    "    \n",
    "    # 直接创建DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': list(labeled_predictions.keys()),\n",
    "        'label': list(labeled_predictions.values())\n",
    "    })\n",
    "    \n",
    "    # 按id列排序\n",
    "    submission_df = submission_df.sort_values(by='id')\n",
    "    \n",
    "    # 检查id列是否有重复值\n",
    "    has_duplicates = submission_df['id'].duplicated().any()\n",
    "    print(f\"id列是否有重复值: {has_duplicates}\")\n",
    "    \n",
    "    # 保存预测结果\n",
    "    output_file = 'cifar10_submission.csv'\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"预测完成，结果已保存至 {output_file}\")\n",
    "\n",
    "# 执行测试集预测\n",
    "img_dir = r\"D:\\cifar-10\\test\\test\"\n",
    "labels_file = r\"D:\\cifar-10\\sampleSubmission.csv\"\n",
    "predict_test_set(model, img_dir, labels_file, device, batch_size=128)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
