### Spark SQL

- spark中用于处理结构化数据的一个模块 (功能跟hive类似，最早shark, 为了解决hive速度慢的问题)

- Spark SQL 和 Spark Core的关系（RDD）与 hive 和 MapReduce关系类似

- Spark SQL 优势 

  - 代码少
  - spark SQL操作数据的两种方式
    - 通过sql语句
    - 自带了DataFrame的API
  - 速度快（对比直接写RDD代码）
    - SparkSQL API 转换成RDD的时候会做执行优化
    - 优化引擎转化的RDD代码比自己写的效率更高
  - DataFrame 在操作结构化数据的时候
    - 引入了schema（表结构）
    - off-heap 

  

- 为什么使用SparkSQL
  - **代码少**
  - **速度快**
  - **可以直接连接数据库**
  - **兼容hive**

### Spark SQL DataFrame介绍

- DataFrame是一个分布式的**行集合**dataset[ROW]
- 基于RDD的
  - Immuatable  不可变的
  - Lazy Evaluations 
    - DataFrame的操作
    - transformation 延迟执行
    - action 执行了action之后transformation才会触发
  - Distributed
- **DataFrame vs RDD 区别**
  - DataFrame相当于是一个带着schema的RDD
  - DataFrame还引入了off-heap,意味着JVM堆以外的内存
  - RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合

- **Pandas DataFrame vs Spark DataFrame**
  - 单机 VS 分布式并行计算
  - Spark DataFrame延迟执行
  - Spark DataFrame 不可变
  - Pandas DataFrame API 更丰富

### 集群启动

首先start-all.sh启动hadoop

**启动Spark集群**

- 进入到$SPARK_HOME/sbin目录

  - 启动Master	

  ```shell
  ./start-master.sh -h 192.168.19.137
  ```

  - 启动Slave

  ```shell
   ./start-slave.sh spark://192.168.19.137:7077
  ```

  - jps查看进程

进入虚拟环境

source activate py365

启动jupyter

jupyter notebook --ip 0.0.0.0 --allow-root

### DataFrame的创建

- 创建DataFrame先需要一个spark session

  ```python
  SPARK_APP_NAME = "dataframetest"
  SPARK_URL = "spark://192.168.19.137:7077"
  
  conf = SparkConf()    # 创建spark config对象
  config = (
  	("spark.app.name", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称
  	("spark.executor.memory", "6g"),    # 设置该app启动时占用的内存用量，默认1g
  	("spark.master", SPARK_URL),    # spark master的地址
      ("spark.executor.cores", "4"),    # 设置spark executor使用的CPU核心数
  conf.setAll(config)
  
  # 利用config对象，创建spark session
  spark = SparkSession.builder.config(conf=conf).getOrCreate()
  ```

- 从RDD创建DataFrame

  ```python
  dataframe_people = spark.createDataFrame(Row对象的RDD)
  ```

- 从CSV文件创建DataFrame

  ```python
  df = spark.read.format('csv').option('header','true').load('/iris.csv')
  ```

- DataFrame常用API

  - DataFrame 经过transformation类算子处理 获取的还是DataFrame 

  - 只有调用Action类算子才会获取结果

  - 通过DataFrame调用.rdd 获取DataFrame所对应的RDD

  - DataFrame调用UDF（自定义函数）方式：

    - 创建函数
    - 创建udf对象 把写好的函数穿进去
    - DataFrame调用的时候 只能调用封装好的udf对象

    

    

- 通过DataFrame创建虚拟的视图，对应一张临时表，可以对这张临时表用sql操作

  ```python
  jsonDF.createOrReplaceTempView('临时表名')
  spark.sql('select * from 临时表名 where pop>40000')
  ```

  

- DataFrame 创建schema

  ```python
  from pyspark.sql.types import *
  jsonSchema = StructType([
      StructField("id", StringType(), True),
      StructField("city", StringType(), True),
      StructField("loc" , ArrayType(DoubleType())),
      StructField("pop", LongType(), True),
      StructField("state", StringType(), True)
  ])
  
  jsonSchema = StructType() \
    .add("id", StringType(),True) \
    .add("city", StringType()) \
    .add("pop" , LongType()) \
    .add("state",StringType())
  ```

  

### Spark SQL 回顾

- 处理结构化的数据
- 有两套处理方式
  - sql
  - DataFrame
- Spark SQL  和 Hive有类似的地方
- Spark SQL 不管是写 SQL 还是用DataFrame最终都会转换成对RDD的操作
- SparkSQL 优势
  - 代码少
  - 速度快（从SparkSQL 生成的RDD spark会优化）
- 如何创建DataFrame
  - SparkSession
  - sparkSession.createDataFrame(内存)
  - sparkSession.read.format('格式').load(地址)
- DataFrame 基于Rdd的
  - 和RDD的相同点 不可变 transformation action 分布式
  - 和RDD的不同点 DataFrame 处理的是结构化的数据 有schema
- DataFrame 数据可以创建一个虚拟的View 指定一个表名 就可以用SQL来操作了
- DataFrame.rdd 直接获取到对应的RDD 从而利用RDD的api来操作数据

### spark Streaming简介

- 可扩展，高吞吐具有容错性的流式计算框架
- 对比常用实时计算/流式计算框架
  - storm 流式计算
    - 来一条处理一条
    - 对比  spark streaming 速度更快
    - 对python不友好
  - flink 流式计算
    - 来一条处理一条
    - 对比  spark streaming 速度更快
  - spark streaming 实时（准实时）
    - micro batch
    - 指定一个时间间隔
    - 间隔一段时间 获取一次数据
    - 能处理的数据量会更大一些
    - 实时性会稍差
    - pyspark

### Spark Streaming组件

- Streaming Context 
  - 流上下文 通过Streaming Context 可以连接数据源获取数据
  - 通过spark context 可以获取到streaming context
  - 在创建Streaming Context 需要指定一个时间间隔（micro batch）
  - Streaming Context调用了stop方法之后 就不能再次调 start()
  -  一个SparkContext创建一个Streaming Context
  - streaming Context上调用Stop方法，默认会把spark context也关掉
  - 如果只想仅关闭Streaming Context对象,设置stop()的可选参数为false
  - 对DStream中数据处理的逻辑要写在Streaming Context开启之前 一旦Streaming Context调用了start方法 就不能再添加新的数据处理逻辑
- DStream 
  - Streaming Context 连接到不同的数据源获取到的数据 抽象成DStream模型
  - 代表一个连续的数据流
  - 由一系列连续的RDD组成
  - 任何对DStreams的操作都转换成了对DStreams隐含的RDD的操作

### Spark streaming 状态更新操作

- 使用有状态操作，需要设置检查点

  ```python
  ssc.checkpoint('checkpoint')
  ```

- updatestateByKey

  - 会保留之前的状态
  - 每个时间片的数据会累计下来

  ```python
  #定义一个状态更新函数
      def updateFunc(new_values,last_state):
          return sum(new_values)+(last_state or 0)
      wordCount = pairs.updateStateByKey(updateFunc = updateFunc)
  ```

  -

- window

  - 指定一个时间窗口 指定给一个窗口滑动的时间间隔
  - 针对在窗口范围内的数据做处理
    - 随着窗口的滑动 会有一部分数据移出窗口
    - 随着窗口的滑动 会有一部分数据移进窗口
  - reduceByKeyAndWindow
    - 相同的Key在同一个window内会走到 同一个reduceByKeyAndWindow中
    - 定义 滑入数据改如何处理的函数
      - addFunc = lambda x, y: x + y
    - 定义 滑出的数据改如何处理的函数
      - invAddFunc = lambda x, y: x - y
    - reduceByKeyAndWindow(addFunc, invAddFunc, 窗口长度, 滑动频率)

- 日志文件夹->Flume->kafka->spark steaming->

  - 推荐模型通过sparkML spark Mllib训练的
  - 通过spark steaming采集到用户实时行为
  - 把排序模型加载到内存中 获取到实时行为 修改召回集合修改用户的长期属性 实时调用排序模型影响推荐结果



- Hadoop
  - hdfs 分布式存储
    - 数据会拆成小块 128MB 一个block
    - 数据会冗余 默认3个副本
    - name node
    - data node 
  - mapreduce 分布式计算
    - 理解map reduce 编程思想
    - hadoop streaming 
    - mrjob
  - yarn 资源调度协调
    - resource manager
    - node manager
    - application master
    - container

- Hive/Hbase
  - SQL 替代编写MapReduce代码
  - 处理结构化的数据
  - metastore Mysql
  - 内部表/外部表
  - 分区表

- HBase
  - nosql数据库
  - 非关系型数据
  - 支撑线上业务 速度比较快
  - 基于hdfs
  - 列式数据库
  - row key   column family
  - 行级别事务
  - cp 强一致性 分区容错性 

spark

- 分布式的计算框架 基于内存 生态比较丰富 速度比mr快，api更丰富

- spark core
  - RDD
- spark sql
  - dataframe
  - sql
- spark streaming
- spark ML spark mllib