{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_feature.csv\tSpark_sql.ipynb  test1.csv  user_profile.csv  电商推荐.ipynb\r\n",
      "raw_sample.csv\tspark-warehouse  test5.csv  电商推荐1.ipynb   推荐概述.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 items\r\n",
      "-rw-r--r--   1 root supergroup      88444 2021-03-29 21:19 /USA.json\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-08-12 15:16 /checkPoint\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-04-10 20:09 /data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-12 15:25 /hbase\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-04-25 15:05 /headlines\r\n",
      "-rw-r--r--   1 root supergroup       4662 2021-03-26 20:22 /iris.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-11 15:00 /output\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-19 16:10 /output1\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-11 15:16 /output2\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-05-17 11:02 /py5\r\n",
      "-rw-r--r--   1 root supergroup   22924462 2021-03-30 20:50 /raw_nyc_phil.json\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-13 11:34 /spark\r\n",
      "drwx------   - root supergroup          0 2022-08-12 14:57 /tmp\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-16 11:12 /user\r\n",
      "-rw-r--r--   1 root supergroup         42 2022-05-17 11:03 /words\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda2/envs/py365/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置spark driver和pyspark运行时，所使用的python解释器路径\n",
    "PYSPARK_PYTHON = \"/miniconda2/envs/py365/bin/python\"\n",
    "JAVA_HOME='/root/bigdata/jdk'\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ['JAVA_HOME']=JAVA_HOME\n",
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "#注意先启动standalone模式\n",
    "# ./start-master.sh -h 192.168.19.137\n",
    "#  ./start-slave.sh spark://192.168.19.137:7077\n",
    "SPARK_APP_NAME = \"preprocessingBehaviorLog\"\n",
    "SPARK_URL = \"spark://192.168.19.137:7077\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark conf对象，把config设置到conf中，conf传入config\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"6g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "\t(\"spark.master\", SPARK_URL),    # spark 集群地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数\n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "#     (\"spark.dynamicAllocation.enabled\", True),\n",
    "#     (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "#     (\"spark.shuffle.service.enabled\", True)\n",
    "# \t('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的代码大纲\n",
    "# 1、通过rdd创建df\n",
    "# 2、通过读取csv得到df\n",
    "# 3、df的groupby等各种聚合操作\n",
    "# 4、df的udf（自定义函数）操作\n",
    "# 5、读取JSON格式数据变为df\n",
    "# 6、df进行数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、通过rdd创建df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过RDD来创建Datarame\n",
    "#创建Datarase需要有sparg session\n",
    "#创刨建RDD蒜要sparkcontext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "# ================直接创建==========================\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "#为数据添加列名，Row是一个row对象，df需要每一行都是一个Row对象\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "#创建DataFrame\n",
    "schemaPeople = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 25|   Ankit|\n",
      "| 22|Jalfaizy|\n",
      "| 20| saurabh|\n",
      "| 26|    Bala|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、通过读取csv得到df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认从hadoop的根加载数据，读取时需要头部option(\"header\", \"true\")\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, SepalLength: string, SepalWidth: string, PetalLength: string, PetalWidth: string, Species: string, cls: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'SepalLength',\n",
       " 'SepalWidth',\n",
       " 'PetalLength',\n",
       " 'PetalWidth',\n",
       " 'Species',\n",
       " 'cls']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|cls|newWidth|\n",
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|  0|     7.0|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|  0|     6.0|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2| setosa|  0|     6.4|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2| setosa|  0|     6.2|\n",
      "|  5|          5|       3.6|        1.4|       0.2| setosa|  0|     7.2|\n",
      "|  6|        5.4|       3.9|        1.7|       0.4| setosa|  0|     7.8|\n",
      "|  7|        4.6|       3.4|        1.4|       0.3| setosa|  0|     6.8|\n",
      "|  8|          5|       3.4|        1.5|       0.2| setosa|  0|     6.8|\n",
      "|  9|        4.4|       2.9|        1.4|       0.2| setosa|  0|     5.8|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|  0|     6.2|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|  0|     7.4|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|  0|     6.8|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|  0|     6.0|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|  0|     6.0|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|  0|     8.0|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|  0|     8.8|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|  0|     7.8|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|  0|     7.0|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|  0|     7.6|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|  0|     7.6|\n",
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============增加一列(或者替换) withColumn===========\n",
    "#定义一个新的列，数据为其他某列数据的两倍\n",
    "#如果操作的是原有列，可以替换原有列的数据\n",
    "df.withColumn('newWidth',df.SepalWidth * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|  5|          5|       3.6|        1.4|       0.2| setosa|\n",
      "|  6|        5.4|       3.9|        1.7|       0.4| setosa|\n",
      "|  7|        4.6|       3.4|        1.4|       0.3| setosa|\n",
      "|  8|          5|       3.4|        1.5|       0.2| setosa|\n",
      "|  9|        4.4|       2.9|        1.4|       0.2| setosa|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========删除一列  drop=========================\n",
    "#删除一列\n",
    "df.drop('cls').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+---+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|cls|\n",
      "+---+-----------+----------+-----------+----------+-------+---+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|  0|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|  0|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2| setosa|  0|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2| setosa|  0|\n",
      "|  5|          5|       3.6|        1.4|       0.2| setosa|  0|\n",
      "|  6|        5.4|       3.9|        1.7|       0.4| setosa|  0|\n",
      "|  7|        4.6|       3.4|        1.4|       0.3| setosa|  0|\n",
      "|  8|          5|       3.4|        1.5|       0.2| setosa|  0|\n",
      "|  9|        4.4|       2.9|        1.4|       0.2| setosa|  0|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|  0|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|  0|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|  0|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|  0|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|  0|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|  0|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|  0|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|  0|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|  0|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|  0|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|  0|\n",
      "+---+-----------+----------+-----------+----------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() #原有的df并没有变，因为df不可变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newiris=df.withColumn('newid',df.id * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+------------------+\n",
      "|summary|                id|       SepalLength|         SepalWidth|       PetalLength|        PetalWidth|  Species|               cls|             newid|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+------------------+\n",
      "|  count|               150|               150|                150|               150|               150|      150|               150|               150|\n",
      "|   mean|              75.5| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|               1.0|              75.5|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|0.8192319205190406|43.445367992456916|\n",
      "|    min|                 1|               4.3|                  2|                 1|               0.1|   setosa|                 0|               1.0|\n",
      "|    max|                99|               7.9|                4.4|               6.9|               2.5|virginica|                 2|             150.0|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_newiris.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "|summary|                id|       SepalLength|         SepalWidth|       PetalLength|        PetalWidth|  Species|               cls|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "|  count|               150|               150|                150|               150|               150|      150|               150|\n",
      "|   mean|              75.5| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|               1.0|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|0.8192319205190406|\n",
      "|    min|                 1|               4.3|                  2|                 1|               0.1|   setosa|                 0|\n",
      "|    max|                99|               7.9|                4.4|               6.9|               2.5|virginica|                 2|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               cls|\n",
      "+-------+------------------+\n",
      "|  count|               150|\n",
      "|   mean|               1.0|\n",
      "| stddev|0.8192319205190406|\n",
      "|    min|                 0|\n",
      "|    max|                 2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================ 统计信息 describe================\n",
    "df.describe().show()  #未转为整型，字符串的最大值是99\n",
    "#计算某一列的描述信息\n",
    "df.describe('cls').show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|SepalLength|SepalWidth|\n",
      "+-----------+----------+\n",
      "|        5.1|       3.5|\n",
      "|        4.9|         3|\n",
      "|        4.7|       3.2|\n",
      "|        4.6|       3.1|\n",
      "|          5|       3.6|\n",
      "|        5.4|       3.9|\n",
      "|        4.6|       3.4|\n",
      "|          5|       3.4|\n",
      "|        4.4|       2.9|\n",
      "|        4.9|       3.1|\n",
      "|        5.4|       3.7|\n",
      "|        4.8|       3.4|\n",
      "|        4.8|         3|\n",
      "|        4.3|         3|\n",
      "|        5.8|         4|\n",
      "|        5.7|       4.4|\n",
      "|        5.4|       3.9|\n",
      "|        5.1|       3.5|\n",
      "|        5.7|       3.8|\n",
      "|        5.1|       3.8|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============提取部分列 select==============\n",
    "df.select('SepalLength','SepalWidth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================基本统计功能 distinct count=====\n",
    "df.select('cls').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------------------+\n",
      "|cls|max(SepalLength)|   avg(SepalWidth)|\n",
      "+---+----------------+------------------+\n",
      "|  0|             5.8| 3.428000000000001|\n",
      "|  1|               7|2.7700000000000005|\n",
      "|  2|             7.9|2.9739999999999998|\n",
      "+---+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组统计 groupby(colname).agg({'col':'fun','col2':'fun2'})\n",
    "df.groupby('cls').agg({'SepalWidth':'mean','SepalLength':'max'}).show()\n",
    "\n",
    "# avg(), count(), countDistinct(), first(), kurtosis(),\n",
    "# max(), mean(), min(), skewness(), stddev(), stddev_pop(),\n",
    "# stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        Computes the cosine inverse of the given value; the returned angle is in the range0.0 through pi.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(add_months(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 5, 8))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. note:: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Returns a new :class:`Column` for approximate distinct count of ``col``.\n",
      "        \n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that have\n",
      "            the same data type.\n",
      "        \n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        :param col: name of column containing array\n",
      "        :param value: value to check for in array\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        Computes the sine inverse of the given value; the returned angle is in the range-pi/2 through pi/2.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan(col)\n",
      "        Computes the tangent inverse of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        Returns the angle theta from the conversion of rectangular coordinates (x, y) topolar coordinates (r, theta).\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input string columns together into a single string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    cos(col)\n",
      "        Computes the cosine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    cosh(col)\n",
      "        Computes the hyperbolic cosine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`Column` for the population covariance of ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`Column` for the sample covariance of ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that grouped\n",
      "            as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date as a date column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp as a timestamp column.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(date_add(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 4, 9))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of the Java class `java.text.SimpleDateFormat` can be used.\n",
      "        \n",
      "        .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "            specialized implementation.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(date_format('a', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(date_sub(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 4, 7))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(dayofmonth('a').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(dayofyear('a').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a [[StructType]] or [[ArrayType]]\n",
      "        of [[StructType]]s with the specified schema. Returns `null`, in the case of an unparseable\n",
      "        string.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param schema: a StructType or ArrayType of StructType to use when parsing the json column\n",
      "        :param options: options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp, which corresponds to a certain time of day in UTC, returns another timestamp\n",
      "        that corresponds to the same time of day in the given timezone.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(from_utc_timestamp(df.t, \"PST\").alias('t')).collect()\n",
      "        [Row(t=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param path: path to the json object to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. note:: The list of columns should match with grouping columns exactly, or empty (means all\n",
      "            the grouping columns).\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(hour('a').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param fields: list of fields to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lead(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    length(col)\n",
      "        Calculates the length of a string or binary expression.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        :param substr: a string\n",
      "        :param str: a Column of :class:`pyspark.sql.types.StringType`\n",
      "        :param pos: start position (zero based)\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string column to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(minute('a').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    month(col)\n",
      "         Extract the month of a given date as integer.\n",
      "        \n",
      "         >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "         >>> df.select(month('a').alias('month')).collect()\n",
      "         [Row(month=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    months_between(date1, date2)\n",
      "        Returns the number of months between date1 and date2.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
      "        >>> df.select(months_between(df.t, df.d).alias('months')).collect()\n",
      "        [Row(months=3.9495967...)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (DoubleType or FloatType).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        :param n: an integer\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(quarter('a').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        from U[0.0, 1.0].\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', '(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    reverse(col)\n",
      "        Reverses the string column and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(second('a').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        Computes the sine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sinh(col)\n",
      "        Computes the hyperbolic sine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. note:: This is indeterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    split(str, pattern)\n",
      "        Splits str around pattern (pattern is a regular expression).\n",
      "        \n",
      "        .. note:: pattern is a string represent the regular expression.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n",
      "        [Row(s=['ab', 'cd'])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions\n",
      "        \n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        Computes the tangent of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    tanh(col)\n",
      "        Computes the hyperbolic tangent of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. note:: Deprecated in 2.1, use degrees instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. note:: Deprecated in 2.1, use radians instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Default format is 'yyyy-MM-dd'.\n",
      "        Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a [[StructType]] or [[ArrayType]] of [[StructType]]s into a\n",
      "        JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        :param col: name of column containing the struct or array of the structs\n",
      "        :param options: options to control converting. accepts the same options as the json datasource\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(name='Alice', age=2))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(name='Alice', age=2), Row(name='Bob', age=3)])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Default format is 'yyyy-MM-dd HH:mm:ss'. Specify\n",
      "        formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp, which corresponds to a certain time of day in the given timezone, returns\n",
      "        another timestamp that corresponds to the same time of day in UTC.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_utc_timestamp(df.t, \"PST\").alias('t')).collect()\n",
      "        [Row(t=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'YYYY', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a :class:`Column` expression representing a user defined function (UDF).\n",
      "        \n",
      "        .. note:: The user-defined functions must be deterministic. Due to optimization,\n",
      "            duplicate invocations may be eliminated or the function may even be invoked more times than\n",
      "            it is present in the query.\n",
      "        \n",
      "        :param f: python function if used as a standalone function\n",
      "        :param returnType: a :class:`pyspark.sql.types.DataType` object\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string column to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(weekofyear(df.a).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "        \n",
      "        :param condition: a boolean :class:`Column` expression.\n",
      "        :param value: a literal value, or a :class:`Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(year('a').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['abs', 'acos', 'add_months', 'approxCountDistinct', 'approx...\n",
      "\n",
      "FILE\n",
      "    /miniconda2/envs/py365/lib/python3.6/site-packages/pyspark-2.2.2-py3.6.egg/pyspark/sql/functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|width_count|distinct_cls_count|\n",
      "+-----------+------------------+\n",
      "|        150|                 3|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 自定义的汇总方法\n",
    "import pyspark.sql.functions as fn\n",
    "#调用函数并起一个别名,fn内部有哪些函数可以在pycharm联想\n",
    "df.agg(fn.count('SepalWidth').alias('width_count'),fn.countDistinct('cls').alias('distinct_cls_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================数据集拆成两部分 randomSplit ===========\n",
    "#设置数据比例将数据划分为两部分\n",
    "trainDF, testDF = df.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================采样数据 sample===========\n",
    "#withReplacement：是否有放回的采样 第一个参数\n",
    "#fraction：采样比例  第二个参数\n",
    "#seed：随机种子 第三个参数\n",
    "sdf = df.sample(False,0.2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cls|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看两个数据集在类别上的差异 subtract，确保训练数据集覆盖了所有分类，输出为空是合理的\n",
    "diff_in_train_test = testDF.select('cls').subtract(trainDF.select('cls'))\n",
    "diff_in_train_test.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|cls_SepalLength|4.3|4.4|4.5|4.6|4.7|4.8|4.9|  5|5.1|5.2|5.3|5.4|5.5|5.6|5.7|5.8|5.9|  6|6.1|6.2|6.3|6.4|6.5|6.6|6.7|6.8|6.9|  7|7.1|7.2|7.3|7.4|7.6|7.7|7.9|\n",
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|              2|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  1|  1|  3|  1|  2|  2|  2|  6|  5|  4|  0|  5|  2|  3|  0|  1|  3|  1|  1|  1|  4|  1|\n",
      "|              1|  0|  0|  0|  0|  0|  0|  1|  2|  1|  1|  0|  1|  5|  5|  5|  3|  2|  4|  4|  2|  3|  2|  1|  2|  3|  1|  1|  1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|              0|  1|  3|  1|  4|  2|  5|  4|  8|  8|  3|  1|  5|  2|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================交叉表 crosstab=============\n",
    "# 一行代表一个类别，这个类别中sepallength的分布式怎么样的\n",
    "df.crosstab('cls','SepalLength').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、df的udf（自定义函数）操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cls|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "['1', '2']\n",
      "+---+-----------+----------+-----------+----------+-------+---+-------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|cls|New_cls|\n",
      "+---+-----------+----------+-----------+----------+-------+---+-------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|  0|      0|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|  0|      0|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|  0|      0|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|  0|      0|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|  0|      0|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|  0|      0|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|  0|      0|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|  0|      0|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|  0|      0|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|  0|      0|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|  0|      0|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|  0|      0|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|  0|      0|\n",
      "| 21|        5.4|       3.4|        1.7|       0.2| setosa|  0|      0|\n",
      "| 22|        5.1|       3.7|        1.5|       0.4| setosa|  0|      0|\n",
      "| 23|        4.6|       3.6|          1|       0.2| setosa|  0|      0|\n",
      "| 24|        5.1|       3.3|        1.7|       0.5| setosa|  0|      0|\n",
      "| 25|        4.8|       3.4|        1.9|       0.2| setosa|  0|      0|\n",
      "| 26|          5|         3|        1.6|       0.2| setosa|  0|      0|\n",
      "| 27|          5|       3.4|        1.6|       0.4| setosa|  0|      0|\n",
      "+---+-----------+----------+-----------+----------+-------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================== 综合案例 + udf================\n",
    "# 测试数据集中有些类别在训练集中是不存在的，找到这些数据集做后续处理\n",
    "trainDF,testDF = df.randomSplit([0.99,0.01])\n",
    "\n",
    "diff_in_train_test = trainDF.select('cls').subtract(testDF.select('cls')).distinct().show()\n",
    "\n",
    "#首先找到这些类，整理到一个列表\n",
    "not_exist_cls = trainDF.select('cls').subtract(testDF.select('cls')).distinct().rdd.map(lambda x :x[0]).collect()\n",
    "print(not_exist_cls)\n",
    "#定义一个方法，用于检测\n",
    "def should_remove(x):\n",
    "    if x in not_exist_cls:\n",
    "        return -1\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "#创建udf，udf函数需要两个参数：\n",
    "# Function\n",
    "# Return type (in my case StringType())\n",
    "\n",
    "#在RDD中可以直接定义函数，交给rdd的transformatioins方法进行执行\n",
    "#在DataFrame中需要通过udf将自定义函数封装成udf函数再交给DataFrame进行调用执行\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#这一步是封装\n",
    "check = udf(should_remove,StringType())\n",
    "#增加了New_cls的列，然后<>是不等于-1的就要，等于-1的就去除掉\n",
    "resultDF = trainDF.withColumn('New_cls',check(trainDF['cls'])).filter('New_cls <> -1')\n",
    "\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 JSON数据的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonString = [\n",
    "\"\"\"{ \"id\" : \"01001\", \"city\" : \"AGAWAM\",  \"pop\" : 15338, \"state\" : \"MA\" }\"\"\",\n",
    "\"\"\"{ \"id\" : \"01002\", \"city\" : \"CUSHMAN\", \"pop\" : 36963, \"state\" : \"MA\" }\"\"\"\n",
    "]\n",
    "jsonRDD = sc.parallelize(jsonString)   # stringJSONRDD\n",
    "jsonDF =  spark.read.json(jsonRDD)  # convert RDD into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- pop: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark处理json会做类型推断\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+\n",
      "|   city|   id|  pop|state|\n",
      "+-------+-----+-----+-----+\n",
      "| AGAWAM|01001|15338|   MA|\n",
      "|CUSHMAN|01002|36963|   MA|\n",
      "+-------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.read.json(\"/USA.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|     _corrupt_record|            geometry|  id|          properties|   type|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|{\"type\":\"FeatureC...|                null|null|                null|   null|\n",
      "|                null|[WrappedArray(Wra...|  01|           [Alabama]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  02|            [Alaska]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  04|           [Arizona]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  05|          [Arkansas]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  06|        [California]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  08|          [Colorado]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  09|       [Connecticut]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  10|          [Delaware]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  11|[District of Colu...|Feature|\n",
      "|                null|[WrappedArray(Wra...|  12|           [Florida]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  13|           [Georgia]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  15|            [Hawaii]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  16|             [Idaho]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  17|          [Illinois]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  18|           [Indiana]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  19|              [Iowa]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  20|            [Kansas]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  21|          [Kentucky]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  22|         [Louisiana]|Feature|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# truncate=False\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#去看每一部分的类型\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---+----------------+-------+\n",
      "|_corrupt_record|            geometry| id|      properties|   type|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "|           null|[WrappedArray(Wra...| 41|        [Oregon]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 42|  [Pennsylvania]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 44|  [Rhode Island]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 45|[South Carolina]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 46|  [South Dakota]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 47|     [Tennessee]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 48|         [Texas]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 49|          [Utah]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 50|       [Vermont]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 51|      [Virginia]|Feature|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.filter(jsonDF.id>40).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.createOrReplaceTempView(\"tmp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---+----------------+-------+\n",
      "|_corrupt_record|            geometry| id|      properties|   type|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "|           null|[WrappedArray(Wra...| 41|        [Oregon]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 42|  [Pennsylvania]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 44|  [Rhode Island]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 45|[South Carolina]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 46|  [South Dakota]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 47|     [Tennessee]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 48|         [Texas]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 49|          [Utah]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 50|       [Vermont]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 51|      [Virginia]|Feature|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF = spark.sql(\"select * from tmp_table where id>40\")\n",
    "resultDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取json格式指定类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- pop: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+----+-------+-----+-----+\n",
      "|  id|   city|  pop|state|\n",
      "+----+-------+-----+-----+\n",
      "|1001| AGAWAM|15338|   MA|\n",
      "|1002|CUSHMAN|36963|   MA|\n",
      "+----+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString = [\n",
    "\"\"\"{ \"id\" : \"1001\", \"city\" : \"AGAWAM\",  \"pop\" : 15338, \"state\" : \"MA\" }\"\"\",\n",
    "\"\"\"{ \"id\" : \"1002\", \"city\" : \"CUSHMAN\", \"pop\" : 36963, \"state\" : \"MA\" }\"\"\"\n",
    "]\n",
    "\n",
    "jsonRDD = sc.parallelize(jsonString)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#定义结构类型\n",
    "#StructType：schema的整体结构，表示JSON的对象结构\n",
    "#XXXStype:指的是某一列的数据类型\n",
    "jsonSchema = StructType() \\\n",
    "  .add(\"id\", StringType(),True) \\\n",
    "  .add(\"city\", StringType()) \\\n",
    "  .add(\"pop\" , LongType()) \\\n",
    "  .add(\"state\",StringType())\n",
    "\n",
    "# 如果类型不对，就会为null\n",
    "# jsonSchema = StructType() \\\n",
    "#   .add(\"id\", LongType(),True) \\\n",
    "#   .add(\"city\", StringType()) \\\n",
    "#   .add(\"pop\" , DoubleType()) \\\n",
    "#   .add(\"state\",StringType())\n",
    "\n",
    "reader = spark.read.schema(jsonSchema) #第一步先read schema\n",
    "\n",
    "jsonDF = reader.json(jsonRDD)\n",
    "jsonDF.printSchema()\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗  删除重复值，空值处理，异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.删除重复数据\n",
    "\n",
    "groupby().count()：可以看到数据的重复情况\n",
    "'''\n",
    "df = spark.createDataFrame([\n",
    "  (1, 144.5, 5.9, 33, 'M'),\n",
    "  (2, 167.2, 5.4, 45, 'M'),\n",
    "  (3, 124.1, 5.2, 23, 'F'),\n",
    "  (4, 144.5, 5.9, 33, 'M'),\n",
    "  (5, 133.2, 5.7, 54, 'F'),\n",
    "  (3, 124.1, 5.2, 23, 'F'),\n",
    "  (5, 129.2, 5.3, 42, 'M'),\n",
    "], ['id', 'weight', 'height', 'age', 'gender'])\n",
    "\n",
    "# 查看重复记录\n",
    "#无意义重复数据去重：数据中行与行完全重复\n",
    "# 1.首先删除完全一样的记录,变为6条了\n",
    "df2 = df.dropDuplicates()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weight', 'height', 'age', 'gender']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in df2.columns if c!='id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#有意义去重：删除除去无意义字段之外的完全重复的行数据\n",
    "# 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）\n",
    "# 删除某些字段值完全一样的重复记录，subset参数定义这些字段\n",
    "df3 = df2.dropDuplicates(subset = [c for c in df2.columns if c!='id'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_count=5, distinct_id_count=4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）\n",
    "# 查看某一列是否有重复值，就是把id取出来，在distinct id取出来，对比\n",
    "import pyspark.sql.functions as fn\n",
    "df3.agg(fn.count('id').alias('id_count'),fn.countDistinct('id').alias('distinct_id_count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.对于id这种无意义的列重复，添加另外一列自增id，仅仅是增加一个无意义的自增id而已\n",
    "df3.withColumn('new_id',fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+------+\n",
      "| id|weight|height| age|gender|income|\n",
      "+---+------+------+----+------+------+\n",
      "|  1| 143.5|   5.6|  28|     M|100000|\n",
      "|  2| 167.2|   5.4|  45|     M|  null|\n",
      "|  3|  null|   5.2|null|  null|  null|\n",
      "|  4| 144.5|   5.9|  33|     M|  null|\n",
      "|  5| 133.2|   5.7|  54|     F|  null|\n",
      "|  6| 124.1|   5.2|null|     F|  null|\n",
      "|  7| 129.2|   5.3|  42|     M| 76000|\n",
      "+---+------+------+----+------+------+\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2.处理缺失值\n",
    "2.1 对缺失值进行删除操作(行，列)\n",
    "2.2 对缺失值进行填充操作(列的均值)\n",
    "2.3 对缺失值对应的行或列进行标记\n",
    "'''\n",
    "df_miss = spark.createDataFrame([\n",
    "(1, 143.5, 5.6, 28,'M', 100000),\n",
    "(2, 167.2, 5.4, 45,'M', None),\n",
    "(3, None , 5.2, None, None, None),\n",
    "(4, 144.5, 5.9, 33, 'M', None),\n",
    "(5, 133.2, 5.7, 54, 'F', None),\n",
    "(6, 124.1, 5.2, None, 'F', None),\n",
    "(7, 129.2, 5.3, 42, 'M', 76000),],\n",
    " ['id', 'weight', 'height', 'age', 'gender', 'income'])\n",
    "\n",
    "df_miss.show()\n",
    "print('-'*50)\n",
    "# 1.计算每条记录的缺失值情况\n",
    "#统计了每一行空值的个数\n",
    "df_miss.rdd.map(lambda row:(row['id'],sum([c==None for c in row]))).collect()\n",
    "# [(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.计算各列的缺失情况百分比  fn.count(c)这一列多少个有值，fn.count(c)这个c为None不会记上的\n",
    "df_miss.agg(*[(1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df_miss.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, weight: double, height: double, age: bigint, gender: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、删除缺失值过于严重的列\n",
    "# 其实是先建一个DF，不要缺失值的列\n",
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != 'income'])\n",
    "\n",
    "df_miss_no_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n",
      "+---+------------------+-----------------+----+\n",
      "| id|            weight|           height| age|\n",
      "+---+------------------+-----------------+----+\n",
      "|4.0|140.28333333333333|5.471428571428571|40.4|\n",
      "+---+------------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4、按照缺失值删除行（thresh是根据一行记录中，缺失字段的百分比的定义,一行有3个缺少，代表要删除）\n",
    "df_miss_no_income.dropna(thresh=3).show()\n",
    "\n",
    "# 5、填充缺失值，可以用fillna来填充缺失值，\n",
    "# 对于bool类型、或者分类类型，可以为缺失值单独设置一个类型，missing\n",
    "# 对于数值类型，可以用均值或者中位数等填充\n",
    "\n",
    "# fillna可以接收两种类型的参数：\n",
    "# 一个数字、字符串，这时整个DataSet中所有的缺失值都会被填充为相同的值。\n",
    "# 也可以接收一个字典｛列名：值｝这样\n",
    "\n",
    "# 先计算均值,均值计算会自动的跳过null的地方\n",
    "means = df_miss_no_income.agg(*[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(28+45+33+54+42)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#可以把spark的df转换为pandas的df\n",
    "import pandas\n",
    "# 先计算均值，并组织成一个字典,records是pd的df转为字典的一种形式参数，这种最常用的\n",
    "means = df_miss_no_income.agg( *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']).toPandas().to_dict('records')[0]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             143.5|   5.6| 28|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.28333333333333|   5.2| 40|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 40|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 然后添加其它的列\n",
    "means['gender'] = 'missing'\n",
    "\n",
    "df_miss_no_income.fillna(means).show()  #因为age是整型，填入的进行了取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_miss_no_income.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': [91.69999999999999, 191.7], 'height': [4.499999999999999, 6.1000000000000005], 'age': [-11.0, 93.0]}\n",
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3、异常值处理\n",
    "异常值：不属于正常的值 包含：缺失值，超过正常范围内的较大值或较小值\n",
    "分位数去极值\n",
    "中位数绝对偏差去极值\n",
    "正态分布去极值\n",
    "上述三种操作的核心都是：通过原始数据设定一个正常的范围，超过此范围的就是一个异常值\n",
    "'''\n",
    "df_outliers = spark.createDataFrame([\n",
    "(1, 143.5, 5.3, 28),\n",
    "(2, 154.2, 5.5, 45),\n",
    "(3, 342.3, 5.1, 99),\n",
    "(4, 144.5, 5.5, 33),\n",
    "(5, 133.2, 5.4, 54),\n",
    "(6, 124.1, 5.1, 21),\n",
    "(7, 129.2, 5.3, 42),\n",
    "], ['id', 'weight', 'height', 'age'])\n",
    "# 设定范围 超出这个范围的 用边界值替换\n",
    "\n",
    "# approxQuantile方法接收三个参数：\n",
    "# 参数1，列名；参数2：想要计算的分位点，可以是一个点，也可以是一个列表（0和1之间的小数），\n",
    "# 第三个参数是能容忍的误差，如果是0，代表百分百精确计算。\n",
    "\n",
    "cols = ['weight', 'height', 'age']\n",
    "\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [\n",
    "        quantiles[0] - 1.5 * IQR,\n",
    "        quantiles[1] + 1.5 * IQR\n",
    "        ]\n",
    "\n",
    "print(bounds)\n",
    "# {'age': [-11.0, 93.0], 'height': [4.499999999999999, 6.1000000000000005], 'weight': [91.69999999999999, 191.7]}\n",
    "\n",
    "# 为异常值字段打标志,如果小于下线，或者大于上线\n",
    "outliers = df_outliers.select(['id'] + [( (df_outliers[c] < bounds[c][0]) | (df_outliers[c] > bounds[c][1]) ).alias(c + '_o') for c in cols ])\n",
    "outliers.show()\n",
    "#\n",
    "# +---+--------+--------+-----+\n",
    "# | id|weight_o|height_o|age_o|\n",
    "# +---+--------+--------+-----+\n",
    "# |  1|   false|   false|false|\n",
    "# |  2|   false|   false|false|\n",
    "# |  3|    true|   false| true|\n",
    "# |  4|   false|   false|false|\n",
    "# |  5|   false|   false|false|\n",
    "# |  6|   false|   false|false|\n",
    "# |  7|   false|   false|false|\n",
    "# +---+--------+--------+-----+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|weight|\n",
      "+---+------+\n",
      "|  3| 342.3|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 再回头看看这些异常值的值，重新和原始数据关联\n",
    "\n",
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.filter('weight_o').select('id', 'weight').show()\n",
    "# +---+------+\n",
    "# | id|weight|\n",
    "# +---+------+\n",
    "# |  3| 342.3|\n",
    "# +---+------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 99|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#这个是年龄的\n",
    "df_outliers.filter('age_o').select('id', 'age').show()\n",
    "# +---+---+\n",
    "# | id|age|\n",
    "# +---+---+\n",
    "# |  3| 99|\n",
    "# +---+---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+--------+--------+-----+\n",
      "| id|weight|height|age|weight_o|height_o|age_o|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "|  3| 342.3|   5.1| 99|    true|   false| true|\n",
      "+---+------+------+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_outliers.filter('weight_o').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
