{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.3\n",
      "numpy 1.26.4\n",
      "pandas 2.3.0\n",
      "sklearn 1.7.0\n",
      "torch 2.7.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82b4e9",
   "metadata": {},
   "source": [
    "# 1. preprocessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e324758",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf6cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May I borrow, this book?\n",
      "¿Puedo tomar prestado este libro?\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "#因为西班牙语有一些是特殊字符，所以我们需要unicode转ascii，\n",
    "# 这样值变小了，因为unicode太大\n",
    "def unicode_to_ascii(s):\n",
    "    #NFD是转换方法，把每一个字节拆开，Mn是重音，所以去除\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#下面我们找个样本测试一下\n",
    "# 加u代表对字符串进行unicode编码\n",
    "en_sentence = u\"May I borrow, this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52ca34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may i borrow , this book ?\n",
      "¿ puedo tomar prestado este libro ?\n",
      "b'\\xc2\\xbf puedo tomar prestado este libro ?'\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w):\n",
    "    #变为小写，去掉多余的空格，变成小写，id少一些\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # eg: \"he is a boy.\" => \"he is a boy . \"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格，你可以保留一些标点符号\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    #因为可能有多余空格，替换为一个空格，所以处理一下\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    w = w.strip() #strip是去掉两边的空格\n",
    "\n",
    "    return w\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))  #¿是占用两个字节的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7670c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'test', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'test', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'test', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train'], dtype='<U5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集和测试集的一个方法\n",
    "split_index1 = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=100)\n",
    "split_index1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc5e69",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915b8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存文件加载train数据...\n",
      "从缓存文件加载test数据...\n"
     ]
    }
   ],
   "source": [
    "# 创建一个继承自torch.utils.data.Dataset的数据集类\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    用于加载和处理英语-西班牙语翻译数据集的自定义Dataset类\n",
    "    参数:\n",
    "        path: 数据文件路径\n",
    "        num_examples: 要使用的样本数量，None表示使用全部\n",
    "        split: 数据集划分，可选'train'或'test'\n",
    "    \"\"\"\n",
    "    def __init__(self, path, num_examples=None, split=None):\n",
    "        # 检查是否存在缓存文件\n",
    "        cache_file_en = f'{split}_en_sentences.npy' if split else 'all_en_sentences.npy'\n",
    "        cache_file_sp = f'{split}_sp_sentences.npy' if split else 'all_sp_sentences.npy'\n",
    "        \n",
    "        # 如果缓存文件存在，直接加载\n",
    "        if os.path.exists(cache_file_en) and os.path.exists(cache_file_sp):\n",
    "            print(f\"从缓存文件加载{split}数据...\")\n",
    "            self.trg = np.load(cache_file_en)\n",
    "            self.src = np.load(cache_file_sp)\n",
    "        else:\n",
    "            print(f\"从{path}读取数据并创建{split}数据集...\")\n",
    "            # 读取文件\n",
    "            lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "            \n",
    "            # 创建空列表存储英语和西班牙语句子对\n",
    "            self.en_sentences = []\n",
    "            self.sp_sentences = []\n",
    "            \n",
    "            # 生成训练集和测试集的索引,如果num_examples为None，则使用所有行，否则使用num_examples行\n",
    "            total_examples = len(lines) if num_examples is None else min(num_examples, len(lines)) \n",
    "            # 使用9:1的比例划分训练集和测试集\n",
    "            split_index = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=total_examples)\n",
    "            \n",
    "            # 遍历每一行，按tab分隔英语和西班牙语\n",
    "            for i, line in enumerate(lines[:total_examples]):\n",
    "                # 如果指定了split，则只保留对应的数据\n",
    "                if split is not None and split_index[i] != split:\n",
    "                    continue\n",
    "                    \n",
    "                # 按tab分隔获取英语和西班牙语句子\n",
    "                en, sp = line.split('\\t')\n",
    "                \n",
    "                # 对句子进行预处理（清理、标准化等）\n",
    "                en = preprocess_sentence(en)\n",
    "                sp = preprocess_sentence(sp)\n",
    "                \n",
    "                # 将处理后的句子添加到对应列表\n",
    "                self.en_sentences.append(en)\n",
    "                self.sp_sentences.append(sp)\n",
    "            \n",
    "            # 转换为numpy数组以提高效率\n",
    "            self.trg = np.array(self.en_sentences) #英语(目标语言)\n",
    "            self.src = np.array(self.sp_sentences) #西班牙语(源语言)\n",
    "            \n",
    "            # 将处理后的数据保存为numpy文件以加速后续加载\n",
    "            np.save(cache_file_en, self.trg)\n",
    "            np.save(cache_file_sp, self.src)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中的样本数量\"\"\"\n",
    "        return len(self.trg)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"返回指定索引的源语言和目标语言句子对\"\"\"\n",
    "        return self.src[idx],self.trg[idx]\n",
    "    \n",
    "\n",
    "# 从spa.txt创建训练集和测试集\n",
    "train_dataset = TranslationDataset('spa.txt', split='train')  # 创建训练数据集\n",
    "test_dataset = TranslationDataset('spa.txt', split='test')    # 创建测试数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ffe83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado .\n",
      "target: if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo .\n"
     ]
    }
   ],
   "source": [
    "print(\"source: {}\\ntarget: {}\".format(*train_dataset[-1])) # print the last training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b17140",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c973165",
   "metadata": {},
   "source": [
    "这里有两种处理方式，分别对应着 encoder 和 decoder 的 word embedding 是否共享，这里实现不共享的方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04dc44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences_pairs):\n",
    "    \"\"\"\n",
    "    构建英语和西班牙语的词典\n",
    "    Args:\n",
    "        sentences_pairs: 包含英语和西班牙语句子对的列表\n",
    "    Returns:\n",
    "        en_vocab: 英语词典\n",
    "        sp_vocab: 西班牙语词典\n",
    "    \"\"\"\n",
    "    # 分别存储英语和西班牙语的单词\n",
    "    en_words = []\n",
    "    sp_words = []\n",
    "    \n",
    "    # 遍历所有句子对，分别提取单词\n",
    "    for en, sp in sentences_pairs:\n",
    "        en_words.extend(en.split())\n",
    "        sp_words.extend(sp.split())\n",
    "    \n",
    "    # 使用Counter统计词频\n",
    "    en_vocab = Counter(en_words)\n",
    "    sp_vocab = Counter(sp_words)\n",
    "    \n",
    "    return en_vocab, sp_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07575987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英语词典大小: 12504\n",
      "西班牙语词典大小: 23719\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 构建英语和西班牙语的词典\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    \"\"\"\n",
    "    构建词典函数\n",
    "    参数:\n",
    "        sentences: 句子列表\n",
    "        min_freq: 最小词频,默认为1\n",
    "    返回:\n",
    "        word2idx: 词到索引的映射字典\n",
    "    \"\"\"\n",
    "    # 初始化词典，包含特殊标记\n",
    "    word2idx = {\n",
    "        \"[PAD]\": 0,     # 填充 token\n",
    "        \"[BOS]\": 1,     # begin of sentence\n",
    "        \"[UNK]\": 2,     # 未知 token\n",
    "        \"[EOS]\": 3,     # end of sentence\n",
    "    }\n",
    "    \n",
    "    # 使用Counter统计词频\n",
    "    # Counter类可以自动统计可迭代对象中每个元素出现的次数\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(sentence.split())  # split()将句子分割成单词列表\n",
    "    \n",
    "    # 按词频排序并添加到词典中\n",
    "    idx = len(word2idx)  # 从特殊标记数量开始编号\n",
    "    # 返回计数最高的前 n 个元素及其计数，若未指定 n 则返回所有元素\n",
    "    for word, count in counter.most_common():\n",
    "        # 只添加频率大于等于min_freq的词\n",
    "        if count >= min_freq:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return word2idx\n",
    "\n",
    "# 构建英语(目标语言)和西班牙语(源语言)词典\n",
    "# trg代表target(目标), src代表source(源)\n",
    "trg_word2idx = build_vocab(train_dataset.trg) #英语\n",
    "src_word2idx = build_vocab(train_dataset.src) #西班牙语\n",
    "\n",
    "# 创建反向映射（索引到词）\n",
    "# 用于之后将模型输出的索引转换回单词\n",
    "trg_idx2word = {idx: word for word, idx in trg_word2idx.items()}\n",
    "src_idx2word = {idx: word for word, idx in src_word2idx.items()}\n",
    "# 打印词典大小，用于检查词典构建是否正确\n",
    "print(f\"英语词典大小: {len(trg_word2idx)}\")\n",
    "print(f\"西班牙语词典大小: {len(src_word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c175fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_text----------\n",
      "['hello', 'world']\n",
      "['tokenize', 'text', 'datas', 'with', 'batch']\n",
      "['this', 'is', 'a', 'test']\n",
      "mask----------\n",
      "tensor([0, 0, 0, 0, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1])\n",
      "indices----------\n",
      "tensor([   1, 1745,  309,    3,    0,    0,    0])\n",
      "tensor([   1,    2, 2103,    2,   39,    2,    3])\n",
      "tensor([  1,  23,  12,  10, 924,   3,   0])\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=500, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        self.word2idx = word2idx  # 词到索引的映射字典\n",
    "        self.idx2word = idx2word  # 索引到词的映射字典\n",
    "        self.max_length = max_length  # 序列的最大长度\n",
    "        self.pad_idx = pad_idx  # 填充标记的索引\n",
    "        self.bos_idx = bos_idx  # 句子开始标记的索引\n",
    "        self.eos_idx = eos_idx  # 句子结束标记的索引\n",
    "        self.unk_idx = unk_idx  # 未知词标记的索引\n",
    "\n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        \"\"\"如果padding_first == True，则padding加载前面，否则加载后面\n",
    "        return_mask: 是否返回mask(掩码），mask用于指示哪些是padding的，哪些是真实的token\n",
    "        \"\"\"\n",
    "        max_length = min(self.max_length, add_eos + add_bos + max([len(text) for text in text_list]))  # 计算实际需要的最大长度\n",
    "        indices_list = []  # 初始化索引列表\n",
    "        for text in text_list:  # 遍历每个文本\n",
    "            indices = [self.word2idx.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]  # 将文本中的词转换为索引，如果词不在词表中则使用unk_idx\n",
    "            if add_bos:  # 如果需要添加句子开始标记\n",
    "                indices = [self.bos_idx] + indices  # 在序列开头添加BOS标记\n",
    "            if add_eos:  # 如果需要添加句子结束标记\n",
    "                indices = indices + [self.eos_idx]  # 在序列末尾添加EOS标记\n",
    "            if padding_first:  # 如果padding需要加在前面\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices  # 在序列前面添加padding\n",
    "            else:  # 如果padding需要加在后面\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))  # 在序列后面添加padding\n",
    "            indices_list.append(indices)  # 将处理后的索引添加到列表中\n",
    "        input_ids = torch.tensor(indices_list)  # 将索引列表转换为tensor\n",
    "        masks = (input_ids == self.pad_idx).to(dtype=torch.int64)  # 创建mask，1表示padding位置，0表示实际token位置\n",
    "        return input_ids if not return_mask else (input_ids, masks)  # 根据return_mask参数决定返回值\n",
    "\n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        text_list = []  # 初始化文本列表\n",
    "        for indices in indices_list:  # 遍历每个索引序列\n",
    "            text = []  # 初始化当前文本\n",
    "            for index in indices:  # 遍历序列中的每个索引\n",
    "                word = self.idx2word.get(index, \"[UNK]\")  # 将索引转换为词，如果索引不在词表中则使用\"[UNK]\"\n",
    "                if remove_bos and word == \"[BOS]\":  # 如果需要移除BOS标记且当前词是BOS\n",
    "                    continue  # 跳过这个词\n",
    "                if remove_eos and word == \"[EOS]\":  # 如果需要移除EOS标记且当前词是EOS\n",
    "                    break  # 结束当前序列的处理\n",
    "                if remove_pad and word == \"[PAD]\":  # 如果需要移除PAD标记且当前词是PAD\n",
    "                    break  # 结束当前序列的处理\n",
    "                text.append(word)  # 将词添加到当前文本中\n",
    "            text_list.append(\" \".join(text) if not split else text)  # 根据split参数决定返回连接后的字符串还是词列表\n",
    "        return text_list  # 返回处理后的文本列表\n",
    "\n",
    "# 两个相对于1个tokenizer的好处是embedding的参数量减少\n",
    "src_tokenizer = Tokenizer(word2idx=src_word2idx, idx2word=src_idx2word)  # 创建源语言(西班牙语)的tokenizer\n",
    "trg_tokenizer = Tokenizer(word2idx=trg_word2idx, idx2word=trg_idx2word)  # 创建目标语言(英语)的tokenizer\n",
    "\n",
    "batch_text = [\"hello world\".split(), \"tokenize text datas with batch\".split(), \"this is a test\".split()]\n",
    "indices,mask = trg_tokenizer.encode(batch_text, padding_first=False, add_bos=True, add_eos=True,return_mask=True)\n",
    "\n",
    "print(\"batch_text\"+'-'*10)\n",
    "for raw in batch_text:\n",
    "    print(raw)\n",
    "print(\"mask\"+'-'*10)\n",
    "for m in mask:\n",
    "    print(m)\n",
    "print(\"indices\"+'-'*10)\n",
    "for index in indices:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de27713",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38104f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    数据批处理函数\n",
    "    \n",
    "    Args:\n",
    "        batch: 批次数据\n",
    "        src_tokenizer: 源语言tokenizer\n",
    "        trg_tokenizer: 目标语言tokenizer\n",
    "        device: 设备，如果指定则将tensor移至该设备\n",
    "    \n",
    "    Returns:\n",
    "        包含编码后的tensor的字典\n",
    "    \"\"\"\n",
    "    src_texts = [pair[0].split() for pair in batch] #取batch内第0列进行分词，赋给src_words\n",
    "    trg_texts = [pair[1].split() for pair in batch] #取batch内第1列进行分词，赋给trg_words\n",
    "    \n",
    "    # 编码源语言输入\n",
    "    encoder_inputs, encoder_inputs_mask = src_tokenizer.encode(\n",
    "        src_texts, \n",
    "        padding_first=True, #padding加在前面\n",
    "        add_bos=True, \n",
    "        add_eos=True, \n",
    "        return_mask=True\n",
    "    )\n",
    "    \n",
    "    # 编码目标语言输入（用于训练时的teacher forcing）\n",
    "    decoder_inputs= trg_tokenizer.encode(\n",
    "        trg_texts, \n",
    "        padding_first=False, #padding加在后面\n",
    "        add_bos=True, \n",
    "        add_eos=False, \n",
    "        return_mask=False\n",
    "    )\n",
    "    \n",
    "    # 编码目标语言标签（用于计算损失）\n",
    "    decoder_labels, decoder_labels_mask = trg_tokenizer.encode(\n",
    "        trg_texts, \n",
    "        padding_first=False, \n",
    "        add_bos=False, \n",
    "        add_eos=True, \n",
    "        return_mask=True\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        \"encoder_inputs\": encoder_inputs,\n",
    "        \"encoder_inputs_mask\": encoder_inputs_mask,\n",
    "        \"decoder_inputs\": decoder_inputs,\n",
    "        \"decoder_labels\": decoder_labels,\n",
    "        \"decoder_labels_mask\": decoder_labels_mask\n",
    "    }\n",
    "    \n",
    "    # 如果指定了设备，将所有tensor移至该设备\n",
    "    if device is not None:\n",
    "        result = {k: v.to(device=device) for k, v in result.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aea7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs\n",
      "tensor([[   0,    1,   17,    6,   72,   15,    7,    4,    3],\n",
      "        [   1,   10, 3919,    7, 1753,  142, 2999,    4,    3]])\n",
      "encoder_inputs_mask\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "decoder_inputs\n",
      "tensor([[   1,    5,   46,   91,   14,  494,   71,    4,    0],\n",
      "        [   1,    9, 2038,    6, 2008,  261,  373,   15,    4]])\n",
      "decoder_labels\n",
      "tensor([[   5,   46,   91,   14,  494,   71,    4,    3,    0],\n",
      "        [   9, 2038,    6, 2008,  261,  373,   15,    4,    3]])\n",
      "decoder_labels_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "sample_dl = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#两次执行这个代码效果不一样，因为每次执行都会shuffle\n",
    "for batch in sample_dl:\n",
    "    for key, value in batch.items():\n",
    "        print(key)\n",
    "        print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc3fd4",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2829d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    序列到序列模型的编码器部分\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout=0.0):\n",
    "        \"\"\"\n",
    "        初始化编码器\n",
    "        \n",
    "        参数:\n",
    "        - vocab_size: 源语言词汇表大小\n",
    "        - embedding_dim: 词嵌入维度\n",
    "        - hidden_size: 隐藏状态维度\n",
    "        - num_layers: GRU层数\n",
    "        - dropout: Dropout比率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 词嵌入层 - 将输入的词索引转换为密集向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU层,batch_first=True表示输入的形状为[batch_size, seq_len]\n",
    "        # GRU是LSTM的简化版本,只有两个门控单元(更新门和重置门)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,  # 输入维度,等于词嵌入维度\n",
    "            hidden_size=hidden_size,   # 隐藏状态维度\n",
    "            num_layers=num_layers,     # GRU层数,可以堆叠多层\n",
    "            batch_first=True,          # 输入张量的第一个维度是batch_size\n",
    "            dropout=dropout if num_layers > 1 else 0  # 多层时才在层间使用dropout\n",
    "        )\n",
    "        \n",
    "        # dropout层 - 用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 保存配置参数供其他地方使用\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_lengths=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        - src: 源语言序列 [batch_size, seq_len]\n",
    "        - src_mask: 源语言序列的掩码 [batch_size, seq_len]\n",
    "        - src_lengths: 源语言序列的实际长度 [batch_size]\n",
    "        \n",
    "        返回:\n",
    "        - encoder_outputs: 编码器所有时间步的输出 [batch_size, seq_len, hidden_size]\n",
    "        - hidden: 解码器初始隐藏状态 [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        # 词嵌入并应用dropout正则化\n",
    "        embedded = self.dropout(self.embedding(src))  #[batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # 通过GRU层处理序列\n",
    "        # GRU的两个输出:\n",
    "        # 1. encoder_outputs包含每个时间步的隐藏状态,用于注意力机制\n",
    "        # 2. hidden是最后一个时间步的隐藏状态,用作解码器的初始状态\n",
    "        # 通过GRU\n",
    "        #[batch_size, seq_len, embedding_dim]-> encoder_outputs [batch_size, seq_len, hidden_dim]\n",
    "        #[batch_size, seq_len, embedding_dim]-> hidden [num_layers, batch_size, hidden_dim]\n",
    "        encoder_outputs, hidden = self.gru(embedded) \n",
    "        \n",
    "        # 返回编码器所有时间步的输出和解码器初始隐藏状态\n",
    "        return encoder_outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fee7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源序列形状: torch.Size([64, 20])\n",
      "编码器输出形状: torch.Size([64, 20, 512])\n",
      "隐藏状态形状: torch.Size([2, 64, 512])\n",
      "Encoder测试通过!\n"
     ]
    }
   ],
   "source": [
    "# 测试Encoder\n",
    "import torch\n",
    "\n",
    "# 创建测试参数\n",
    "vocab_size = len(src_tokenizer.word2idx)\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "batch_size = 64\n",
    "seq_len = 20\n",
    "\n",
    "# 实例化Encoder\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_size, num_layers, dropout)\n",
    "\n",
    "# 创建测试输入\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_len))  # [batch_size, seq_len]\n",
    "\n",
    "# 前向传播\n",
    "encoder_outputs, hidden = encoder(src)\n",
    "\n",
    "# 打印输出形状\n",
    "print(f\"源序列形状: {src.shape}\")\n",
    "print(f\"编码器输出形状: {encoder_outputs.shape}\")\n",
    "print(f\"隐藏状态形状: {hidden.shape}\")\n",
    "\n",
    "# 验证输出维度是否符合预期\n",
    "assert encoder_outputs.shape == (batch_size, seq_len, hidden_size)\n",
    "assert hidden.shape == (num_layers, batch_size, hidden_size)\n",
    "\n",
    "print(\"Encoder测试通过!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4493b22a",
   "metadata": {},
   "source": [
    "# Bahdanau注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "990d9a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., -inf, -inf]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟 logits 和 mask\n",
    "logits = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "mask = torch.tensor([[1, 1, 0, 0]])  # 只让前两个位置有效\n",
    "\n",
    "# 把无效位置设为 -inf\n",
    "masked_logits = logits.masked_fill(mask == 0, float('-inf'))\n",
    "masked_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9e931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau注意力机制\n",
    "    \n",
    "    参数:\n",
    "        hidden_size: 隐藏状态的维度\n",
    "        key_size: 键向量的维度（如果与隐藏状态不同）\n",
    "        value_size: 值向量的维度（如果与隐藏状态不同）\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, key_size=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 如果key_size和value_size未指定，则默认与hidden_size相同\n",
    "        if key_size is None:\n",
    "            key_size = hidden_size\n",
    "            \n",
    "        # 定义注意力层\n",
    "        self.Wq = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.Wk = nn.Linear(key_size, hidden_size)  \n",
    "        self.V = nn.Linear(hidden_size, 1, bias=False)  \n",
    "        \n",
    "    def forward(self, query, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            query:decoder的隐藏状态 查询向量 [batch_size, hidden_size]\n",
    "            keys: EO 键向量 [batch_size, src_len, key_size]\n",
    "            values:EO  值向量 [batch_size, src_len, value_size]\n",
    "            attn_mask: 注意力掩码 [batch_size, src_len]\n",
    "            \n",
    "        返回:\n",
    "            context: 上下文向量 [batch_size, value_size]\n",
    "            attention_weights: 注意力权重 [batch_size, src_len]\n",
    "        \"\"\"\n",
    "        src_len = keys.size(1)  # 输入序列的长度\n",
    "        \n",
    "        # 将query从[batch_size, hidden_size]转换为[batch_size, 1, hidden_size]\n",
    "        query = self.Wq(query).unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # 转换keys,shape=[batch_size, src_len, hidden_size]\n",
    "        keys = self.Wk(keys)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        energy = torch.tanh(keys+query)\n",
    "        # [batch_size, src_len, hidden_size] -> [batch_size, src_len, 1] -> [batch_size, src_len]\n",
    "        attention = self.V(energy).squeeze(2) \n",
    "        \n",
    "        # 应用注意力掩码（如果提供）\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask * -1e16 \n",
    "            attention += attn_mask #加上一个负无穷，让padding部分经过softmax后为0\n",
    "        \n",
    "        # 使用softmax归一化注意力权重  [batch_size, src_len]\n",
    "        attention_weights = F.softmax(attention, dim=1)\n",
    "        \n",
    "        # 计算上下文向量,values是EO,shape=[batch_size, src_len, hidden_dim]\n",
    "        context_vector = torch.mul(attention_weights.unsqueeze(-1), values).sum(dim=1) #对每一个词的score和对应的value做乘法，然后在seq_len维度上求和，得到context_vector\n",
    "        # context_vector.shape = [batch size, hidden_dim]\n",
    "        #attention_weights用于最后的画图\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79afe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query shape: torch.Size([2, 8])\n",
      "Keys shape: torch.Size([2, 4, 8])\n",
      "Values shape: torch.Size([2, 4, 8])\n",
      "Context vector shape: torch.Size([2, 8])\n",
      "Attention weights shape: torch.Size([2, 4])\n",
      "\n",
      "Attention weights:\n",
      "tensor([[0.0000, 0.4765, 0.2317, 0.2918],\n",
      "        [0.2452, 0.2049, 0.2382, 0.3118]], grad_fn=<SoftmaxBackward0>)\n",
      "Encoder输出形状: torch.Size([1, 3, 4])\n",
      "Decoder隐藏状态形状: torch.Size([1, 4])\n",
      "注意力权重形状: torch.Size([1, 3, 1])\n",
      "注意力权重值:\n",
      " tensor([[[0.8072],\n",
      "         [0.0852],\n",
      "         [0.1077]]])\n",
      "上下文向量形状: torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFQCAYAAAAMWiHwAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAANpNJREFUeJzt3Qm8TeX6B/Bno+OYhZAhc+ZZpq4pZChR6qJcQ7j1FwlxTRmipCLduIQo0yUVKnMyhk6GRDeSzBkzD8fh7PX//B537bv3Ofsce69z9jl7rfP79lkfZ6+9195rLdrPed/3eZ/XZRiGIURERDaQLrVPgIiIKFAMWkREZBsMWkREZBsMWkREZBsMWkREZBsMWkREZBsMWkREZBsMWkREZBsZUvsEiIgoeURHR0tMTIzl4yMiIiQyMlLCGYMWEZFDAlaxIlnl1JlYy++RP39+OXToUFgHLgYtIiIHiImJ0YB1aEcRyZ4t+JGfy1fcUqz6EX0fBi0iIkoRWbLe2YIVa5MqtAxaREQO4hZDNyvH2QGDFhGRg7j1P2vH2QFT3omIyDbY0iIicpBYw9DNynF2wKBFROQgbo5pERGRXbjFkFgGLSIisgM3W1pERGQXsQ4f02L2IBER2QZbWkREDuL+72blODtg0CIicpBYi4kYVo5JDQxaREQOEmtYqyPI2oNERJTi3OweJCIiu3CLS2LFZek4O2D2IBER2QZbWkREDuI27mxWjrMDBi0iIgeJtdg9aOWY1MCgRUTkILEMWkREZBduw6WblePsgEGLiMhBYh3e0mL2IBER2QZbWkREDhIr6XQL/jh7YNAiInIQw+KYFo6zAwYtIiIHiXX4mBaDFhGRg8Qa6XQL/jixBQYtIiIHcYtL3BbGtNw2WZqE2YNERGQbbGkRETlILMe0iIjI+WNahtgBgxYRkePGtFyWjrMDBi0iIgdxW5xcbJdEDAYtIiIHiXV49yCzB4mIyDbY0iIiclj3oJvdg0REZAexhks3K8fZAYMWEZGDxFqu8s6WFhERpTC3kU634I9j0CIiohQW6/CWFrMHiYjINtjSIiJyELfFpAocZwcMWkREDuK2nPJuj443Bi0iIgeJtVwRwx5Byx5nSUREQRXMtbIFa/LkyVK0aFGJjIyUWrVqSVRUVEDHLViwQFwul7Rp0yboz2TQIiJyYEsr1sIWjIULF0q/fv1kxIgRsnPnTqlcubI0a9ZMzpw5k+hxhw8flldffVXq1atn6foYtIiIKGgTJkyQHj16SNeuXaVcuXIydepUyZw5s8ycOTPBY2JjY+W5556TUaNGSfHixYP/UAYtIiJnztOKtbDB5cuXfbabN2/G+4yYmBjZsWOHNGnSxLMvXbp0+njr1q0Jntvrr78uefPmlW7dulm+PgYtIiIHcRsuyxsULlxYcuTI4dnGjh0b7zPOnTunraZ8+fL57MfjU6dO+T2vzZs3y0cffSTTp09P0vUxe5CIyEHclheBvHPMsWPHJHv27J79GTNmTPI5XblyRf72t79pwMqTJ0+S3otBi4jIQdyWaw/eOQYByzto+YPAkz59ejl9+rTPfjzOnz9/vNcfPHhQEzBatWr1v89z35nOnCFDBtm/f7+UKFEioPNk9yARkYPEisvyFqiIiAipXr26rF271icI4XGdOnXivb5MmTKyZ88e+fHHHz3bE088IY0aNdKf0SUZKLa0iIgoaEh379y5s9SoUUNq1qwpEydOlGvXrmk2IXTq1EkKFiyoY2KYx1WhQgWf43PmzKl/xt1/NwxaREQO4k5i92Cg2rVrJ2fPnpXhw4dr8kWVKlVk5cqVnuSMo0ePakZhcnMZhk0WUSEiogQhPR3ZfsO/byKRWe+RYEVfvSWv1/pGLl26dNcxrdTElhYRkYO4U6illVoYtIiIHCTW4QVzGbSIiBzEsFj8FsfZgT1CKxEREVtaRETOEuvw7kF7nCVRiGBNn5EjR4pTNWzYUDerxwY7h4bsX3sw3DFokWX/+te/9Esfi7/585///EcDAsq3+Dv2448/ToGzFFm+fHlYBaa3335b79uuXbt89mP2yb333qvPHTp0yOe56OhorQH37LPPSrj5448/9P6isgHZv8p7uLPHWVJYmjdvnq5aitVKf/vtN79BC+vmhEPQwnn4c+PGDRk2bJikpL/85S+eqtfefv75Z7l48aLWYvvuu+98nvvhhx90OQjz2ECtXr1at1AHLdxfBq3w4GZLiyg+tAS2bNmiC8Hdd999GsDsCOVlECRSEsre4HPjBi0Eqty5c0vjxo3jPWc+DjZooUYcNko73JLO8mYH9jhLCjsIUujKeuyxx+Tpp5+OF7TQinrmmWf0ZxTFRJcXtvXr12vrDK2KDRs2ePZ7j7ugtfHKK69oEU10iZUsWVLGjRvnqQoNaL3huHfffVemTZumFaLx2oceekhbJaYuXbrI5MmT9Wfzs7AlNqaFbrsWLVpoVYCsWbNqENm2bVu868OxCDSowYbAnSVLFnnyySe1tE1iEERwnnFbU3iMYqMPP/yw3+dQq80cY8K9QK238uXLawBE6ZwXXnhBLly4cNcxrSNHjmixUpwvFuTr27evrFq1yvP346/FjL9DrEqLWnLo3jTh9bgWQM058/6aregDBw5I27ZttfI3zrNQoULSvn17rbpAZAWzB8kSBKmnnnpKv4A7dOggU6ZM0WBhfoHVr19fXn75ZfnnP/8pQ4YMkbJly+p+/Ikv2969e2tAGDp0qO4365Vdv35dGjRoICdOnNAv4QceeEBbdIMHD5aTJ0/qsd7mz5+va/XgtfiyxBcqzuv333+Xe+65R/ej+2rNmjUyZ86cu14Xgmm9evU0YA0cOFDf48MPP9QvfgTZuON3uA4E7xEjRmggxfn16tVLFi5cmOjnoMW0adMmPQZB3AxM3bt31+KjeD8EbwQqjHXhHiCgmbXccF0IDAgUuM9o+U6aNEkDLt4H5+0PCpo+8sgjei/79OmjwQT3cN26dX5fjyDYvHlzvad//etf5bPPPpN//OMfUrFiRQ3s+PvEarSoP/f3v/9d7x3UrVtXuzObNWumK9/iPuGz8Pf69ddf67Wh5BAlv1jDpZuV42wBtQeJgrF9+3bUqzTWrFmjj91ut1GoUCGjT58+Pq9btGiRvm7dunXx3qN8+fJGgwYN4u0fPXq0kSVLFuPXX3/12T9o0CAjffr0xtGjR/XxoUOH9L1z585tnD9/3vO6pUuX6v6vvvrKs++ll17Sff5g/4gRIzyP27RpY0RERBgHDx707Pvjjz+MbNmyGfXr1/fsmzVrlh7bpEkTvX5T37599TwvXrxoJGbZsmV6/Jw5c/TxyZMn9fGGDRuMK1eu6HvgNbB371597o033tDHmzZt0sfz5s3zec+VK1fG24977H2fx48fr69ZsmSJZ9+NGzeMMmXKxPu7wnHYN3v2bM++mzdvGvnz5zfatm3r2ffDDz/o63BPvO3atUv3498Bhd6lS5f0fr+wsa3x8s72QW84DsfjfcIZuwfJUisLLSN0GQFaOKj4vGDBAl2COykWLVqkv62j9YIlvc2tSZMm+t4bN270eT0+F681mb/po6UVLLw/khbatGkjxYsX9+y///77NWsP40ooSuoNrQvv7kZ8Pt4HXXCJQUsErSZzrMpsHaGlihZopUqVPF2E5p/meBbuEVopTZs29blHWN8IxybUagJU4UYXH7oHTei269Gjh9/X4/06duzoeYyWNVqCgdxfsyWFrke0oCllGP+tPRjsZpdwYI+zpLCBL2QEJwQsdEkhaxAbus2waqn3onBWYAwEX6wYI/LeELTgzJkzPq9H96E3M4DFHdsJBMai8OVaunTpeM+hGwzjSFiKPDk+H91+GI/yDkxVq1aVTJkyeYKa93NmsDDvEcaEMB4V9z5dvXo13j3yhmCK8T/vQAsYN/QHY1BxX4trDOT+FitWTMf7ZsyYoSvdoqsQ44scz7L/IpCpiWNaFJRvv/1Wx0MQuLD5a4U9+uijlt8fgQEtCIwn+fPggw/6PMaS3/6k1Io7Sfl8tJymTp2q4zsITAhUJvw8c+ZMuXXrlrbG0IpCi8i8RwhYCWVsIngll6Te3/Hjx2syzNKlS7UVi/E3LAqIxBYEREp+buNO2ruV4+yAQYuCgi9KfGGaGXnevvjiC1m8eLF+EaPFEPc3dG8JPYdWAFoLZssqOSR2HnG/7JEht3///njP7du3T7vzglkWPJCghQSWb775RhMoBgwY4BO0MIds2bJl2hWHDDzve4RjkGVotswCVaRIEc0GRNDxvi/+5tkl1/1F0gY2zIdDQgnOG/9GxowZY/kzKe1i9yAFDF+iCEyPP/64prnH3ZA1h0y+L7/8Ul+PlGpASyIuPOdvPzLUtm7dquMgceH1t2/fDvq8EzuPuK0KtBLRKvCeEI1uT2TYIcgk5+J45hgV5rqhReXd0kJGIcbSzPRy7/lZuEfoph09enS898T9Sew60UWHDD7z78istjF9+nTL15HQ/cX4X9y/LwQvBH9kFFJouC2OaXE9LXIcfNEhKHkP4nurXbu2Z6IxEiSw/DYCAeZYYRwD86iQbo2WGrq70MrAb9sYT8E+PIfWBj4HgRHdSngd0rT37Nmj6dYIJhgfCQbeA9A1hS9tnBPmCvmD80F6PIJEz549deIxUt7xJes9Pyk5YDwMLTcEaQSpAgUK+DyPIPb5559rSwatExOmBCDlHd1sqEKBQIskDox1IUnj/fff118i/MFxSI3HNAWkvCMw4u/L7HoMtFXqDS0/jNGh9ZQtWzYNYhjj3L17t/4ig/l66NZFAMO0A9x/75YjJS+3xaVJrByTGhi0KGDmlxvGnPzBb9CYbIzX/fnnnzovB19k+HLt1q2btg6Q2YYAhXk9SApAIEAgxBcxgha65zAf6s0339Qv4NmzZ2vrBl96KBVkZW4P5hhhnhDG4ObOnatdYwkFLSRHYP4U5oXhvDF+hC9gHJdQjcWkQHD897//7dPKMiFQIWiVKVNGK2V4w31FMEZAxTw4BFcEPmT6eQc4f9mAGJfE/UBww+NOnTrp5yOQmMErGAiYn3zyid6zF198UYPTrFmz9O8UvyR89dVX2rrD323lypVlxYoV+gsOhUasw+dpuZD3ntonQUSpC5OiURnj+PHjmhJP9nP58mX9pa792o4SkTX40l0xV2NkQeO52iuSnN3gyc0enZhElKxjk94wpoUWW6lSpRiwKOyxe5AojUF3KcbTMOaI36rR9YnsSLsWPSY/Y1pWUt45pkVE4QjjTJjwiyCFccZy5crpeB+SZ8j+DIuJGDjODmzTPXj+/Hl57rnntK8VmUoY2Md8nsSgyKl3ZW9sGCgmSstQQX/v3r36/w+6Cnfs2MGA5SBuh6+nZZuWFgIWKjEgHRlzWlDdGnXfMH8mMaiphirUJmQwERE5ldvinCvO00pGv/zyi9ajw9IXWEAPPvjgA2nZsqWupxR3fos3BCmkXhMRpQVui60mtrSSESZfokvQDFiAMj+YF/T999/rwnsJQb89BpoRuFq1aiWvvfZaoq0tTCL1nq2PeTromsQ8GSsTL4mI7gYzjzBfEb+Am2umkY2D1qlTp3RCqjdMpsyVK5c+lxAsJ4Faa/iH8NNPP+nidagrh1JECcGEUkxiJSJKaVhFIKmFhN2siBE6gwYN0hI/d+satApjXt41z1CyBkunHzx4UEvP+INZ/VhOwYSUYKQH/0VaSgbxvxpsWrX41z2pfQphq8qn3VL7FMJS+kLXUvsUwpL7xk05/H/jtQxWkt/LYPdgyPTv31/ryyUGi/Ghay/uGkEoFYNuu2DGq8wyPKhonVDQQn08bHEhYGVwMWh5y56N3RgJSWehHFJakC5z0hYJdbrkGIJwM2iFjrlw3d3UqVNHK0gjNdcsfor6aWZduEChuCigxUVE5ERuhwctW/yqjFVjmzdvrunrUVFRumAeqkej6KmZOYiCnCgsiucBXYBYugGBDpXBUTkchUHr16+vS5kTETmR2+HztGwRtMwsQAQljEkh1R3VsadNm+Z5HnO3kGSB5dIBy5NjoTws24Dj0BWJKtaoOE1ERPZki+xBQKZgYhOJsSyDd8F6rFOEJS6IiNISw2ImoF2W+7BN0CIiortz+pgWgxYRkYO4GbSIiMgu3AxaRERkF26HBy3bZA8SERGxpUVE5CCG4dLNynF2wKBFROQgbhbMJSIiu3A7fEyLQYuIyEEMdg8SEZFduB3e0mL2IBER2QZbWkREDmKwe5CIiOzCsNg9yKBFREQpztAAZO04O2DQIiJyELe49D8rx9kBgxYRkYMYDh/TYvYgERHZhqWgNWfOHHn44YelQIECcuTIEd03ceJEWbp0aXKfHxERWZin5bawOTJoTZkyRfr16yctW7aUixcvSmxsrO7PmTOnBi4iIko9hmF9c2TQ+uCDD2T69OkydOhQSZ8+vWd/jRo1ZM+ePcl9fkREZGFMy7CwOTJoHTp0SKpWrRpvf8aMGeXatWvJdV5ERBTmQWvy5MlStGhRiYyMlFq1aklUVFSCr/3iiy+0cYNeuSxZskiVKlV0qCnkQatYsWLy448/xtu/cuVKKVu2bNAnQERE9hvTWrhwoQ4VjRgxQnbu3CmVK1eWZs2ayZkzZ/y+PleuXNpDt3XrVvnpp5+ka9euuq1atSq0Ke84yZdeekmio6PFMAyNrP/+979l7NixMmPGjGDfjoiIbGjChAnSo0cPDTwwdepUWbZsmcycOVMGDRoU7/UNGzb0edynTx/55JNPZPPmzRrsQha0unfvLpkyZZJhw4bJ9evX5dlnn9Uswvfff1/at28f7NsREVEyMiwmVZjHXL58Od7QDzZvMTExsmPHDhk8eLBnX7p06aRJkybakrr7Zxny7bffyv79+2XcuHGhT3l/7rnn5MCBA3L16lU5deqUHD9+XLp16yYpIZg+VFi0aJGUKVNGX1+xYkVZvnx5ipwnEVHqBS2Xhe3O8YULF5YcOXJ4NvSixXXu3DnNHM+XL5/PfjxGTEjIpUuXJGvWrBIRESGPPfaYJvY1bdo05SpiZM6cWbeUYvahohmKgIUUezQrEa3z5s0b7/VbtmyRDh066E1//PHHZf78+dKmTRvtf61QoUKKnTcRkV0qYhw7dkyyZ8/u2R+3lZUU2bJl05wINHjWrl2r3+fFixeP13WY5KCFbEGXK7CbgIAQLn2o6LJs3ry5DBgwQB+PHj1a1qxZI5MmTdJjiYgcWTBXrB0HCFjeQcufPHny6JSn06dP++zH4/z58yd4HLoQS5YsqT8je/CXX37RRkWyBy20TlKblT5U7Eck94aW2ZIlSxL8nJs3b+pmitu/S0SU1msPRkRESPXq1bW1ZMYHt9utj3v16hXw++AY7+/bZAtaSGlMbYn1oe7bt8/vMehbDbbPFVF/1KhRyXTWRETO1K9fP+ncubPOvapZs6YO12CurtkT1qlTJylYsKBnTAx/4rUlSpTQQIX8AszTQpWlFBnT2r59uzbtoFy5chp1nQAtOe/WGVpaGJgkIkoT/YMBateunZw9e1aGDx+uDQF092G+rtlQOHr0qPaGmRDQevbsqYl7yEBHgtzcuXP1fUIatPCBSG747rvvdGYzoAZh3bp1ZcGCBVKoUCEJBSt9qNgfbJ+rv/ROIiLbMCyWZLJwDLoCE+oOXL9+vc/jMWPG6JZU6azM07p165a2ss6fP68bfkbfJJ4LFe8+VJPZh1qnTh2/x2C/9+sBiRgJvZ6IyO4MhxfMDbqltWHDBk0lL126tGcffka+fb169SSc+lAx47pBgwYyfvx4nROAliC6NadNmxbS8yQiSi2GwxeBDDpoYXwHLa24kCSByhihFGwfKrosMTcL1TuGDBkipUqV0sxBztEiIscyXJa6+iwdY4eg9c4770jv3r21MgVaPIDWC1o17777roRaMH2o8Mwzz+hGRET2F3TQ6tKli9YcREWKDBnuHH779m39+fnnn9fNhPEuIiKyT+1BxwUtrk5MRBTGjJRJebdN0EIiBBERhSeDiRj+YaEvbEg791apUqXkOC8iIrLKEMcKOmih/h9aW5ibhTVRvKGoLrIIiYgodRhsaflCosWDDz4oH330kaaaB1r9nYiIKMWD1u+//y6ff/65p7w8ERGFEcPZiRhBl3Fq3Lix7N69OzRnQ0RESeRKwubAltaMGTN0TGvv3r1aWeKee+7xef6JJ55IzvMjIqJgGM5uaQUdtLCwIiq8r1ixIt5zTMQgIkplhrODVtDdgyjh1LFjRzl58qSmu3tvDFhERGFSe9CwsDkxaP3555/St2/feCsCExERhV3Qeuqpp2TdunWhORsiIkoSg+tp+cIcLSxJv3nzZqlYsWK8RIyXX345Oc+PiIiCYTh7TMtS9mDWrFl1MUhscRMxGLSIiFKRwfW0fBw6dCg0Z0JEREnmMu5sVo5zdMFcIiIKQwa7B+M5fvy4fPnll7q8fUxMjM9zEyZMSK5zIyIiSlrQWrt2rVa9KF68uOzbt0+rYhw+fFgrvlerVi3YtyMiouRkOHtMK+iUd2QOvvrqq7Jnzx6JjIzU4rnHjh2TBg0ayDPPPBOasyQiouC6B61sTgxaWEerU6dO+nOGDBnkxo0bmk34+uuvy7hx40JxjkREFCiDQctHlixZPONY999/vxw8eNDz3Llz55L37IiIKDiGs4NW0GNatWvX1onFZcuWlZYtW0r//v21q/CLL77Q54iIKBUZzh7TCjpoITvw6tWr+vOoUaP054ULF0qpUqWYOUhEROEVtJA16N1VOHXq1OQ+JyIissjl8MnFQY9peYuOjpZPPvlEpkyZIr/99pukhMmTJ0vRokU1c7FWrVoSFRWV4Gs//vhjLS3lveE4IiLHMjimpfr16ye3bt2SDz74QB8jGaNOnTry888/S+bMmWXAgAGyZs0a3Rcq6IbEeaB1h4A1ceJEadasmezfv1/y5s3r95js2bPr8yYELiIisqeAW1qrV6+Wpk2beh7PmzdPjhw5IgcOHJALFy7oHK0xY8ZIKGHMrEePHtK1a1cpV66cBi8EzJkzZyZ4DIJU/vz5PRvXASMiJ3N5dREGtYnDWloo2YRA4R3Enn76aSlSpIg+7tOnj2YThgpadjt27NDJzaZ06dJJkyZNZOvWrQkeh0QRnCNWVkbFjjfffFPKly+f4Otv3rypm+ny5cv658G3a0i6TOxa9FZ2at3UPoWwZeR0p/YphKWsmaNT+xTCUqz87zuHkqmlhQCBUk2mbdu2+aS458yZU1tcoYI5YLGxsfFaSnh86tQpv8eULl1aW2FLly6VuXPnauCqW7eu1k5MyNixYyVHjhyerXDhwsl+LUREIU95NyxsTgpamJf11Vdf6c8Yx0LLq1GjRp7n0VUYbl1vGF9D9Y4qVapomSnMJbvvvvvkww8/TPAYtOQuXbrk2VCiiojINgwmYqiBAwdK+/btZdmyZRq00BVYrFgxz/PLly+XmjVrhuo8JU+ePJI+fXo5ffq0z348xlhVILDKctWqVRPNdMyYMaNuRES2ZDh7aZKAW1pPPvmkBqZKlSpJ3759NZPPGxIievbsKaESEREh1atX1yrzJnT34XGgGYvoXkT1DpSfIiJyIpeVJAyLc7vCfnJx48aNdfNnxIgREmpId+/cubPUqFFDW3VIeb927ZpmEwK6AgsWLKjjUoAivhh3K1mypFy8eFHeeecd7cbs3r17yM+ViChVGM5uadlq5eJ27drJ2bNnZfjw4Zp8gbGqlStXesbSMM6GhBETEkOQIo/X3nvvvdpS27Jli08WJBER2Yetghb06tVLN3/Wr1/v8/i9997TjYgozTDY0iIiIptwObz2IIMWEZGTGFyahIiI7MJwdvdg0FXeMS/qb3/7mxQoUEAyZMigc6e8NyIiSj0uprz76tKli2bpvfbaazrfiVXTiYgobIPW5s2bZdOmTZpuTkREYcZwdvdg0EELBWS9C+cSEVEYMSx29Tl1TAtVKAYNGiSHDx8OzRkREZF1BgvmajUJ77ErlE4qUaKE1htEEVpv58+fT/6zJCKiwLB78E7rioiIwp+Lk4tFi9QSERHZbkwLy5OsWrUq3v7Vq1fLihUrkuu8iIiIkh60kISBdaniwtpWeI6IiFKR4exEjKCD1oEDB/wu7VGmTJlEVwQmIiJnVcSYPHmyFC1aVCIjI6VWrVoSFRWV4GunT58u9erV08Q+bE2aNEn09ckWtHLkyCG///57vP0IWFmyZAn6BIiIKJkZoW9lYfV6LMyLBYB37twplStXlmbNmsmZM2cSXDqqQ4cOsm7dOtm6davO+X300UflxIkToQ1arVu3lldeeUUOHjzoE7D69+8vTzzxRLBvR0RENuwenDBhgi6yi5Xj0fs2depUnQY1c+ZMv6+fN2+e9OzZU6spoWduxowZOqy0du3a0Aatt99+W1tU+NBixYrpVrZsWcmdO7e8++67wb4dERGFkcuXL/tsN2/ejPeamJgY2bFjh3bxmbBqPB6jFRWI69evy61btyRXrlyhLeOE7kEsWb9mzRrZvXu3ZMqUSSpVqiT169cP9q2IiCjM5mkVLlzYZz+6/0aOHOmz79y5c5qQly9fPp/9eLxv376APu8f//iHrhbiHfhCErRmz54t7dq1075IbN6Rd8GCBdKpU6dg35KIiMKkIsaxY8cke/bsnt0ZM2aU5PbWW29pvMA4F5I4Qto9iP7LS5cuxdt/5coVfY6IiOybPZg9e3afzV/QypMnj66fiPUVveFx/vz5Ez0/DCMhaGFuL3rpghV00EKFd39raB0/fly7DomIyNmJGBEREVK9enWfJAozqaJOnTqJ5kSMHj1aVq5cKTVq1LB0eQF3D1atWlWDFbbGjRvrqsUm9G0eOnRImjdvbukkiIjIXgVz+/XrpyX+EHxq1qypNWpRTN3sccNQUcGCBWXs2LH6eNy4cTJ8+HCZP3++zu06deqU7s+aNatuyR602rRpo3/++OOPmovv/SGIujiJtm3bBn7FRERkW+3atZOzZ89qIEIAQio7WlBmcgZWuEdGoWnKlCma+/D000/fNdEjWYIW3hgQnHCywQ6eERGRs6q89+rVSzd/kGThLbnWYAx6TAvNwdQKWBs3bpRWrVppmiS6KZcsWXLXY3DjqlWrpoOJJUuWlI8//jhFzpWIKFUYrD3oA+NXyP5AHyayRDAxzHsLJfSXolQI6l0FAuNsjz32mDRq1Ei7NVHJo3v37n6r1BMROYLh7KAV9DytUaNGafkNlG0aNmyYDB06VJt9aPWgbzOUWrRooVugUFYEFTvGjx+vj1G5Y/PmzfLee+/puBwRkdO4HL4IZNAtLdSPQrVeBC1kEKIAIoIYAta2bdsknKCcSNzZ1ghWiZUZQcmSuGVMiIhsw3B2SyvooIUskYoVK+rPyCA0Jxo//vjjsmzZMgknOFd/ZUYQiG7cuOH3GKRnYr6ZucUtaUJERDYKWoUKFZKTJ0/qzyVKlNBZzfDDDz+EpNxHShs8eLAGYnNDSRMiIrtwpeB6WrYY03ryySd11jMW/Ordu7d07NhRPvroI83J79u3r4QTJIr4KzOC0iQo9OsPAq8Tgi8RpVFGykwutk3QQs0oE+ZrPfDAAzpGVKpUKU1HDycoJ7J8+XKffahOn1iZESIiWzMYtBKFAJBSQeDq1au64KR3SjtS2ZFqj+CJrj2sgolK9PDiiy/KpEmTZODAgfL888/Lt99+K59++mnYjb0RESUX1383K8c5Mmj9+eefuuAjYLwHmYRIasCqxfXq1ZNQ2r59u8658q59ZU54xqRhjLWhm9KEdHcEKHRbvv/++zoeh0xHprsTkWMZbGmpPXv2aPcfAhW6ArEWCgrkYsIv6kth7tNnn33mqVEYCg0bNtQq8wnxV+0Cx+zatStk50RERGGYPYguNqS6o5QSAgFS3FFtAhl2Fy5ckBdeeMFnvIuIiFKei9mD4klpx5gQFu1CKaVp06ZJz549PVV8kUlYu3btUJ4rERHdDbsH7zh//rxnRUpMKs6SJYvce++9nufxM1YvJiKiVGaIYwWViBF3xWJ/KxgTEVHqcTm89mBQQatLly6eibfR0dGaUo4Wl1mzj4iIUpnB7kFPWrk3VMKIC8srExERpXrQmjVrVshOgoiIkoeL3YNERGQbBrsHiYjIJlxsaRERkW0YbGkREZFdGM4OWkEvAklERJRa2NIiInIQF8e0iIjINgxndw8yaBEROYjLMHSzcpwdMGgRETmJwZYWERHZhMvhY1rMHiQiIttgS4uIyEkMdg8SEZFNuBzePcigRUTkJAZbWkREZBMutrSIiMg2HN7SYvYgERHZhq2C1saNG6VVq1ZSoEABcblcsmTJkkRfv379en1d3O3UqVMpds5ERKnVRegKYrMLWwWta9euSeXKlWXy5MlBHbd//345efKkZ8ubN2/IzpGIKFUZhvXNBmw1ptWiRQvdgoUglTNnzpCcExFROHExEcP+qlSpIjdv3pQKFSrIyJEj5eGHH07wtXgdNtOlS5f0T3d0dIqcq53E3kyf2qcQttzR7tQ+hbAUe/1//29R/PtiJEdrx3B2Ioajg9b9998vU6dOlRo1amggmjFjhjRs2FC+//57qVatmt9jxo4dK6NGjYq3/8TwN1LgjIkoLbty5YrkyJEjSe/hct/ZrBxnBy4jWUJ7ykNCxeLFi6VNmzZBHdegQQN54IEHZM6cOQG1tNxut5w/f15y586tn5maLl++LIULF5Zjx45J9uzZU/VcwgnvS8J4b+xxX/A1jICFJLN06dJZvqYcOXLIQ0+OkQz3RAZ9/O1b0fLD4mHauxQO9yRNtrT8qVmzpmzevDnB5zNmzKibt3AbD8M/qHD+R5VaeF8SxnsT/vclqS0sD3YPOsuPP/6o3YZERE7kYiJG+Lh69ar89ttvnseHDh3SIJQrVy7t8hs8eLCcOHFCZs+erc9PnDhRihUrJuXLl5fo6Ggd0/r2229l9erVqXgVREQhZFhMX7fJSJGtgtb27dulUaNGnsf9+vXTPzt37iwff/yxzsE6evSo5/mYmBjp37+/BrLMmTNLpUqV5JtvvvF5DztBt+WIESPidV+mdbwvCeO9SXv3xeXwlpZtEzGIiCh+IkatVqMtJ2J8/9VrTMQgIqIUZDARg4iIbMLl8O5BBi0iIicxmIhBREQ24XJ4S8tWVd7TOlS3L1q0qERGRkqtWrUkKipK0rpgl6tJK1CO7KGHHpJs2bJpwWhUjsFqB2ndlClTNIvYnFRcp04dWbFihThyTMuwsNkAg5ZNLFy4UFP8kaa7c+dOXaKlWbNmcubMGUnLrC5X43QbNmyQl156SbZt2yZr1qyRW7duyaOPPqr3Ky0rVKiQvPXWW7Jjxw6dQvPII49I69at5eeff07tU6MAMeXdJtCywm/OkyZN8tRERO203r17y6BBg1L79GxdjzItOHv2rLa4EMzq16+f2qcTVlCc4J133pFu3bqJE1Le6zZ73XLK+5ZVw8M+5Z0tLRvAJGn8ZtikSRPPPhTVxOOtW7em6rmRPZhL7OALmu6IjY2VBQsWaOsT3YSO4TasbzbAoGUD586d0//B8uXL57Mfj0+dOpVq50X2gFb5K6+8ouvIYU25tG7Pnj2SNWtWrYbx4osvauu8XLly4hhGyo1pBTPOji7Ytm3b6uvRK4Iye1YwaBE5HMa29u7dq60KEildurTWLMW6ev/3f/+nZeD+85//iFO4vDIIg9pCPM5+/fp1KV68uI4p5s+f3/L1MWjZQJ48eSR9+vRy+vRpn/14nJS/fHK+Xr16yddffy3r1q3TJAQSiYiIkJIlS0r16tU1yxJftu+//744bp6WYWELwoQJE6RHjx7StWtXbaliwV3UeJ05c6bf12NMHmOH7du3T1LNRwYtm/xPhv/B1q5d69Plg8eO6ounZIP8KgQsdH1hZQOsdkD+4f8l74Vf07rLly/7bP7uTWqOs3NysU2gGY5ujBo1auhClugPxgAyfstJy+62XE1a7hKcP3++LF26VOdqmWOfyC7LlCmTpFVYvqhFixb6bwMrBeMerV+/XlatWiVO4Uri5GJkJXtD99/IkSMDHmfft2+fhBKDlk20a9dO05aHDx+uX0BVqlSRlStXxvtHk9bcbbmatDyJFho2bOizf9asWdKlSxdJqzDe0qlTJ13GCAEcE40RsJo2bSqOYSStYO6xY8d8Ut7DbfkWBi0bQXcPNvoffClzqmF8vCf+ffTRR+J0LsPQzcpxYFYLCddxdo5pERE5iTsJmw3G2dnSIiJyEFcSW1rJNc6ObtiCBQtqhqaZvGFOLcDPWFEe48+YM4dszkAxaBERUbKPsx89elQzCk1//PGHVK1a1fP43Xff1a1BgwaaDBMo1h4kInKAy/+tPVj/L8MlQwYLtQdvR8vGza+Hfe1BtrSIiJzE4CKQRERkEy6HLwLJoEVE5CQGW1pERGQTLvedzcpxdsB5WkSUJKiwcbeFN5EdhuUoLl68mGLnRc7EoJUGIU0VSzKg/hpKtGAGO5YU+O6778SOnHY9oYCAYW7IMMPaWiikmxxQId27ZBaqlGD9Lm9169b1lE4iZ1R5Ty3sHkyDsBAbJvd98sknur4NSq9gJvuff/4Z0s/FZ2ImvVOux25Qd7B58+Za7HTo0KHy+OOP6zpbuGdJEUggwt87l9GxR+3BcMeWVhqD7plNmzbJuHHjtNBskSJFdDY7ql8/8cQTntdhYmDr1q11tjrmbPz1r3/1qTPmr0sIv117F2jFz6iViP2oVYbWj7mCKb4w8b6oQF6vXj05ePCg57gZM2ZI2bJldTXUMmXKyL/+9a8kXw9e1717d7nvvvv0cx955BHZvXu3z3thcTpMjMQ5devWTQYNGqQTJhNrQeAeeBegxTIOr776qlYCyJIli67m6j1xEi2SnDlzapFWXCPuLwIJWiHesCZR+fLlteV4//33+9ScDORa/MHnInBg9WIU1L1x44asWbNGn9uwYYPeN/PzcO23b9/2HPvZZ59JxYoVtUJ87ty5dQkKVD+I+28BP+O90PoyW3aHDx/22z34+eefe64Rq9mOHz/e53yx780335Tnn39e/07Qkp42bdpdrzOtc/23IoaVzQ4YtNIYfEliW7JkSYJrCKGGGALW+fPn9QsIX2y///67zoAPFlo/+C0bXXVYJA6lW+rXr69fVOiewpo8+FIyvyDnzZunM+zfeOMN+eWXX/RL67XXXtP3sXo98Mwzz2iF7xUrVuhnVqtWTRo3bqzXCJ9++qkuv4DPQ+V4fHEnFiwTguCC9YSwSvBPP/2kn4ugdODAAZ8VXFEJYM6cObJx40b9BQGBzoSAgqVF/v73v+vS8F9++aVPmZu7XUsgzOVJzHI6LVu21EX6EPzw+SgsO2bMGH0NAmqHDh307wl/JwhATz31lN+ivAhWqD2HxQFxHLa4S10Azhu/CGFBQFwj7j3+nuNW5kcgQ5mgXbt2Sc+ePbUbeP/+/QFfZ5pkOLt7EP/wKI357LPPjHvvvdeIjIw06tatawwePNjYvXu35/nVq1cb6dOnN44ePerZ9/PPP2unQ1RUlD7u3Lmz0bp1a5/37dOnj9GgQQPPY/xctWpVn9fgs4oVK2bExMT4PbcSJUoY8+fP99k3evRoo06dOpavZ9OmTUb27NmN6OjoeJ/14Ycf6s94/549e/o8X6tWLaNy5co+14Nr9IZ7gHsBR44c0ft24sQJn9c0btxYzwlmzZql9/G3337zPD958mQjX758nscFChQwhg4d6vdaA7kWf/CZixcv1p+vXbum14pzxX0aMmSIUbp0acPtdvucU9asWY3Y2Fhjx44devzhw4f9vnfcfwv+7tO6dev0PS5cuKCPn332WaNp06Y+rxkwYIBRrlw5z+MiRYoYHTt29DzG+eXNm9eYMmVKgteZll26dEnvcaNqg42mD40KesNxOB7vE87Y0kqDMAaEOmD4DR6tAPzmjN/Wzd9y8ds0fjv2/g0Zy2mjewnPBQOVoL2hQCa6A++55554r0V3E7oJ0TVntqCw4Td+7+7DYK8HrQcsFoluLe/3xYKR5vviutCV5y3YatVoMWBhvAcffNDnc9Ba9T5/LEleokQJz2O06tByAvyJa0HLyZ9AriUhaC3htehqQ9ccWlNYTwrXjmtF950JiRr4nOPHj+ty9DgfdA+ilTd9+nS5cOGCJAU+E5/hDY/RIsU9NOH8TDg/dG+a94rSJiZipFEYL8LCd9jQLYMxEqxQGugCgSiEGbd76NatW/Feh3Edb4mtmosvScCXYtwAgrV7rF4P3heBwV9RTgTiQN3tmvE5OE90fcU9XwQLU9yAjS9j833vtqpwUq7lvffe07EoJE5gPCxQuBZ0EW/ZskVWr14tH3zwgSZyfP/991KsWDEJJX/3Ct3XlPpV3lMLW1rkaUmZA+tIEMDqpdhMWFIAg+h4HeBLL27yAFpRd4PfnJE44S/AIQmiQIECOn6GMRzvLdgvR+/rQasLVagzZMgQ732RIGJeM76EvW3bts3ncdxrRosA2XcmVLDGPrQE4n5OoJlzaAUhAcF7nSJvgVxLQnAOeF3cgIVrxzicd0DGGCTOpVChQp5ggZbQqFGjdHwJ45SLFy/2+zl4zru15A8+M+6UBDxGK/Vuv6DQXWgnn5UxLbEFBq00BmngyDabO3euJgqgW2nRokXy9ttva/IF4LdxdAU999xzsnPnTomKitK1cbCEAAbFAe+BhIXZs2drlw5aNd5f4IklKqAaNQbgcTyORUKCObiOL0Wsv/PPf/5Tfv31V+1yQ6r2hAkTknQ96P5ChhtaCshmQ6sBrQWcA/Tp00cz9vBZ+FxcD7IcveFzli1bptu+ffs0KcA7Gw5fuLhnuFdffPGFngvuHa4HxwQKSQlIQMA9wP3B3wFaN4FeS7CQ4IBfUHr37q3XtXTpUr1+rJeE1iWCuZmggqQRXBvmxiHw+IOgi2Nwbkiv99cy6t+/vwbm0aNH6/1Gos2kSZN8ElLIIsPZiRjsHkxj0E2Frjd0FWEMBC0ejF0h22vIkCGe36rxxYUvMWT64YsLY0XmFycgfR3dcAMHDpTo6GjNLMOXNYJMYjAWg6zBAQMGaBDEb9VIKzfHN9CthzGfd955R1+D7kUE0Lip5sFez/Lly/WLHQvU4QsXrQ5cm7n2DzIjcbx5PRgnQ1BCaroJ14gxJVwnWjp9+/bVNHtvCHoYg8OXMrLy0PqpXbu2pvgHCgvr4RxwTfgSx3s8/fTTAV9LsJCej/fE/cb4Va5cuXRccdiwYfo80uqR5YhF/vALB6YVIKi2aNHC7/vhnHENaO0irR7B21+LERmbyBRF4EKX5+uvvx5w9zQlAr8j/G94MnA26XXlelpEibR4kEofSLcnUbisp9W4wkDJkD5j0Mffjr0pa/e+HfbrabF7kIiIbIPdg0RETmI4e2kStrSIEukeZNcg2Y7BRAwiIrILw9ktLQYtIiIncTs7e5BBi4jIQVysiEFERBQe2NIiInISg2NaRERkF24DfX3WjrMBBi0iIicx2NIiIiLbMCwGIAYtIiJKaYazW1rMHiQiIttgS4uIyEncugqkxePCH4MWEZGTGO47m5XjbIBBi4jISQxnj2kxaBEROYmb3YNERGQXhrNbWsweJCIi22BLi4jISQyLrSZ7NLQYtIiIHMVwdvcggxYRkZO4kbrutnhc+GPQIiJyEoMtLSIisgvD2UGL2YNERGQbbGkRETmJm5OLiYjIJgzDrZuV4+yAQYuIyEkMw1qrySZjWgxaREROYljsHmTQIiKiFOd2i7icuzQJsweJiMg22NIiInISg92DRERkE4bbLYaF7kFmDxIRUcoz2NIiIiK7cBsiLgYtIiKyAwPBx+3YoMXsQSIisg22tIiIHMRwG2JY6B402NIiIqIUZ7itb0GaPHmyFC1aVCIjI6VWrVoSFRWV6OsXLVokZcqU0ddXrFhRli9fHvRnMmgRETmtpeW2tgVj4cKF0q9fPxkxYoTs3LlTKleuLM2aNZMzZ874ff2WLVukQ4cO0q1bN9m1a5e0adNGt7179wb1uS7DLm1CIiJK0OXLlyVHjhzSUFpLBtc9Eqzbxi1ZL0vl0qVLkj179ru+Hi2rhx56SCZNmqSP3W63FC5cWHr37i2DBg2K9/p27drJtWvX5Ouvv/bsq127tlSpUkWmTp0a8HmypUVE5CC35ZYGoKA3ueUJft7bzZs3431GTEyM7NixQ5o0aeLZly5dOn28detWv+eF/d6vB7TMEnp9QpiIQUTkABEREZI/f37ZfCr4cSJT1qxZtbXkDd1/I0eO9Nl37tw5iY2NlXz58vnsx+N9+/b5fe9Tp075fT32B4NBi4jIASIjI+XQoUPaCrIKo0Uul8tnX8aMGSWcMGgRETkocEVGRob8c/LkySPp06eX06dP++zHY7T2/MH+YF6fEI5pERFR0F2R1atXl7Vr13r2IREDj+vUqeP3GOz3fj2sWbMmwdcnhC0tIiIKGtLdO3fuLDVq1JCaNWvKxIkTNTuwa9eu+nynTp2kYMGCMnbsWH3cp08fadCggYwfP14ee+wxWbBggWzfvl2mTZsW1OcyaBERUdCQwn727FkZPny4JlMgdX3lypWeZIujR49qRqGpbt26Mn/+fBk2bJgMGTJESpUqJUuWLJEKFSoE9bmcp0VERLbBMS0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIrINBi0iIhK7+H82VeXjCIr7fwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试BahdanauAttention\n",
    "def test_bahdanau_attention():\n",
    "    # 设置参数\n",
    "    batch_size = 2\n",
    "    hidden_size = 8\n",
    "    src_len = 4\n",
    "    \n",
    "    # 创建输入数据\n",
    "    query = torch.randn(batch_size, hidden_size)  # Decoder的隐藏状态\n",
    "    keys = torch.randn(batch_size, src_len, hidden_size)  # Encoder输出\n",
    "    values = torch.randn(batch_size, src_len, hidden_size)  # 通常与keys相同\n",
    "    \n",
    "    # 创建注意力掩码，模拟序列填充的情况\n",
    "    attn_mask = torch.zeros(batch_size, src_len)\n",
    "    attn_mask[0, 0] = 1  # 假设第一个样本的最后一个token是padding\n",
    "    # attn_mask[1, 2:] = 1  # 假设第二个样本的最后两个tokens是padding\n",
    "    \n",
    "    # 初始化Bahdanau注意力机制\n",
    "    attention = BahdanauAttention(hidden_size, hidden_size)\n",
    "    \n",
    "    # 前向传播\n",
    "    context, attn_weights = attention(query, keys, values, attn_mask)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Query shape: {query.shape}\") # (batch_size, hidden_size)\n",
    "    print(f\"Keys shape: {keys.shape}\") # (batch_size, src_len, hidden_size)\n",
    "    print(f\"Values shape: {values.shape}\") # (batch_size, src_len, hidden_size)\n",
    "    print(f\"Context vector shape: {context.shape}\") # (batch_size, hidden_size)\n",
    "    print(f\"Attention weights shape: {attn_weights.shape}\") # (batch_size, src_len)\n",
    "    \n",
    "    # 验证注意力权重是否在掩码位置接近于0\n",
    "    print(\"\\nAttention weights:\")\n",
    "    print(attn_weights)\n",
    "    \n",
    "    # 可视化注意力权重\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(attn_weights.detach().numpy(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Attention Weights')\n",
    "    plt.xlabel('Source Sequence Position')\n",
    "    plt.ylabel('Batch Sample')\n",
    "    \n",
    "    return context, attn_weights\n",
    "\n",
    "# 运行测试\n",
    "context, attn_weights = test_bahdanau_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7ac23",
   "metadata": {},
   "source": [
    "# Decoder 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=256,\n",
    "        hidden_dim=1024,\n",
    "        num_layers=1,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) #最后分类,词典大小是多少，就输出多少个分类\n",
    "        self.dropout = nn.Dropout(0.6) #0.6可以调整的超参数\n",
    "        self.attention = BahdanauAttention(hidden_dim) #注意力得到的context_vector\n",
    "\n",
    "    def forward(self, decoder_input, hidden, encoder_outputs, attn_mask=None):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            decoder_input: 解码器的输入，形状为 [batch_size, 1]\n",
    "            hidden: 解码器的隐藏状态，形状为 [batch_size, hidden_dim],第一次使用的是encoder的hidden\n",
    "            encoder_outputs: 编码器的输出，形状为 [batch_size, sequence_length, hidden_dim]\n",
    "            attn_mask: 注意力掩码，形状为 [batch_size, sequence_length],是encoder_inputs_mask\n",
    "        \n",
    "        返回:\n",
    "            logits: 解码器的输出，形状为 [batch_size, 1, vocab_size]\n",
    "            hidden: 解码器的隐藏状态，形状为 [batch_size, hidden_dim]\n",
    "            attention_score: 注意力权重，形状为 [batch_size, sequence_length, 1]\n",
    "        \"\"\"\n",
    "        #断言，确保输入的形状是正确的\n",
    "        # decoder_input.shape = [batch size, 1]\n",
    "        assert len(decoder_input.shape) == 2 and decoder_input.shape[-1] == 1, f\"decoder_input.shape = {decoder_input.shape} is not valid\"\n",
    "        # hidden.shape = [batch size, hidden_dim]，decoder_hidden,而第一次使用的是encoder的hidden\n",
    "        assert len(hidden.shape) == 2, f\"hidden.shape = {hidden.shape} is not valid\"\n",
    "        # encoder_outputs.shape = [batch size, sequence length, hidden_dim]\n",
    "        assert len(encoder_outputs.shape) == 3, f\"encoder_outputs.shape = {encoder_outputs.shape} is not valid\"\n",
    "        # context_vector.shape = [batch_size, hidden_dim]\n",
    "        \n",
    "        # 注意力机制\n",
    "        context_vector, attention_score = self.attention(\n",
    "            query=hidden, keys=encoder_outputs, values=encoder_outputs, attn_mask=attn_mask)\n",
    "        # decoder_input.shape = [batch size, 1]-->embeds.shape = [batch size, 1, embedding_dim]\n",
    "        embeds = self.embedding(decoder_input)\n",
    "\n",
    "        # context_vector.shape = [batch size, hidden_dim] -->unsqueeze(-2)增加维度 [batch size, 1, hidden_dim]\n",
    "        embeds = torch.cat([context_vector.unsqueeze(-2), embeds], dim=-1) #cat上下两个维度，变成[batch size, 1, embedding_dim + hidden_dim]\n",
    "        # 新的embeds.shape = [batch size, 1, embedding_dim + hidden_dim]\n",
    "        seq_output, hidden = self.gru(embeds) #这里需要把前面的decode hidden再次输入，需要改进\n",
    "        # seq_output.shape = [batch size, 1, hidden_dim]\n",
    "        logits = self.fc(self.dropout(seq_output)) #这里需要dropout，防止过拟合\n",
    "        # logits.shape = [batch size, 1, vocab size]，attention_score = [batch size, sequence length, 1]\n",
    "        return logits, hidden, attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13078c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input.shape: torch.Size([2, 1])\n",
      "decoder_hidden.shape: torch.Size([2, 32])\n",
      "encoder_outputs.shape: torch.Size([2, 5, 32])\n",
      "logits.shape: torch.Size([2, 1, 1000])\n",
      "hidden.shape: torch.Size([1, 2, 32])\n",
      "attention_score.shape: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 前向计算验证Decoder是否ok\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "hidden_dim = 32\n",
    "vocab_size = 1000\n",
    "embedding_dim = 64\n",
    "\n",
    "# 创建模拟数据\n",
    "decoder_input = torch.randint(0, vocab_size, (batch_size, 1))  # [batch_size, 1]\n",
    "decoder_hidden = torch.randn(batch_size, hidden_dim)  # [batch_size, hidden_dim]\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_dim)  # [batch_size, seq_len, hidden_dim]\n",
    "attn_mask = torch.ones(batch_size, seq_len)  # [batch_size, seq_len]\n",
    "\n",
    "# 创建Decoder模型\n",
    "decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# 前向计算\n",
    "logits, hidden, attention_score = decoder(decoder_input, decoder_hidden, encoder_outputs, attn_mask)\n",
    "\n",
    "# 打印输出形状\n",
    "print(f\"decoder_input.shape: {decoder_input.shape}\") # [batch_size, 1]\n",
    "print(f\"decoder_hidden.shape: {decoder_hidden.shape}\") # [batch_size, hidden_dim] \n",
    "print(f\"encoder_outputs.shape: {encoder_outputs.shape}\") # [batch_size, seq_len, hidden_dim]\n",
    "print(f\"logits.shape: {logits.shape}\") # [batch_size,1,vocab_size]\n",
    "print(f\"hidden.shape: {hidden.shape}\") # [num_layers, batch_size, hidden_dim]\n",
    "print(f\"attention_score.shape: {attention_score.shape}\") # [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6633ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, num_layers=1, bidirectional=False, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        # 如果是双向的，则Decoder的hidden_dim是encoder的两倍\n",
    "        decoder_hidden_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.decoder = Decoder(output_vocab_size, embedding_dim, decoder_hidden_dim, num_layers)\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        # src_mask: [batch_size, src_len]\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        \n",
    "        # 编码\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src, src_mask)\n",
    "        \n",
    "        # 初始化解码器输入和隐藏状态\n",
    "        decoder_input = tgt[:, 0:1]  # 使用目标序列的第一个token作为初始输入\n",
    "        # 获取encoder_hidden的最后一层\n",
    "        if len(encoder_hidden.shape) == 3:\n",
    "            # 如果encoder_hidden形状为[num_layers, batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden[-1]  # 只使用最后一层的隐藏状态\n",
    "        else:\n",
    "            # 如果已经是[batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "        # 存储所有解码器输出\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.output_vocab_size).to(src.device)\n",
    "        attention_scores = torch.zeros(batch_size, tgt_len, src.shape[1]).to(src.device)\n",
    "        \n",
    "        # 逐个时间步解码\n",
    "        for t in range(1, tgt_len):\n",
    "            # 解码器前向传播\n",
    "            decoder_output, decoder_hidden, attention_score = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "            )\n",
    "            \n",
    "            # 存储当前输出和注意力分数\n",
    "            outputs[:, t-1:t, :] = decoder_output\n",
    "            attention_scores[:, t-1:t, :] = attention_score\n",
    "            \n",
    "            # 决定是否使用教师强制\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # 获取当前预测的token\n",
    "            top1 = decoder_output.argmax(2)\n",
    "            \n",
    "            # 下一个输入：如果使用教师强制，则使用实际目标；否则使用预测\n",
    "            decoder_input = tgt[:, t:t+1] if teacher_force else top1\n",
    "        \n",
    "        # 处理最后一个时间步\n",
    "        decoder_output, _, attention_score = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "        )\n",
    "        outputs[:, -1:, :] = decoder_output\n",
    "        attention_scores[:, -1:, :] = attention_score\n",
    "        \n",
    "        return outputs, attention_scores\n",
    "    \n",
    "    def translate(self, src, src_mask=None, max_len=50, sos_idx=1, eos_idx=3):\n",
    "        \"\"\"\n",
    "        用于推理/翻译\n",
    "        \n",
    "        参数:\n",
    "            src: 源语言输入 [batch_size, src_len]\n",
    "            src_mask: 源语言掩码 [batch_size, src_len]\n",
    "            max_len: 生成的最大长度\n",
    "            sos_idx: 起始符索引\n",
    "            eos_idx: 结束符索引\n",
    "            \n",
    "        返回:\n",
    "            predictions: 预测的目标语言序列 [batch_size, max_len]\n",
    "            attention_scores: 注意力分数 [batch_size, max_len, src_len]\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # 编码\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src, src_mask)\n",
    "        \n",
    "        # 初始化解码器输入（起始符）\n",
    "        decoder_input = torch.LongTensor([[sos_idx]] * batch_size).to(src.device)\n",
    "        \n",
    "        # 获取encoder_hidden的最后一层\n",
    "        if len(encoder_hidden.shape) == 3:\n",
    "            # 如果encoder_hidden形状为[num_layers, batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden[-1]  # 只使用最后一层的隐藏状态\n",
    "        else:\n",
    "            # 如果已经是[batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # 存储预测和注意力分数\n",
    "        predictions = torch.zeros(batch_size, max_len, dtype=torch.long).to(src.device)\n",
    "        attention_scores = torch.zeros(batch_size, max_len, src_len).to(src.device)\n",
    "        \n",
    "        # 记录每个样本是否已完成生成（遇到EOS）\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool).to(src.device)\n",
    "        \n",
    "        # 逐步解码\n",
    "        for t in range(max_len):\n",
    "            # 解码器前向传播\n",
    "            decoder_output, decoder_hidden, attention_score = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "            )\n",
    "            \n",
    "            # 获取当前步的预测和注意力分数\n",
    "            top1 = decoder_output.argmax(2)\n",
    "            predictions[:, t:t+1] = top1\n",
    "            attention_scores[:, t:t+1, :] = attention_score\n",
    "            \n",
    "            # 检查是否所有样本都已完成\n",
    "            finished = finished | (top1.squeeze(1) == eos_idx)\n",
    "            if finished.all():\n",
    "                break\n",
    "                \n",
    "            # 使用当前预测作为下一步的输入\n",
    "            decoder_input = top1\n",
    "            \n",
    "        return predictions, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84490c4",
   "metadata": {},
   "source": [
    "# Sequence2Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_dim, num_layers=1, bidirectional=False, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # 初始化编码器，输入词表大小、词嵌入维度、隐藏层维度、层数和dropout\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        # 如果是双向的，则Decoder的hidden_dim是encoder的两倍\n",
    "        decoder_hidden_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        # 初始化解码器，输出词表大小、词嵌入维度、解码器隐藏层维度和层数\n",
    "        self.decoder = Decoder(output_vocab_size, embedding_dim, decoder_hidden_dim, num_layers)\n",
    "        # 保存输出词表大小和隐藏层维度，供后续使用\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len] - 源序列输入\n",
    "        # tgt: [batch_size, tgt_len] - 目标序列输入\n",
    "        # src_mask: [batch_size, src_len] - 源序列的掩码\n",
    "        # teacher_forcing_ratio: 使用教师强制的概率\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        \n",
    "        # 通过编码器获取编码器输出和最终隐藏状态\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src, src_mask)\n",
    "        \n",
    "        # 初始化解码器输入和隐藏状态\n",
    "        decoder_input = tgt[:, 0:1]  # 使用目标序列的第一个token作为初始输入\n",
    "        # 获取encoder_hidden的最后一层作为解码器的初始隐藏状态\n",
    "        if len(encoder_hidden.shape) == 3:\n",
    "            # 如果encoder_hidden形状为[num_layers, batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden[-1]  # 只使用最后一层的隐藏状态\n",
    "        else:\n",
    "            # 如果已经是[batch_size, hidden_dim]\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "        # 创建张量存储所有时间步的解码器输出和注意力分数\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.output_vocab_size).to(src.device)\n",
    "        attention_scores = torch.zeros(batch_size, tgt_len, src.shape[1]).to(src.device)\n",
    "        \n",
    "        # 逐个时间步解码，从第1个时间步开始（第0个是初始输入）\n",
    "        for t in range(1, tgt_len):\n",
    "            # 解码器单步前向传播，获取当前输出、更新后的隐藏状态和注意力分数\n",
    "            decoder_output, decoder_hidden, attention_score = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "            )\n",
    "            \n",
    "            # 保存当前时间步的输出和注意力分数\n",
    "            outputs[:, t-1:t, :] = decoder_output\n",
    "            attention_scores[:, t-1:t, :] = attention_score\n",
    "            \n",
    "            # 随机决定是否使用教师强制\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # 获取当前预测的token（概率最大的词）\n",
    "            top1 = decoder_output.argmax(2)\n",
    "            \n",
    "            # 确定下一个时间步的输入：如果使用教师强制，则使用真实目标词；否则使用预测词\n",
    "            decoder_input = tgt[:, t:t+1] if teacher_force else top1\n",
    "        \n",
    "        # 单独处理最后一个时间步，因为前面的循环少处理了一步\n",
    "        decoder_output, _, attention_score = self.decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "        )\n",
    "        outputs[:, -1:, :] = decoder_output\n",
    "        attention_scores[:, -1:, :] = attention_score\n",
    "        \n",
    "        return outputs, attention_scores\n",
    "    \n",
    "    def translate(self, src, src_mask=None, max_len=50, sos_idx=1, eos_idx=3):\n",
    "        \"\"\"\n",
    "        用于推理/翻译\n",
    "        \n",
    "        参数:\n",
    "            src: 源语言输入 [batch_size, src_len]\n",
    "            src_mask: 源语言掩码 [batch_size, src_len]\n",
    "            max_len: 生成的最大长度\n",
    "            sos_idx: 起始符索引\n",
    "            eos_idx: 结束符索引\n",
    "            \n",
    "        返回:\n",
    "            predictions: 预测的目标语言序列 [batch_size, max_len]\n",
    "            attention_scores: 注意力分数 [batch_size, max_len, src_len]\n",
    "        \"\"\"\n",
    "        # 获取输入的batch大小和序列长度\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # 首先通过编码器编码源序列\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src, src_mask)\n",
    "        \n",
    "        # 创建解码器的初始输入（batch个起始符号）\n",
    "        decoder_input = torch.LongTensor([[sos_idx]] * batch_size).to(src.device)\n",
    "        \n",
    "        # 处理编码器隐藏状态，确保维度正确\n",
    "        if len(encoder_hidden.shape) == 3:\n",
    "            # 如果是多层的隐藏状态，只取最后一层\n",
    "            decoder_hidden = encoder_hidden[-1]\n",
    "        else:\n",
    "            # 如果已经是单层，直接使用\n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # 创建张量存储预测序列和注意力分数\n",
    "        predictions = torch.zeros(batch_size, max_len, dtype=torch.long).to(src.device)\n",
    "        attention_scores = torch.zeros(batch_size, max_len, src_len).to(src.device)\n",
    "        \n",
    "        # 创建标记张量，记录每个样本是否已生成结束符号\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool).to(src.device)\n",
    "        \n",
    "        # 自回归解码：逐步生成目标序列\n",
    "        for t in range(max_len):\n",
    "            # 解码器单步前向传播\n",
    "            decoder_output, decoder_hidden, attention_score = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs, src_mask\n",
    "            )\n",
    "            \n",
    "            # 获取当前时间步的预测和注意力分数\n",
    "            top1 = decoder_output.argmax(2)  # 选择概率最大的词\n",
    "            predictions[:, t:t+1] = top1\n",
    "            attention_scores[:, t:t+1, :] = attention_score\n",
    "            \n",
    "            # 更新完成标记：如果预测到结束符号，则标记为完成\n",
    "            finished = finished | (top1.squeeze(1) == eos_idx)\n",
    "            # 如果所有样本都完成，提前结束解码\n",
    "            if finished.all():\n",
    "                break\n",
    "                \n",
    "            # 将当前预测作为下一步的输入，继续解码\n",
    "            decoder_input = top1\n",
    "            \n",
    "        return predictions, attention_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
