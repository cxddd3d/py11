{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.3\n",
      "numpy 1.26.4\n",
      "pandas 2.3.0\n",
      "sklearn 1.7.0\n",
      "torch 2.7.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82b4e9",
   "metadata": {},
   "source": [
    "# 1. preprocessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e324758",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf6cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May I borrow, this book?\n",
      "¿Puedo tomar prestado este libro?\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "#因为西班牙语有一些是特殊字符，所以我们需要unicode转ascii，\n",
    "# 这样值变小了，因为unicode太大\n",
    "def unicode_to_ascii(s):\n",
    "    #NFD是转换方法，把每一个字节拆开，Mn是重音，所以去除\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "#下面我们找个样本测试一下\n",
    "# 加u代表对字符串进行unicode编码\n",
    "en_sentence = u\"May I borrow, this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52ca34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may i borrow , this book ?\n",
      "¿ puedo tomar prestado este libro ?\n",
      "b'\\xc2\\xbf puedo tomar prestado este libro ?'\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(w):\n",
    "    #变为小写，去掉多余的空格，变成小写，id少一些\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # eg: \"he is a boy.\" => \"he is a boy . \"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格，你可以保留一些标点符号\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    #因为可能有多余空格，替换为一个空格，所以处理一下\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    w = w.strip() #strip是去掉两边的空格\n",
    "\n",
    "    return w\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))  #¿是占用两个字节的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7670c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'test', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'test', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'test', 'train', 'test', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'test',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train', 'train', 'train', 'train', 'train', 'train',\n",
       "       'train', 'train'], dtype='<U5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集和测试集的一个方法\n",
    "split_index1 = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=100)\n",
    "split_index1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc5e69",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915b8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从spa.txt读取数据并创建train数据集...\n",
      "从spa.txt读取数据并创建test数据集...\n"
     ]
    }
   ],
   "source": [
    "# 创建一个继承自torch.utils.data.Dataset的数据集类\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    用于加载和处理英语-西班牙语翻译数据集的自定义Dataset类\n",
    "    参数:\n",
    "        path: 数据文件路径\n",
    "        num_examples: 要使用的样本数量，None表示使用全部\n",
    "        split: 数据集划分，可选'train'或'test'\n",
    "    \"\"\"\n",
    "    def __init__(self, path, num_examples=None, split=None):\n",
    "        # 检查是否存在缓存文件\n",
    "        cache_file_en = f'{split}_en_sentences.npy' if split else 'all_en_sentences.npy'\n",
    "        cache_file_sp = f'{split}_sp_sentences.npy' if split else 'all_sp_sentences.npy'\n",
    "        \n",
    "        # 如果缓存文件存在，直接加载\n",
    "        if os.path.exists(cache_file_en) and os.path.exists(cache_file_sp):\n",
    "            print(f\"从缓存文件加载{split}数据...\")\n",
    "            self.en_array = np.load(cache_file_en)\n",
    "            self.sp_array = np.load(cache_file_sp)\n",
    "        else:\n",
    "            print(f\"从{path}读取数据并创建{split}数据集...\")\n",
    "            # 读取文件\n",
    "            lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "            \n",
    "            # 创建空列表存储英语和西班牙语句子对\n",
    "            self.en_sentences = []\n",
    "            self.sp_sentences = []\n",
    "            \n",
    "            # 生成训练集和测试集的索引,如果num_examples为None，则使用所有行，否则使用num_examples行\n",
    "            total_examples = len(lines) if num_examples is None else min(num_examples, len(lines)) \n",
    "            # 使用9:1的比例划分训练集和测试集\n",
    "            split_index = np.random.choice(a=[\"train\", \"test\"], replace=True, p=[0.9, 0.1], size=total_examples)\n",
    "            \n",
    "            # 遍历每一行，按tab分隔英语和西班牙语\n",
    "            for i, line in enumerate(lines[:total_examples]):\n",
    "                # 如果指定了split，则只保留对应的数据\n",
    "                if split is not None and split_index[i] != split:\n",
    "                    continue\n",
    "                    \n",
    "                # 按tab分隔获取英语和西班牙语句子\n",
    "                en, sp = line.split('\\t')\n",
    "                \n",
    "                # 对句子进行预处理（清理、标准化等）\n",
    "                en = preprocess_sentence(en)\n",
    "                sp = preprocess_sentence(sp)\n",
    "                \n",
    "                # 将处理后的句子添加到对应列表\n",
    "                self.en_sentences.append(en)\n",
    "                self.sp_sentences.append(sp)\n",
    "            \n",
    "            # 转换为numpy数组以提高效率\n",
    "            self.trg = np.array(self.en_sentences) #英语(目标语言)\n",
    "            self.src = np.array(self.sp_sentences) #西班牙语(源语言)\n",
    "            \n",
    "            # 将处理后的数据保存为numpy文件以加速后续加载\n",
    "            np.save(cache_file_en, self.trg)\n",
    "            np.save(cache_file_sp, self.src)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集中的样本数量\"\"\"\n",
    "        return len(self.trg)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"返回指定索引的源语言和目标语言句子对\"\"\"\n",
    "        return self.src[idx],self.trg[idx]\n",
    "    \n",
    "\n",
    "# 从spa.txt创建训练集和测试集\n",
    "train_dataset = TranslationDataset('spa.txt', split='train')  # 创建训练数据集\n",
    "test_dataset = TranslationDataset('spa.txt', split='test')    # 创建测试数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffe83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado .\n",
      "target: if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo .\n"
     ]
    }
   ],
   "source": [
    "print(\"source: {}\\ntarget: {}\".format(*train_dataset[-1])) # print the last training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b17140",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c973165",
   "metadata": {},
   "source": [
    "这里有两种处理方式，分别对应着 encoder 和 decoder 的 word embedding 是否共享，这里实现不共享的方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc44c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences_pairs):\n",
    "    \"\"\"\n",
    "    构建英语和西班牙语的词典\n",
    "    Args:\n",
    "        sentences_pairs: 包含英语和西班牙语句子对的列表\n",
    "    Returns:\n",
    "        en_vocab: 英语词典\n",
    "        sp_vocab: 西班牙语词典\n",
    "    \"\"\"\n",
    "    # 分别存储英语和西班牙语的单词\n",
    "    en_words = []\n",
    "    sp_words = []\n",
    "    \n",
    "    # 遍历所有句子对，分别提取单词\n",
    "    for en, sp in sentences_pairs:\n",
    "        en_words.extend(en.split())\n",
    "        sp_words.extend(sp.split())\n",
    "    \n",
    "    # 使用Counter统计词频\n",
    "    en_vocab = Counter(en_words)\n",
    "    sp_vocab = Counter(sp_words)\n",
    "    \n",
    "    return en_vocab, sp_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07575987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英语词典大小: 12504\n",
      "西班牙语词典大小: 23719\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 构建英语和西班牙语的词典\n",
    "def build_vocab(sentences, min_freq=1):\n",
    "    \"\"\"\n",
    "    构建词典函数\n",
    "    参数:\n",
    "        sentences: 句子列表\n",
    "        min_freq: 最小词频,默认为1\n",
    "    返回:\n",
    "        word2idx: 词到索引的映射字典\n",
    "    \"\"\"\n",
    "    # 初始化词典，包含特殊标记\n",
    "    word2idx = {\n",
    "        \"[PAD]\": 0,     # 填充 token\n",
    "        \"[BOS]\": 1,     # begin of sentence\n",
    "        \"[UNK]\": 2,     # 未知 token\n",
    "        \"[EOS]\": 3,     # end of sentence\n",
    "    }\n",
    "    \n",
    "    # 使用Counter统计词频\n",
    "    # Counter类可以自动统计可迭代对象中每个元素出现的次数\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(sentence.split())  # split()将句子分割成单词列表\n",
    "    \n",
    "    # 按词频排序并添加到词典中\n",
    "    idx = len(word2idx)  # 从特殊标记数量开始编号\n",
    "    # 返回计数最高的前 n 个元素及其计数，若未指定 n 则返回所有元素\n",
    "    for word, count in counter.most_common():\n",
    "        # 只添加频率大于等于min_freq的词\n",
    "        if count >= min_freq:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return word2idx\n",
    "\n",
    "# 构建英语(目标语言)和西班牙语(源语言)词典\n",
    "# trg代表target(目标), src代表source(源)\n",
    "trg_word2idx = build_vocab(train_dataset.trg) #英语\n",
    "src_word2idx = build_vocab(train_dataset.src) #西班牙语\n",
    "\n",
    "# 创建反向映射（索引到词）\n",
    "# 用于之后将模型输出的索引转换回单词\n",
    "trg_idx2word = {idx: word for word, idx in trg_word2idx.items()}\n",
    "src_idx2word = {idx: word for word, idx in src_word2idx.items()}\n",
    "# 打印词典大小，用于检查词典构建是否正确\n",
    "print(f\"英语词典大小: {len(trg_word2idx)}\")\n",
    "print(f\"西班牙语词典大小: {len(src_word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c175fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_text----------\n",
      "['hello', 'world']\n",
      "['tokenize', 'text', 'datas', 'with', 'batch']\n",
      "['this', 'is', 'a', 'test']\n",
      "mask----------\n",
      "tensor([0, 0, 0, 0, 1, 1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1])\n",
      "indices----------\n",
      "tensor([   1, 1745,  309,    3,    0,    0,    0])\n",
      "tensor([   1,    2, 2103,    2,   39,    2,    3])\n",
      "tensor([  1,  23,  12,  10, 924,   3,   0])\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2idx, idx2word, max_length=500, pad_idx=0, bos_idx=1, eos_idx=3, unk_idx=2):\n",
    "        self.word2idx = word2idx  # 词到索引的映射字典\n",
    "        self.idx2word = idx2word  # 索引到词的映射字典\n",
    "        self.max_length = max_length  # 序列的最大长度\n",
    "        self.pad_idx = pad_idx  # 填充标记的索引\n",
    "        self.bos_idx = bos_idx  # 句子开始标记的索引\n",
    "        self.eos_idx = eos_idx  # 句子结束标记的索引\n",
    "        self.unk_idx = unk_idx  # 未知词标记的索引\n",
    "\n",
    "    def encode(self, text_list, padding_first=False, add_bos=True, add_eos=True, return_mask=False):\n",
    "        \"\"\"如果padding_first == True，则padding加载前面，否则加载后面\n",
    "        return_mask: 是否返回mask(掩码），mask用于指示哪些是padding的，哪些是真实的token\n",
    "        \"\"\"\n",
    "        max_length = min(self.max_length, add_eos + add_bos + max([len(text) for text in text_list]))  # 计算实际需要的最大长度\n",
    "        indices_list = []  # 初始化索引列表\n",
    "        for text in text_list:  # 遍历每个文本\n",
    "            indices = [self.word2idx.get(word, self.unk_idx) for word in text[:max_length - add_bos - add_eos]]  # 将文本中的词转换为索引，如果词不在词表中则使用unk_idx\n",
    "            if add_bos:  # 如果需要添加句子开始标记\n",
    "                indices = [self.bos_idx] + indices  # 在序列开头添加BOS标记\n",
    "            if add_eos:  # 如果需要添加句子结束标记\n",
    "                indices = indices + [self.eos_idx]  # 在序列末尾添加EOS标记\n",
    "            if padding_first:  # 如果padding需要加在前面\n",
    "                indices = [self.pad_idx] * (max_length - len(indices)) + indices  # 在序列前面添加padding\n",
    "            else:  # 如果padding需要加在后面\n",
    "                indices = indices + [self.pad_idx] * (max_length - len(indices))  # 在序列后面添加padding\n",
    "            indices_list.append(indices)  # 将处理后的索引添加到列表中\n",
    "        input_ids = torch.tensor(indices_list)  # 将索引列表转换为tensor\n",
    "        masks = (input_ids == self.pad_idx).to(dtype=torch.int64)  # 创建mask，1表示padding位置，0表示实际token位置\n",
    "        return input_ids if not return_mask else (input_ids, masks)  # 根据return_mask参数决定返回值\n",
    "\n",
    "    def decode(self, indices_list, remove_bos=True, remove_eos=True, remove_pad=True, split=False):\n",
    "        text_list = []  # 初始化文本列表\n",
    "        for indices in indices_list:  # 遍历每个索引序列\n",
    "            text = []  # 初始化当前文本\n",
    "            for index in indices:  # 遍历序列中的每个索引\n",
    "                word = self.idx2word.get(index, \"[UNK]\")  # 将索引转换为词，如果索引不在词表中则使用\"[UNK]\"\n",
    "                if remove_bos and word == \"[BOS]\":  # 如果需要移除BOS标记且当前词是BOS\n",
    "                    continue  # 跳过这个词\n",
    "                if remove_eos and word == \"[EOS]\":  # 如果需要移除EOS标记且当前词是EOS\n",
    "                    break  # 结束当前序列的处理\n",
    "                if remove_pad and word == \"[PAD]\":  # 如果需要移除PAD标记且当前词是PAD\n",
    "                    break  # 结束当前序列的处理\n",
    "                text.append(word)  # 将词添加到当前文本中\n",
    "            text_list.append(\" \".join(text) if not split else text)  # 根据split参数决定返回连接后的字符串还是词列表\n",
    "        return text_list  # 返回处理后的文本列表\n",
    "\n",
    "# 两个相对于1个tokenizer的好处是embedding的参数量减少\n",
    "src_tokenizer = Tokenizer(word2idx=src_word2idx, idx2word=src_idx2word)  # 创建源语言(西班牙语)的tokenizer\n",
    "trg_tokenizer = Tokenizer(word2idx=trg_word2idx, idx2word=trg_idx2word)  # 创建目标语言(英语)的tokenizer\n",
    "\n",
    "batch_text = [\"hello world\".split(), \"tokenize text datas with batch\".split(), \"this is a test\".split()]\n",
    "indices,mask = trg_tokenizer.encode(batch_text, padding_first=False, add_bos=True, add_eos=True,return_mask=True)\n",
    "\n",
    "print(\"batch_text\"+'-'*10)\n",
    "for raw in batch_text:\n",
    "    print(raw)\n",
    "print(\"mask\"+'-'*10)\n",
    "for m in mask:\n",
    "    print(m)\n",
    "print(\"indices\"+'-'*10)\n",
    "for index in indices:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de27713",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38104f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    数据批处理函数\n",
    "    \n",
    "    Args:\n",
    "        batch: 批次数据\n",
    "        src_tokenizer: 源语言tokenizer\n",
    "        trg_tokenizer: 目标语言tokenizer\n",
    "        device: 设备，如果指定则将tensor移至该设备\n",
    "    \n",
    "    Returns:\n",
    "        包含编码后的tensor的字典\n",
    "    \"\"\"\n",
    "    src_texts = [pair[0].split() for pair in batch] #取batch内第0列进行分词，赋给src_words\n",
    "    trg_texts = [pair[1].split() for pair in batch] #取batch内第1列进行分词，赋给trg_words\n",
    "    \n",
    "    # 编码源语言输入\n",
    "    encoder_inputs, encoder_inputs_mask = src_tokenizer.encode(\n",
    "        src_texts, \n",
    "        padding_first=True, #padding加在前面\n",
    "        add_bos=True, \n",
    "        add_eos=True, \n",
    "        return_mask=True\n",
    "    )\n",
    "    \n",
    "    # 编码目标语言输入（用于训练时的teacher forcing）\n",
    "    decoder_inputs= trg_tokenizer.encode(\n",
    "        trg_texts, \n",
    "        padding_first=False, #padding加在后面\n",
    "        add_bos=True, \n",
    "        add_eos=False, \n",
    "        return_mask=False\n",
    "    )\n",
    "    \n",
    "    # 编码目标语言标签（用于计算损失）\n",
    "    decoder_labels, decoder_labels_mask = trg_tokenizer.encode(\n",
    "        trg_texts, \n",
    "        padding_first=False, \n",
    "        add_bos=False, \n",
    "        add_eos=True, \n",
    "        return_mask=True\n",
    "    )\n",
    "    \n",
    "    result = {\n",
    "        \"encoder_inputs\": encoder_inputs,\n",
    "        \"encoder_inputs_mask\": encoder_inputs_mask,\n",
    "        \"decoder_inputs\": decoder_inputs,\n",
    "        \"decoder_labels\": decoder_labels,\n",
    "        \"decoder_labels_mask\": decoder_labels_mask\n",
    "    }\n",
    "    \n",
    "    # 如果指定了设备，将所有tensor移至该设备\n",
    "    if device is not None:\n",
    "        result = {k: v.to(device=device) for k, v in result.items()}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aea7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs\n",
      "tensor([[   0,    1,   17,    6,   72,   15,    7,    4,    3],\n",
      "        [   1,   10, 3919,    7, 1753,  142, 2999,    4,    3]])\n",
      "encoder_inputs_mask\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "decoder_inputs\n",
      "tensor([[   1,    5,   46,   91,   14,  494,   71,    4,    0],\n",
      "        [   1,    9, 2038,    6, 2008,  261,  373,   15,    4]])\n",
      "decoder_labels\n",
      "tensor([[   5,   46,   91,   14,  494,   71,    4,    3,    0],\n",
      "        [   9, 2038,    6, 2008,  261,  373,   15,    4,    3]])\n",
      "decoder_labels_mask\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "sample_dl = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#两次执行这个代码效果不一样，因为每次执行都会shuffle\n",
    "for batch in sample_dl:\n",
    "    for key, value in batch.items():\n",
    "        print(key)\n",
    "        print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc3fd4",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2829d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    序列到序列模型的编码器部分\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, dropout=0.0):\n",
    "        \"\"\"\n",
    "        初始化编码器\n",
    "        \n",
    "        参数:\n",
    "        - vocab_size: 源语言词汇表大小\n",
    "        - embedding_dim: 词嵌入维度\n",
    "        - hidden_size: 隐藏状态维度\n",
    "        - num_layers: GRU层数\n",
    "        - dropout: Dropout比率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 词嵌入层 - 将输入的词索引转换为密集向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU层,batch_first=True表示输入的形状为[batch_size, seq_len]\n",
    "        # GRU是LSTM的简化版本,只有两个门控单元(更新门和重置门)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,  # 输入维度,等于词嵌入维度\n",
    "            hidden_size=hidden_size,   # 隐藏状态维度\n",
    "            num_layers=num_layers,     # GRU层数,可以堆叠多层\n",
    "            batch_first=True,          # 输入张量的第一个维度是batch_size\n",
    "            dropout=dropout if num_layers > 1 else 0  # 多层时才在层间使用dropout\n",
    "        )\n",
    "        \n",
    "        # dropout层 - 用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 保存配置参数供其他地方使用\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_lengths=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        - src: 源语言序列 [batch_size, seq_len]\n",
    "        - src_mask: 源语言序列的掩码 [batch_size, seq_len]\n",
    "        - src_lengths: 源语言序列的实际长度 [batch_size]\n",
    "        \n",
    "        返回:\n",
    "        - encoder_outputs: 编码器所有时间步的输出 [batch_size, seq_len, hidden_size]\n",
    "        - hidden: 解码器初始隐藏状态 [num_layers, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        # 词嵌入并应用dropout正则化\n",
    "        embedded = self.dropout(self.embedding(src))  #[batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # 通过GRU层处理序列\n",
    "        # GRU的两个输出:\n",
    "        # 1. encoder_outputs包含每个时间步的隐藏状态,用于注意力机制\n",
    "        # 2. hidden是最后一个时间步的隐藏状态,用作解码器的初始状态\n",
    "        # 通过GRU\n",
    "        #[batch_size, seq_len, embedding_dim]-> encoder_outputs [batch_size, seq_len, hidden_dim]\n",
    "        #[batch_size, seq_len, embedding_dim]-> hidden [num_layers, batch_size, hidden_dim]\n",
    "        encoder_outputs, hidden = self.gru(embedded) \n",
    "        \n",
    "        # 返回编码器所有时间步的输出和解码器初始隐藏状态\n",
    "        return encoder_outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fee7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源序列形状: torch.Size([64, 20])\n",
      "编码器输出形状: torch.Size([64, 20, 512])\n",
      "隐藏状态形状: torch.Size([2, 64, 512])\n",
      "Encoder测试通过!\n"
     ]
    }
   ],
   "source": [
    "# 测试Encoder\n",
    "import torch\n",
    "\n",
    "# 创建测试参数\n",
    "vocab_size = len(src_tokenizer.word2idx)\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "batch_size = 64\n",
    "seq_len = 20\n",
    "\n",
    "# 实例化Encoder\n",
    "encoder = Encoder(vocab_size, embedding_dim, hidden_size, num_layers, dropout)\n",
    "\n",
    "# 创建测试输入\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_len))  # [batch_size, seq_len]\n",
    "\n",
    "# 前向传播\n",
    "encoder_outputs, hidden = encoder(src)\n",
    "\n",
    "# 打印输出形状\n",
    "print(f\"源序列形状: {src.shape}\")\n",
    "print(f\"编码器输出形状: {encoder_outputs.shape}\")\n",
    "print(f\"隐藏状态形状: {hidden.shape}\")\n",
    "\n",
    "# 验证输出维度是否符合预期\n",
    "assert encoder_outputs.shape == (batch_size, seq_len, hidden_size)\n",
    "assert hidden.shape == (num_layers, batch_size, hidden_size)\n",
    "\n",
    "print(\"Encoder测试通过!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
