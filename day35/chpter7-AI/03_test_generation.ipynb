{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad28ea9",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1fa851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本长度: 1115394\n",
      "文本前100个字符:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "字典大小: 65\n",
      "字典内容: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "字符到索引的映射示例:\n",
      "'F' -> 18\n",
      "'i' -> 47\n",
      "'r' -> 56\n",
      "'s' -> 57\n",
      "'t' -> 58\n",
      "' ' -> 1\n",
      "'C' -> 15\n",
      "'i' -> 47\n",
      "'t' -> 58\n",
      "'i' -> 47\n",
      "'z' -> 64\n",
      "'e' -> 43\n",
      "'n' -> 52\n",
      "':' -> 10\n",
      "'\n",
      "' -> 0\n",
      "'B' -> 14\n",
      "'e' -> 43\n",
      "'f' -> 44\n",
      "'o' -> 53\n",
      "'r' -> 56\n",
      "\n",
      "文本转换为数字序列的前20个元素:\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n",
      "将数字序列转回字符:\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 读取Shakespeare文本文件\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 打印文本的前100个字符\n",
    "print(f\"文本长度: {len(text)}\")\n",
    "print(f\"文本前100个字符:\\n{text[:100]}\")\n",
    "\n",
    "# 创建字符级别的字典\n",
    "vocab = sorted(set(text))\n",
    "print(f\"字典大小: {len(vocab)}\")\n",
    "print(f\"字典内容: {vocab}\")\n",
    "\n",
    "# 创建字符到索引的映射\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "\n",
    "# 打印映射示例\n",
    "print(\"\\n字符到索引的映射示例:\")\n",
    "for char in text[:20]:\n",
    "    print(f\"'{char}' -> {char_to_idx[char]}\")\n",
    "\n",
    "# 将文本转换为数字序列\n",
    "text_as_int = np.array([char_to_idx[c] for c in text]) #把全部文本都变为id\n",
    "print(f\"\\n文本转换为数字序列的前20个元素:\\n{text_as_int[:20]}\")\n",
    "print(f\"将数字序列转回字符:\\n{''.join([idx_to_char[idx] for idx in text_as_int[:20]])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758b35f",
   "metadata": {},
   "source": [
    "# 把莎士比亚文集分成一个一个的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28fc864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入批次形状: torch.Size([64, 100])\n",
      "目标批次形状: torch.Size([64, 100])\n",
      "tensor([[53, 59, 56,  ...,  6,  0, 32],\n",
      "        [58, 46,  1,  ..., 57, 58, 56],\n",
      "        [52, 42,  1,  ..., 52, 41, 43],\n",
      "        ...,\n",
      "        [43, 43,  1,  ..., 43, 39, 58],\n",
      "        [39, 52, 42,  ...,  1, 57, 53],\n",
      "        [52, 58, 11,  ..., 21, 26, 19]], dtype=torch.int32)\n",
      "tensor([[59, 56,  1,  ...,  0, 32, 46],\n",
      "        [46,  1, 53,  ..., 58, 56, 47],\n",
      "        [42,  1, 35,  ..., 41, 43,  1],\n",
      "        ...,\n",
      "        [43,  1, 53,  ..., 39, 58, 46],\n",
      "        [52, 42,  1,  ..., 57, 53, 52],\n",
      "        [58, 11,  1,  ..., 26, 19,  1]], dtype=torch.int32)\n",
      "\n",
      "数据集大小: 11043\n",
      "批次数量: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HDS\\AppData\\Local\\Temp\\ipykernel_27708\\2088263501.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  batch = torch.tensor(batch)\n"
     ]
    }
   ],
   "source": [
    "# 定义序列长度和批次大小\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "seq_length = 100  # 每个样本的序列长度\n",
    "batch_size = 64   # 每个批次的样本数量\n",
    "\n",
    "# 创建自定义数据集类\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text_as_int, seq_length):\n",
    "        self.text_as_int = text_as_int\n",
    "        self.seq_length = seq_length\n",
    "        self.sub_len = seq_length + 1 #一个样本的长度\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 计算可能的序列数量\n",
    "        return len(self.text_as_int)//(self.seq_length+1) #+1是因为要预测下一个字符\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_as_int[idx*self.sub_len:(idx+1)*self.sub_len] #返回一个样本\n",
    "\n",
    "# 定义collate函数，用于处理批次数据\n",
    "def collate_fct(batch):\n",
    "    # 将批次数据转换为张量\n",
    "    batch = torch.tensor(batch)\n",
    "    # 输入序列是除了最后一个字符的所有字符\n",
    "    input_batch = batch[:, :-1]\n",
    "    # 目标序列是除了第一个字符的所有字符\n",
    "    target_batch = batch[:, 1:]\n",
    "    return input_batch, target_batch\n",
    "\n",
    "# 创建数据集实例\n",
    "shakespeare_dataset = ShakespeareDataset(text_as_int, seq_length)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataloader = DataLoader(shakespeare_dataset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=collate_fct)\n",
    "\n",
    "# 打印示例，查看输入和目标\n",
    "for input_batch, target_batch in dataloader:\n",
    "    print(f\"输入批次形状: {input_batch.shape}\")\n",
    "    print(f\"目标批次形状: {target_batch.shape}\")\n",
    "    \n",
    "    # 打印第一个样本的输入和目标\n",
    "    print(input_batch)\n",
    "    print(target_batch)\n",
    "    break\n",
    "\n",
    "print(f\"\\n数据集大小: {len(shakespeare_dataset)}\")\n",
    "print(f\"批次数量: {len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5825e7",
   "metadata": {},
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShakespeareRNN(\n",
      "  (embedding): Embedding(65, 256)\n",
      "  (rnn): RNN(256, 1024, batch_first=True)\n",
      "  (dense): Linear(in_features=1024, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义莎士比亚文本生成的RNN模型\n",
    "class ShakespeareRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        \"\"\"\n",
    "        初始化RNN模型\n",
    "        \n",
    "        参数:\n",
    "        vocab_size: 词汇表大小，即字符数量\n",
    "        embedding_dim: 嵌入层维度，将字符转换为向量表示\n",
    "        hidden_dim: RNN隐藏层维度\n",
    "        batch_size: 批次大小\n",
    "        \"\"\"\n",
    "        super(ShakespeareRNN, self).__init__()\n",
    "        \n",
    "        # 嵌入层：将字符ID转换为密集向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN层：处理序列信息\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,      # 输入特征维度\n",
    "            hidden_dim,         # 隐藏状态维度\n",
    "            num_layers=1,       # RNN层数\n",
    "            bidirectional=False, # 单向RNN\n",
    "            batch_first=True    # 批次维度在前\n",
    "        )\n",
    "        \n",
    "        # 全连接层：将RNN输出映射到词汇表大小\n",
    "        self.dense = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        参数:\n",
    "        x: 输入序列，形状为 [batch_size, sequence_length]\n",
    "        hidden: 初始隐藏状态，默认为None\n",
    "        \n",
    "        返回:\n",
    "        output: 模型输出，形状为 [batch_size, sequence_length, vocab_size]\n",
    "        hidden: 最终隐藏状态\n",
    "        \"\"\"\n",
    "        # 输入形状: [batch_size, sequence_length]\n",
    "        x = self.embedding(x)  # 嵌入层：形状变为 [batch_size, sequence_length, embedding_dim]\n",
    "        output, hidden = self.rnn(x, hidden)  # RNN层：形状为 [batch_size, sequence_length, hidden_dim]\n",
    "        output = self.dense(output)  # 全连接层：形状为 [batch_size, sequence_length, vocab_size]\n",
    "        return output, hidden\n",
    "    \n",
    "\n",
    "# 定义模型超参数\n",
    "vocab_size = len(char_to_idx)  # 词汇表大小：字符到索引的映射数量\n",
    "embedding_dim = 256  # 嵌入维度：字符向量表示的维度\n",
    "rnn_units = 1024  # RNN隐藏单元数量：控制模型的表达能力\n",
    "\n",
    "# 实例化模型\n",
    "model = ShakespeareRNN(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c36ad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10])\n",
      "输入数据:\n",
      "tensor([[23,  3, 55,  6, 13, 41, 51, 18, 64, 54],\n",
      "        [ 0, 50, 41, 15, 47, 59, 37, 40,  7, 55]])\n",
      "\n",
      "输出形状: torch.Size([2, 10, 65])\n",
      "隐藏状态形状: torch.Size([1, 2, 1024])\n",
      "输出数据形状: torch.Size([2, 10, 65])\n",
      "词汇表大小: 65\n",
      "\n",
      "验证结果:\n",
      "输出形状正确: True\n",
      "隐藏状态形状正确: True\n",
      "\n",
      "✅ 模型前向传播测试通过！\n"
     ]
    }
   ],
   "source": [
    "# 创建测试数据\n",
    "batch_size = 2\n",
    "sequence_length = 10\n",
    "\n",
    "# 生成随机输入序列（字符ID）\n",
    "test_input = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "print(f\"输入形状: {test_input.shape}\")\n",
    "print(f\"输入数据:\\n{test_input}\")\n",
    "\n",
    "# 进行前向传播\n",
    "with torch.no_grad():  # 不计算梯度，节省内存\n",
    "    output, hidden = model(test_input)\n",
    "\n",
    "# 打印输出信息\n",
    "print(f\"\\n输出形状: {output.shape}\")\n",
    "print(f\"隐藏状态形状: {hidden.shape}\")\n",
    "print(f\"输出数据形状: {output.shape}\")\n",
    "print(f\"词汇表大小: {vocab_size}\")\n",
    "\n",
    "# 验证输出维度是否正确\n",
    "expected_output_shape = (batch_size, sequence_length, vocab_size)\n",
    "expected_hidden_shape = (1, batch_size, rnn_units)  # (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "print(f\"\\n验证结果:\")\n",
    "print(f\"输出形状正确: {output.shape == expected_output_shape}\")\n",
    "print(f\"隐藏状态形状正确: {hidden.shape == expected_hidden_shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ 模型前向传播测试通过！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
