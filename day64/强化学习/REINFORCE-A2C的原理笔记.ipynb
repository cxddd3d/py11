{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# REINFORCE 与 A2C 原理笔记\n",
    "## 一、策略梯度方法基础\n",
    "策略梯度方法的核心思想是将策略从表格形式转化为函数形式（如神经网络），通过优化参数化策略以最大化目标函数。其核心流程为：定义目标函数→计算目标函数梯度→通过梯度上升优化策略。\n",
    "1. 目标函数\n",
    "策略梯度方法的目标函数用于衡量策略性能，主要包括两类：\n",
    "平均状态值（\n",
    "v\n",
    "  \n",
    "π\n",
    "​\n",
    " \n",
    "）\n",
    "定义为状态值的加权平均：\n",
    "v\n",
    "  \n",
    "π\n",
    "​\n",
    " =∑ \n",
    "s∈S\n",
    "​\n",
    " d(s)v \n",
    "π\n",
    "​\n",
    " (s)=E \n",
    "S∼d\n",
    "​\n",
    " [v \n",
    "π\n",
    "​\n",
    " (S)]\n",
    "其中，\n",
    "d(s)\n",
    " 是状态权重（概率分布），\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)\n",
    " 是状态\n",
    "s\n",
    "的长期回报期望。\n",
    "若\n",
    "d\n",
    "独立于策略（记为\n",
    "d \n",
    "0\n",
    "​\n",
    " \n",
    "），可设为均匀分布（\n",
    "d \n",
    "0\n",
    "​\n",
    " (s)=1/∣S∣\n",
    "）或仅关注起始状态（\n",
    "d \n",
    "0\n",
    "​\n",
    " (s \n",
    "0\n",
    "​\n",
    " )=1\n",
    "）。\n",
    "若\n",
    "d\n",
    "依赖策略（记为\n",
    "d \n",
    "π\n",
    "​\n",
    " \n",
    "），通常采用策略\n",
    "π\n",
    "的平稳分布（长期访问频率越高的状态权重越大）。\n",
    "平均奖励（\n",
    "r\n",
    "  \n",
    "π\n",
    "​\n",
    " \n",
    "）\n",
    "定义为单步即时奖励的加权平均：\n",
    "r\n",
    "  \n",
    "π\n",
    "​\n",
    " =∑ \n",
    "s∈S\n",
    "​\n",
    " d \n",
    "π\n",
    "​\n",
    " (s)r \n",
    "π\n",
    "​\n",
    " (s)=E \n",
    "S∼d \n",
    "π\n",
    "​\n",
    " \n",
    "​\n",
    " [r \n",
    "π\n",
    "​\n",
    " (S)]\n",
    "其中，\n",
    "r \n",
    "π\n",
    "​\n",
    " (s)=∑ \n",
    "a∈A\n",
    "​\n",
    " π(a∣s)r(s,a)\n",
    " 是状态\n",
    "s\n",
    "下的平均单步奖励。\n",
    "其等价形式为无穷步奖励的平均极限：\n",
    "r\n",
    "  \n",
    "π\n",
    "​\n",
    " =lim \n",
    "n→∞\n",
    "​\n",
    "  \n",
    "n\n",
    "1\n",
    "​\n",
    " E[∑ \n",
    "k=1\n",
    "n\n",
    "​\n",
    " R \n",
    "t+k\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s \n",
    "0\n",
    "​\n",
    " ]\n",
    "两者关系：在折扣场景（\n",
    "γ∈(0,1)\n",
    "）中，\n",
    "r\n",
    "  \n",
    "π\n",
    "​\n",
    " =(1−γ) \n",
    "v\n",
    "  \n",
    "π\n",
    "​\n",
    " \n",
    "，优化其一等价于优化其二。\n",
    "2. 目标函数梯度\n",
    "目标函数梯度是策略更新的核心，其通用形式为：\n",
    "∇ \n",
    "θ\n",
    "​\n",
    " J(θ)=E[∇ \n",
    "θ\n",
    "​\n",
    " lnπ(A∣S,θ)⋅q \n",
    "π\n",
    "​\n",
    " (S,A)]\n",
    "其中：\n",
    "J(θ)\n",
    " 可为\n",
    "v\n",
    "  \n",
    "π\n",
    "​\n",
    " \n",
    "、\n",
    "r\n",
    "  \n",
    "π\n",
    "​\n",
    " \n",
    "等目标函数；\n",
    "π(a∣s,θ)\n",
    " 是参数化策略（如神经网络输出的动作概率）；\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    " 是动作值函数（状态\n",
    "s\n",
    "下采取动作\n",
    "a\n",
    "的长期回报期望）；\n",
    "推导核心：利用\n",
    "∇ \n",
    "θ\n",
    "​\n",
    " π(a∣s,θ)=π(a∣s,θ)⋅∇ \n",
    "θ\n",
    "​\n",
    " lnπ(a∣s,θ)\n",
    "，将梯度转化为对数概率的期望形式，便于采样估计。\n",
    "3. 策略表示与探索利用平衡\n",
    "策略表示：通常用 softmax 函数确保输出为合法概率分布：\n",
    "π(a∣s,θ)= \n",
    "∑ \n",
    "a \n",
    "′\n",
    " ∈A\n",
    "​\n",
    " e \n",
    "h(s,a \n",
    "′\n",
    " ,θ)\n",
    " \n",
    "e \n",
    "h(s,a,θ)\n",
    " \n",
    "​\n",
    " \n",
    "其中\n",
    "h(s,a,θ)\n",
    "为神经网络输出的分数，softmax 保证\n",
    "π(a∣s,θ)∈(0,1)\n",
    "且和为 1。\n",
    "探索与利用：通过参数更新自动平衡：\n",
    "利用（Exploitation）：若\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    "较大（动作价值高），参数更新会提高\n",
    "π(a∣s,θ)\n",
    "；\n",
    "探索（Exploration）：若\n",
    "π(a∣s,θ)\n",
    "较小（动作被选概率低），参数更新会增强其概率。\n",
    "## 二、REINFORCE 算法\n",
    "REINFORCE 是最早的策略梯度算法之一，基于蒙特卡洛（Monte Carlo）方法估计动作值函数，属于离线（offline）算法。\n",
    "1. 核心思想\n",
    "用蒙特卡洛估计的回报（Gt） 近似动作值函数\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    "，再通过梯度上升更新策略参数。\n",
    "2. 参数更新公式\n",
    "θ \n",
    "t+1\n",
    "​\n",
    " =θ \n",
    "t\n",
    "​\n",
    " +α⋅∇ \n",
    "θ\n",
    "​\n",
    " lnπ(a \n",
    "t\n",
    "​\n",
    " ∣s \n",
    "t\n",
    "​\n",
    " ,θ \n",
    "t\n",
    "​\n",
    " )⋅G \n",
    "t\n",
    "​\n",
    " \n",
    "其中：\n",
    "α\n",
    " 是学习率；\n",
    "G \n",
    "t\n",
    "​\n",
    " =∑ \n",
    "k=0\n",
    "T−t\n",
    "​\n",
    " γ \n",
    "k\n",
    " R \n",
    "t+k+1\n",
    "​\n",
    " \n",
    " 是从时刻\n",
    "t\n",
    "开始的折扣回报（蒙特卡洛估计的动作值）。\n",
    "3. 算法流程（伪代码）\n",
    "初始化策略参数\n",
    "θ\n",
    "、学习率\n",
    "α\n",
    "、折扣因子\n",
    "γ\n",
    "；\n",
    "对于第\n",
    "k\n",
    "次迭代：\n",
    "从初始状态\n",
    "s \n",
    "0\n",
    "​\n",
    " \n",
    "出发，根据当前策略\n",
    "π(⋅∣⋅,θ)\n",
    "生成完整轨迹：\n",
    "(s \n",
    "0\n",
    "​\n",
    " ,a \n",
    "0\n",
    "​\n",
    " ,R \n",
    "1\n",
    "​\n",
    " ,s \n",
    "1\n",
    "​\n",
    " ,a \n",
    "1\n",
    "​\n",
    " ,R \n",
    "2\n",
    "​\n",
    " ,...,s \n",
    "T−1\n",
    "​\n",
    " ,a \n",
    "T−1\n",
    "​\n",
    " ,R \n",
    "T\n",
    "​\n",
    " )\n",
    "；\n",
    "对轨迹中每个时刻\n",
    "t\n",
    "（\n",
    "0≤t<T\n",
    "）：\n",
    "计算回报\n",
    "G \n",
    "t\n",
    "​\n",
    " =R \n",
    "t+1\n",
    "​\n",
    " +γR \n",
    "t+2\n",
    "​\n",
    " +...+γ \n",
    "T−t−1\n",
    " R \n",
    "T\n",
    "​\n",
    " \n",
    "；\n",
    "更新参数：\n",
    "θ=θ+α⋅∇ \n",
    "θ\n",
    "​\n",
    " lnπ(a \n",
    "t\n",
    "​\n",
    " ∣s \n",
    "t\n",
    "​\n",
    " ,θ)⋅G \n",
    "t\n",
    "​\n",
    " \n",
    "；\n",
    "重复迭代直至收敛。\n",
    "4. 特点\n",
    "优点：实现简单，直接优化策略，无需构建复杂价值函数，适用于连续动作空间；\n",
    "缺点：\n",
    "方差大（依赖完整轨迹的回报估计）；\n",
    "离线更新（需等待轨迹结束才能更新参数），样本利用率低；\n",
    "对学习率敏感，收敛不稳定。\n",
    "## 三、A2C（Advantage Actor-Critic）算法\n",
    "A2C 是 actor-critic 框架的典型代表，结合了策略梯度（actor）和价值函数估计（critic），属于在线（online）算法，可减少方差并提高效率。\n",
    "1. 核心思想\n",
    "Actor：负责策略优化，输出动作概率\n",
    "π(a∣s,θ)\n",
    "，类似 REINFORCE；\n",
    "Critic：负责估计优势函数（Advantage Function） \n",
    "A(s,a)=q \n",
    "π\n",
    "​\n",
    " (s,a)−v \n",
    "π\n",
    "​\n",
    " (s)\n",
    "，用于指导 actor 更新，减少方差。\n",
    "2. 优势函数\n",
    "优势函数衡量动作\n",
    "a\n",
    "相对于状态\n",
    "s\n",
    "平均价值的优劣：\n",
    "若\n",
    "A(s,a)>0\n",
    "：动作\n",
    "a\n",
    "优于状态\n",
    "s\n",
    "的平均动作；\n",
    "若\n",
    "A(s,a)<0\n",
    "：动作\n",
    "a\n",
    "劣于平均。\n",
    "通过时序差分（TD）方法估计：\n",
    "A(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )≈R \n",
    "t+1\n",
    "​\n",
    " +γv(s \n",
    "t+1\n",
    "​\n",
    " )−v(s \n",
    "t\n",
    "​\n",
    " )\n",
    "，其中\n",
    "v(s)\n",
    "是状态值函数（critic 输出）。\n",
    "3. 参数更新公式\n",
    "Actor 更新（策略梯度）：\n",
    "θ \n",
    "t+1\n",
    "​\n",
    " =θ \n",
    "t\n",
    "​\n",
    " +α⋅∇ \n",
    "θ\n",
    "​\n",
    " lnπ(a \n",
    "t\n",
    "​\n",
    " ∣s \n",
    "t\n",
    "​\n",
    " ,θ \n",
    "t\n",
    "​\n",
    " )⋅A(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )\n",
    "Critic 更新（价值函数拟合）：\n",
    "通过 TD 误差优化\n",
    "v(s)\n",
    "：\n",
    "v(s \n",
    "t\n",
    "​\n",
    " )←v(s \n",
    "t\n",
    "​\n",
    " )+β⋅(R \n",
    "t+1\n",
    "​\n",
    " +γv(s \n",
    "t+1\n",
    "​\n",
    " )−v(s \n",
    "t\n",
    "​\n",
    " ))\n",
    "，其中\n",
    "β\n",
    "是 critic 的学习率。\n",
    "4. 算法流程\n",
    "初始化 actor 参数\n",
    "θ\n",
    "、critic 参数\n",
    "ϕ\n",
    "（用于估计\n",
    "v(s;ϕ)\n",
    "）、学习率\n",
    "α,β\n",
    "、折扣因子\n",
    "γ\n",
    "；\n",
    "对于每个时间步\n",
    "t\n",
    "：\n",
    "Actor 根据\n",
    "π(⋅∣s \n",
    "t\n",
    "​\n",
    " ,θ)\n",
    "选择动作\n",
    "a \n",
    "t\n",
    "​\n",
    " \n",
    "，与环境交互得到\n",
    "R \n",
    "t+1\n",
    "​\n",
    " ,s \n",
    "t+1\n",
    "​\n",
    " \n",
    "；\n",
    "Critic 计算 TD 误差：\n",
    "δ \n",
    "t\n",
    "​\n",
    " =R \n",
    "t+1\n",
    "​\n",
    " +γv(s \n",
    "t+1\n",
    "​\n",
    " ;ϕ)−v(s \n",
    "t\n",
    "​\n",
    " ;ϕ)\n",
    "（即优势函数近似\n",
    "A(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )\n",
    "）；\n",
    "更新 actor：\n",
    "θ=θ+α⋅∇ \n",
    "θ\n",
    "​\n",
    " lnπ(a \n",
    "t\n",
    "​\n",
    " ∣s \n",
    "t\n",
    "​\n",
    " ,θ)⋅δ \n",
    "t\n",
    "​\n",
    " \n",
    "；\n",
    "更新 critic：\n",
    "ϕ=ϕ+β⋅δ \n",
    "t\n",
    "​\n",
    " ⋅∇ \n",
    "ϕ\n",
    "​\n",
    " v(s \n",
    "t\n",
    "​\n",
    " ;ϕ)\n",
    "；\n",
    "重复迭代直至收敛。\n",
    "5. 特点\n",
    "优点：\n",
    "方差小（用优势函数替代蒙特卡洛回报，减少噪声）；\n",
    "在线更新（每步即可更新参数），样本利用率高；\n",
    "收敛更稳定，适合复杂环境；\n",
    "缺点：需同时训练 actor 和 critic，参数调优更复杂；critic 的估计误差可能影响 actor 更新。\n",
    "## 四、REINFORCE 与 A2C 的对比\n",
    "维度\tREINFORCE\tA2C\n",
    "价值估计方法\t蒙特卡洛（完整轨迹回报）\t时序差分（TD 误差，优势函数）\n",
    "更新方式\t离线（轨迹结束后更新）\t在线（每步更新）\n",
    "方差\t高\t低（优势函数减少噪声）\n",
    "样本利用率\t低\t高\n",
    "复杂度\t简单（仅需训练 actor）\t复杂（需同步训练 actor 和 critic）\n",
    "适用场景\t简单环境，对实时性要求低\t复杂环境，实时性要求"
   ],
   "id": "ad5c32e2cac3fc68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
