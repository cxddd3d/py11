{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 蒙特卡洛（MC）强化学习方法笔记\n",
    "## 一、蒙特卡洛估计基础\n",
    "蒙特卡洛（Monte Carlo）方法的核心思想是通过大量采样估计随机变量的期望，基于大数定律：当采样数量足够多时，样本平均值会收敛到真实期望。在强化学习中，用于估计状态价值（\n",
    "v\n",
    "）或动作价值（\n",
    "q\n",
    "），无需依赖环境模型（model-free）。\n",
    "## 二、三种蒙特卡洛强化学习方法\n",
    "### 1. MC Basic（基础蒙特卡洛方法）\n",
    "原理\n",
    "MC Basic 是最基础的无模型蒙特卡洛方法，核心是将策略迭代（Policy Iteration）中依赖模型的部分替换为无模型采样，直接估计动作价值（\n",
    "q\n",
    "值）。\n",
    "核心步骤\n",
    "策略评估（Policy Evaluation）\n",
    "对每个状态 - 动作对\n",
    "(s,a)\n",
    "，收集足够多从\n",
    "(s,a)\n",
    "出发、遵循当前策略\n",
    "π \n",
    "k\n",
    "​\n",
    " \n",
    "的 episode，计算这些 episode 的回报（return）平均值，作为\n",
    "q \n",
    "π \n",
    "k\n",
    "​\n",
    " \n",
    "​\n",
    " (s,a)\n",
    "的估计（\n",
    "q \n",
    "k\n",
    "​\n",
    " (s,a)\n",
    "）。\n",
    "公式：\n",
    "q \n",
    "k\n",
    "​\n",
    " (s,a)≈ \n",
    "N\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " g \n",
    "(i)\n",
    " (s,a)\n",
    "，其中\n",
    "g \n",
    "(i)\n",
    " (s,a)\n",
    "是第\n",
    "i\n",
    "个 episode 的回报。\n",
    "策略改进（Policy Improvement）\n",
    "基于估计的\n",
    "q \n",
    "k\n",
    "​\n",
    " (s,a)\n",
    "，采用贪心策略更新：对每个状态\n",
    "s\n",
    "，选择使\n",
    "q \n",
    "k\n",
    "​\n",
    " (s,a)\n",
    "最大的动作\n",
    "a \n",
    "∗\n",
    " \n",
    "，新策略\n",
    "π \n",
    "k+1\n",
    "​\n",
    " (a \n",
    "∗\n",
    " ∣s)=1\n",
    "（确定性策略）。\n",
    "优缺点\n",
    "优点：直观展示无模型蒙特卡洛方法的核心思想，与策略迭代直接对应。\n",
    "缺点：效率极低（需为每个\n",
    "(s,a)\n",
    "单独收集 episode），实际中不实用。\n",
    "### 2. MC Exploring Starts（带探索起点的蒙特卡洛方法）\n",
    "原理\n",
    "MC Exploring Starts 是 MC Basic 的优化，通过高效利用数据和逐 episode 更新提升效率，依赖 “探索起点（Exploring Starts）” 假设。\n",
    "核心改进\n",
    "数据高效利用\n",
    "不再仅使用 episode 的初始\n",
    "(s,a)\n",
    "，而是利用 episode 中所有访问过的\n",
    "(s,a)\n",
    "对：\n",
    "First-visit：仅用每个\n",
    "(s,a)\n",
    "在 episode 中首次出现后的回报更新\n",
    "q\n",
    "值。\n",
    "Every-visit：用每个\n",
    "(s,a)\n",
    "在 episode 中每次出现后的回报更新\n",
    "q\n",
    "值。\n",
    "逐 episode 更新策略\n",
    "无需等待所有 episode 收集完成，每生成一个 episode 就更新\n",
    "q\n",
    "值和策略，符合广义策略迭代（GPI） 框架（策略评估和改进交替进行，不要求精确估计）。\n",
    "探索起点假设\n",
    "要求每个\n",
    "(s,a)\n",
    "对都能作为 episode 的起点被访问，确保所有\n",
    "(s,a)\n",
    "都有足够数据。\n",
    "核心步骤\n",
    "随机选择初始状态 - 动作对\n",
    "(s \n",
    "0\n",
    "​\n",
    " ,a \n",
    "0\n",
    "​\n",
    " )\n",
    "（满足探索起点假设），遵循当前策略生成 episode。\n",
    "从 episode 末尾倒推计算每个\n",
    "(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )\n",
    "的回报\n",
    "g\n",
    "，用 first-visit 或 every-visit 方法更新\n",
    "q\n",
    "值。\n",
    "基于更新后的\n",
    "q\n",
    "值，用贪心策略改进策略。\n",
    "优缺点\n",
    "优点：数据利用率高，收敛速度快于 MC Basic。\n",
    "缺点：依赖探索起点假设，实际场景中难以实现（如机器人无法随意设置初始状态 - 动作）。\n",
    "### 3. MC ε-Greedy（ε- 贪心蒙特卡洛方法）\n",
    "原理\n",
    "MC ε-Greedy 去除了探索起点假设，通过软策略（ε-greedy） 平衡探索与利用，确保所有状态 - 动作对能被访问。\n",
    "核心改进\n",
    "ε-greedy 策略（软策略）\n",
    "对每个状态\n",
    "s\n",
    "，以概率\n",
    "1−ε+ε/∣A(s)∣\n",
    "选择当前最优动作（利用），以概率\n",
    "ε/∣A(s)∣\n",
    "选择其他动作（探索），其中\n",
    "∣A(s)∣\n",
    "是状态\n",
    "s\n",
    "的动作数。\n",
    "公式：\n",
    "$$\n",
    "π(a∣s)={ \n",
    "1−ε+ε/∣A(s)∣\n",
    "ε/∣A(s)∣\n",
    "​\n",
    "  \n",
    "若 a是最优动作\n",
    "其他动作\n",
    "​\n",
    " \n",
    "$\n",
    "$$$\n",
    "无需探索起点\n",
    "由于 ε-greedy 策略确保每个动作有正概率被选择，足够长的 episode 可覆盖所有\n",
    "(s,a)\n",
    "对，无需强制从每个\n",
    "(s,a)\n",
    "出发。\n",
    "核心步骤\n",
    "随机选择初始状态 - 动作对，遵循当前 ε-greedy 策略生成足够长的 episode。\n",
    "用 every-visit 方法（充分利用数据）从 episode 倒推计算回报，更新\n",
    "q\n",
    "值。\n",
    "基于更新后的\n",
    "q\n",
    "值，用 ε-greedy 策略改进策略（而非纯贪心）。\n",
    "优缺点\n",
    "优点：无需探索起点假设，实际场景中易实现；通过 ε 平衡探索与利用。\n",
    "缺点：策略最优性受 ε 影响（ε 越大，探索越强但最优性越差），通常需随学习进程减小 ε。\n",
    "## 三、三种方法对比\n",
    "方法\t核心改进\t依赖假设\t数据利用率\t实用性\n",
    "MC Basic\t无模型估计动作价值\t无（但效率极低）\t低\t理论参考\n",
    "MC Exploring Starts\t高效利用数据、逐 episode 更新\t探索起点假设\t中\t部分场景适用\n",
    "MC ε-Greedy\tε-greedy 策略平衡探索利用\t无（依赖长 episode 覆盖）\t高\t广泛实用\n",
    "## 四、关键结论\n",
    "三种方法均基于蒙特卡洛估计，核心是通过采样回报的平均值估计动作价值。\n",
    "演进逻辑：从低效基础方法（MC Basic）到高效但有假设（MC Exploring Starts），再到实用无假设（MC ε-Greedy）。\n",
    "实际应用中，MC ε-Greedy 是主流，通过调整 ε 可平衡探索与利用，适应复杂环境。"
   ],
   "id": "5474341299c109d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
