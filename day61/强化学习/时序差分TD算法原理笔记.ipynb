{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 时序差分（TD）学习方法原理笔记\n",
    "## 一、TD 方法概述\n",
    "时序差分（Temporal-Difference, TD）学习是强化学习中一类无模型（model-free） 的方法，核心是通过时序差分校正实现价值估计的迭代更新。它结合了蒙特卡洛（MC）方法的采样特性和动态规划（DP）的自举（bootstrapping）特性，能够在线更新价值估计，无需等待整个回合（episode）结束。\n",
    "TD 方法的核心目标是估计状态价值（\n",
    "v\n",
    "）或动作价值（\n",
    "q\n",
    "），并通过价值估计优化策略，最终找到最优策略。\n",
    "## 二、TD 方法的核心原理\n",
    "1. 核心思想\n",
    "TD 方法通过当前估计和下一时刻的估计之间的差异（时序差分）来更新价值，公式的通用形式为：\n",
    "新估计=当前估计+步长×(目标−当前估计)\n",
    "其中，“目标” 称为TD 目标（TD target），“目标与当前估计的差” 称为TD 误差（TD error）。\n",
    "2. 数学基础\n",
    "TD 方法本质是求解贝尔曼方程的随机近似算法（基于 Robbins-Monro 算法）。对于给定策略\n",
    "π\n",
    "，状态价值\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)\n",
    "满足贝尔曼期望方程：\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)=E[R+γv \n",
    "π\n",
    "​\n",
    " (S \n",
    "′\n",
    " )∣S=s]\n",
    "TD 方法通过采样数据（\n",
    "s,r,s \n",
    "′\n",
    " \n",
    "）迭代逼近该方程的解，无需已知环境模型（状态转移概率\n",
    "p(s \n",
    "′\n",
    " ∣s,a)\n",
    "）。\n",
    "3. 关键概念\n",
    "自举（Bootstrapping）：使用当前价值估计（而非真实回报）更新价值，如利用\n",
    "v(s \n",
    "′\n",
    " )\n",
    "估计\n",
    "v(s)\n",
    "。\n",
    "在线更新（Online Update）：每一步交互后立即更新价值，无需等待回合结束（区别于 MC 方法）。\n",
    "TD 目标（TD Target）：用于校正当前估计的目标值，通常为即时奖励加折扣后的下一状态 / 动作价值估计。\n",
    "TD 误差（TD Error）：当前估计与 TD 目标的差，反映估计的偏差，用于驱动更新。\n",
    "## 三、主要 TD 算法\n",
    "1. 估计状态价值的 TD 算法\n",
    "原理\n",
    "针对给定策略\n",
    "π\n",
    "，估计状态价值\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)\n",
    "，更新公式为：\n",
    "v \n",
    "t+1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )=v \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )⋅δ \n",
    "t\n",
    "​\n",
    " \n",
    "其中：\n",
    "v \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )\n",
    "：\n",
    "t\n",
    "时刻对\n",
    "s \n",
    "t\n",
    "​\n",
    " \n",
    "的状态价值估计；\n",
    "α \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )\n",
    "：学习率（步长）；\n",
    "TD 误差：\n",
    "δ \n",
    "t\n",
    "​\n",
    " =v \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " )−[r \n",
    "t+1\n",
    "​\n",
    " +γv \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " )]\n",
    "；\n",
    "TD 目标：\n",
    "v\n",
    "ˉ\n",
    "  \n",
    "t\n",
    "​\n",
    " =r \n",
    "t+1\n",
    "​\n",
    " +γv \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " )\n",
    "（即时奖励加折扣后的下一状态价值估计）。\n",
    "特点\n",
    "仅更新当前访问状态\n",
    "s \n",
    "t\n",
    "​\n",
    " \n",
    "的价值，未访问状态价值保持不变；\n",
    "收敛性：当学习率满足\n",
    "∑α \n",
    "t\n",
    "​\n",
    " (s)=∞\n",
    "且\n",
    "∑α \n",
    "t\n",
    "2\n",
    "​\n",
    " (s)<∞\n",
    "时，以概率 1 收敛到真实状态价值\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)\n",
    "。\n",
    "2. Sarsa（估计动作价值）\n",
    "原理\n",
    "Sarsa 是用于估计动作价值（\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    "）的 TD 算法，因使用数据\n",
    "(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " ,r \n",
    "t+1\n",
    "​\n",
    " ,s \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "t+1\n",
    "​\n",
    " )\n",
    "而得名（State-Action-Reward-State-Action）。更新公式为：\n",
    "q \n",
    "t+1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )=q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )⋅δ \n",
    "t\n",
    "​\n",
    " \n",
    "其中：\n",
    "TD 误差：\n",
    "δ \n",
    "t\n",
    "​\n",
    " =q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−[r \n",
    "t+1\n",
    "​\n",
    " +γq \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "t+1\n",
    "​\n",
    " )]\n",
    "；\n",
    "TD 目标：\n",
    "q\n",
    "ˉ\n",
    "​\n",
    "  \n",
    "t\n",
    "​\n",
    " =r \n",
    "t+1\n",
    "​\n",
    " +γq \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "t+1\n",
    "​\n",
    " )\n",
    "。\n",
    "特点\n",
    "On-policy：行为策略（生成数据的策略）与目标策略（待优化的策略）相同；\n",
    "需结合策略改进（如\n",
    "ϵ\n",
    "- 贪心策略）实现最优策略搜索。\n",
    "3. Expected Sarsa\n",
    "原理\n",
    "Sarsa 的变体，用期望代替下一个动作的采样，减少估计方差。更新公式为：\n",
    "q \n",
    "t+1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )=q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )⋅δ \n",
    "t\n",
    "​\n",
    " \n",
    "其中TD 目标为：\n",
    "q\n",
    "ˉ\n",
    "​\n",
    "  \n",
    "t\n",
    "​\n",
    " =r \n",
    "t+1\n",
    "​\n",
    " +γE[q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,A)∣s \n",
    "t+1\n",
    "​\n",
    " ]=r \n",
    "t+1\n",
    "​\n",
    " +γ∑ \n",
    "a\n",
    "​\n",
    " π(a∣s \n",
    "t+1\n",
    "​\n",
    " )q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a)\n",
    "特点\n",
    "计算量大于 Sarsa（需计算所有可能下一个动作的期望），但方差更小；\n",
    "仍为 On-policy 算法。\n",
    "4. n-step Sarsa\n",
    "原理\n",
    "结合 Sarsa 和 MC 方法，通过n 步回报更新价值，平衡偏差和方差。n 步 TD 目标定义为：\n",
    "G \n",
    "t\n",
    "(n)\n",
    "​\n",
    " =r \n",
    "t+1\n",
    "​\n",
    " +γr \n",
    "t+2\n",
    "​\n",
    " +⋯+γ \n",
    "n\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+n\n",
    "​\n",
    " ,a \n",
    "t+n\n",
    "​\n",
    " )\n",
    "更新公式为：\n",
    "q \n",
    "t+n\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )=q \n",
    "t+n−1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t+n−1\n",
    "​\n",
    " ⋅[q \n",
    "t+n−1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−G \n",
    "t\n",
    "(n)\n",
    "​\n",
    " ]\n",
    "特点\n",
    "当\n",
    "n=1\n",
    "时退化为 Sarsa，当\n",
    "n→∞\n",
    "时接近 MC 方法；\n",
    "n\n",
    "越大，偏差越小但方差越大。\n",
    "5. Q-learning\n",
    "原理\n",
    "直接估计最优动作价值（\n",
    "q \n",
    "∗\n",
    "​\n",
    " (s,a)\n",
    "）的 Off-policy 算法，更新公式为：\n",
    "q \n",
    "t+1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )=q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )⋅δ \n",
    "t\n",
    "​\n",
    " \n",
    "其中TD 目标为：\n",
    "q\n",
    "ˉ\n",
    "​\n",
    "  \n",
    "t\n",
    "​\n",
    " =r \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a\n",
    "​\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a)\n",
    "特点\n",
    "Off-policy：行为策略（如\n",
    "ϵ\n",
    "- 贪心）与目标策略（贪心策略）可不同，灵活性更高；\n",
    "直接求解贝尔曼最优方程：\n",
    "q \n",
    "∗\n",
    "​\n",
    " (s,a)=E[R+γmax \n",
    "a\n",
    "​\n",
    " q \n",
    "∗\n",
    "​\n",
    " (s \n",
    "′\n",
    " ,a)∣s,a]\n",
    "；\n",
    "收敛性：满足学习率条件时，收敛到最优动作价值\n",
    "q \n",
    "∗\n",
    "​\n",
    " (s,a)\n",
    "。\n",
    "## 四、TD 方法与其他方法的对比\n",
    "特性\tTD 方法\t蒙特卡洛（MC）方法\t动态规划（DP）方法\n",
    "模型依赖\t无（model-free）\t无（model-free）\t有（需状态转移概率）\n",
    "更新时机\t在线（每步后）\t离线（回合结束后）\t全状态同步更新\n",
    "自举（Bootstrapping）\t是\t否\t是\n",
    "方差\t低（依赖少量采样）\t高（依赖整个回合）\t无（基于模型）\n",
    "偏差\t有（依赖估计值）\t无（基于真实回报）\t无（基于模型）\n",
    "## 五、统一视角\n",
    "所有 TD 算法可表示为统一形式：\n",
    "新估计=当前估计−α⋅(当前估计−TD目标)\n",
    "差异仅在于TD 目标的定义：\n",
    "算法\tTD 目标（\n",
    "q\n",
    "ˉ\n",
    "​\n",
    "  \n",
    "t\n",
    "​\n",
    " \n",
    "）\n",
    "Sarsa\t\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γq \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "t+1\n",
    "​\n",
    " )\n",
    "Expected Sarsa\t\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γ∑ \n",
    "a\n",
    "​\n",
    " π(a∣s \n",
    "t+1\n",
    "​\n",
    " )q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a)\n",
    "n-step Sarsa\t\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γr \n",
    "t+2\n",
    "​\n",
    " +⋯+γ \n",
    "n\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+n\n",
    "​\n",
    " ,a \n",
    "t+n\n",
    "​\n",
    " )\n",
    "Q-learning\t\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a\n",
    "​\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a)\n",
    "MC 方法\t\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γr \n",
    "t+2\n",
    "​\n",
    " +⋯\n",
    "（无自举）\n",
    "## 六、总结\n",
    "TD 方法是强化学习中极具影响力的无模型方法，其核心是通过时序差分校正实现价值的在线更新。关键优势包括：\n",
    "无需环境模型，适用于复杂场景；\n",
    "在线更新，可处理持续任务（continuing tasks）；\n",
    "方差低于 MC 方法，收敛更稳定。"
   ],
   "id": "b3cba70f71842782"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
