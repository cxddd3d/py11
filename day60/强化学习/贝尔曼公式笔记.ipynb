{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 贝尔曼公式（Bellman Equation）笔记\n",
    "## 1. 核心概念概述\n",
    "贝尔曼公式是强化学习中的核心工具，用于描述状态价值（state value） 之间的关系，通过数学形式连接即时奖励与未来奖励的期望值，为策略评估和优化提供理论基础。\n",
    "核心作用：将复杂的决策问题分解为递归的子问题，建立当前状态价值与未来状态价值的关联。\n",
    "关键关联：通过回报（return）的期望值定义状态价值，进而推导状态价值之间的递归关系。\n",
    "## 2. 状态价值（State Value）\n",
    "### 2.1 定义\n",
    "状态价值函数用 \n",
    "v \n",
    "π\n",
    "​\n",
    " (s)\n",
    " 表示，定义为：在策略 \n",
    "π\n",
    " 下，从状态 \n",
    "s\n",
    " 出发所能获得的期望折扣回报（discounted return）。\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)≐E[G \n",
    "t\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s]\n",
    "其中，\n",
    "G \n",
    "t\n",
    "​\n",
    " \n",
    " 为折扣回报（随机变量），计算公式为：\n",
    "G \n",
    "t\n",
    "​\n",
    " ≐R \n",
    "t+1\n",
    "​\n",
    " +γR \n",
    "t+2\n",
    "​\n",
    " +γ \n",
    "2\n",
    " R \n",
    "t+3\n",
    "​\n",
    " +…\n",
    "γ∈(0,1)\n",
    "：折扣因子，用于衡量未来奖励的当前价值（越接近 1，未来奖励影响越大）。\n",
    "R \n",
    "t+n\n",
    "​\n",
    " \n",
    "：第 \n",
    "t+n\n",
    " 步获得的即时奖励。\n",
    "### 2.2 关键性质\n",
    "依赖于策略 \n",
    "π\n",
    "：不同策略会产生不同的状态价值。\n",
    "与时间无关：仅由当前状态和策略决定，与时间步 \n",
    "t\n",
    " 无关。\n",
    "随机性来源：回报 \n",
    "G \n",
    "t\n",
    "​\n",
    " \n",
    " 的随机性源于奖励 \n",
    "R\n",
    " 和状态转移的概率分布。\n",
    "## 3. 贝尔曼公式的推导\n",
    "### 3.1 核心思路\n",
    "将折扣回报 \n",
    "G \n",
    "t\n",
    "​\n",
    " \n",
    " 分解为即时奖励和未来奖励的折扣值，进而推导状态价值的递归关系。\n",
    "### 3.2 推导步骤\n",
    "回报的分解：\n",
    "折扣回报可拆分为即时奖励与未来回报的折扣项：\n",
    "G \n",
    "t\n",
    "​\n",
    " =R \n",
    "t+1\n",
    "​\n",
    " +γG \n",
    "t+1\n",
    "​\n",
    " \n",
    "其中，\n",
    "G \n",
    "t+1\n",
    "​\n",
    " =R \n",
    "t+2\n",
    "​\n",
    " +γR \n",
    "t+3\n",
    "​\n",
    " +…\n",
    "。\n",
    "状态价值的递归关系：\n",
    "对 \n",
    "G \n",
    "t\n",
    "​\n",
    " \n",
    " 求期望，结合状态价值定义：\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)=E[R \n",
    "t+1\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s]+γE[G \n",
    "t+1\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s]\n",
    "展开期望项：\n",
    "即时奖励期望：考虑策略 \n",
    "π\n",
    " 下的动作选择概率和奖励分布：\n",
    "E[R \n",
    "t+1\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s]=∑ \n",
    "a∈A\n",
    "​\n",
    " π(a∣s)∑ \n",
    "r∈R\n",
    "​\n",
    " p(r∣s,a)r\n",
    "未来奖励期望：利用马尔可夫性质，仅依赖下一状态 \n",
    "s \n",
    "′\n",
    " \n",
    " 的价值：\n",
    "E[G \n",
    "t+1\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s]=∑ \n",
    "s \n",
    "′\n",
    " ∈S\n",
    "​\n",
    " v \n",
    "π\n",
    "​\n",
    " (s \n",
    "′\n",
    " )∑ \n",
    "a∈A\n",
    "​\n",
    " p(s \n",
    "′\n",
    " ∣s,a)π(a∣s)\n",
    "最终形式：\n",
    "合并上述两项，得到贝尔曼公式：\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)=∑ \n",
    "a∈A\n",
    "​\n",
    " π(a∣s)[∑ \n",
    "r∈R\n",
    "​\n",
    " p(r∣s,a)r+γ∑ \n",
    "s \n",
    "′\n",
    " ∈S\n",
    "​\n",
    " p(s \n",
    "′\n",
    " ∣s,a)v \n",
    "π\n",
    "​\n",
    " (s \n",
    "′\n",
    " )]\n",
    "## 4. 矩阵形式与求解\n",
    "### 4.1 矩阵表示\n",
    "将所有状态的贝尔曼方程整合为矩阵形式，简化计算：\n",
    "v \n",
    "π\n",
    "​\n",
    " =r \n",
    "π\n",
    "​\n",
    " +γP \n",
    "π\n",
    "​\n",
    " v \n",
    "π\n",
    "​\n",
    " \n",
    "v \n",
    "π\n",
    "​\n",
    " \n",
    "：状态价值向量（维度为状态数 \n",
    "n\n",
    "）。\n",
    "r \n",
    "π\n",
    "​\n",
    " \n",
    "：策略 \n",
    "π\n",
    " 下的即时奖励向量。\n",
    "P \n",
    "π\n",
    "​\n",
    " \n",
    "：状态转移概率矩阵（\n",
    "P \n",
    "π\n",
    "​\n",
    " [i][j]\n",
    " 表示从状态 \n",
    "i\n",
    " 转移到 \n",
    "j\n",
    " 的概率）。\n",
    "γ\n",
    "：折扣因子。\n",
    "### 4.2 求解方法\n",
    "解析解（Closed-form Solution）：\n",
    "直接通过矩阵求逆求解：\n",
    "v \n",
    "π\n",
    "​\n",
    " =(I−γP \n",
    "π\n",
    "​\n",
    " ) \n",
    "−1\n",
    " r \n",
    "π\n",
    "​\n",
    " \n",
    "其中 \n",
    "I\n",
    " 为单位矩阵。缺点是计算量随状态数指数增长，仅适用于小规模问题。\n",
    "迭代解法：\n",
    "通过迭代更新逼近真实价值，适用于大规模问题：\n",
    "v \n",
    "k+1\n",
    "​\n",
    " =r \n",
    "π\n",
    "​\n",
    " +γP \n",
    "π\n",
    "​\n",
    " v \n",
    "k\n",
    "​\n",
    " \n",
    "初始值 \n",
    "v \n",
    "0\n",
    "​\n",
    " \n",
    " 可任意设定（如全 0）。\n",
    "收敛性：当 \n",
    "k→∞\n",
    " 时，\n",
    "v \n",
    "k\n",
    "​\n",
    " →v \n",
    "π\n",
    "​\n",
    " \n",
    "，误差以指数速度衰减。\n",
    "## 5. 动作价值（Action Value）\n",
    "### 5.1 定义\n",
    "动作价值函数 \n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    " 表示：在策略 \n",
    "π\n",
    " 下，从状态 \n",
    "s\n",
    " 出发选择动作 \n",
    "a\n",
    " 后，所能获得的期望折扣回报。\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)≐E[G \n",
    "t\n",
    "​\n",
    " ∣S \n",
    "t\n",
    "​\n",
    " =s,A \n",
    "t\n",
    "​\n",
    " =a]\n",
    "### 5.2 与状态价值的关系\n",
    "状态价值是动作价值的加权平均（权重为策略 \n",
    "π\n",
    " 的动作选择概率）：\n",
    "v \n",
    "π\n",
    "​\n",
    " (s)=∑ \n",
    "a∈A\n",
    "​\n",
    " π(a∣s)q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    "动作价值可通过下一状态的状态价值计算：\n",
    "q \n",
    "π\n",
    "​\n",
    " (s,a)=∑ \n",
    "r∈R\n",
    "​\n",
    " p(r∣s,a)r+γ∑ \n",
    "s \n",
    "′\n",
    " ∈S\n",
    "​\n",
    " p(s \n",
    "′\n",
    " ∣s,a)v \n",
    "π\n",
    "​\n",
    " (s \n",
    "′\n",
    " )\n",
    "## 6. 关键概念：Bootstrapping\n",
    "定义：状态价值的计算依赖于其他状态的价值（递归性），通过迭代更新不断优化估计值。\n",
    "意义：体现了强化学习中 “利用已有估计改进估计” 的核心思想，是迭代解法的理论基础。\n"
   ],
   "id": "cb70fa680fb208b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
