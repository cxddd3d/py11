{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 贝尔曼最优公式（Bellman Optimality Equation）笔记\n",
    "## 1. 核心目标\n",
    "强化学习的终极目标是寻找最优策略，即能使所有状态的期望回报均达到最大的策略。贝尔曼最优公式是实现这一目标的核心工具，用于描述最优状态价值与最优策略的关系。\n",
    "## 2. 关键概念\n",
    "最优状态价值（v ∗）：在最优策略下，从状态\n",
    "s\n",
    "出发的期望折扣回报。\n",
    "最优策略（π ∗）：对于所有状态\n",
    "s\n",
    "，均满足\n",
    "v \n",
    "π \n",
    "∗\n",
    " \n",
    "​\n",
    " (s)≥v \n",
    "π\n",
    "​\n",
    " (s)\n",
    "的策略（\n",
    "π\n",
    "为任意其他策略）。\n",
    "贝尔曼最优公式：刻画最优状态价值与最优动作价值之间的递归关系，包含最大化操作以求解最优策略。\n",
    "## 3. 公式形式\n",
    "### 3.1 元素形式（Elementwise Form）\n",
    "v(s)=max \n",
    "π\n",
    "​\n",
    " ∑ \n",
    "a\n",
    "​\n",
    " π(a∣s)[∑ \n",
    "r\n",
    "​\n",
    " p(r∣s,a)r+γ∑ \n",
    "s \n",
    "′\n",
    " \n",
    "​\n",
    " p(s \n",
    "′\n",
    " ∣s,a)v(s \n",
    "′\n",
    " )]∀s∈S\n",
    "含义：最优状态价值等于在最优策略下，所有可能动作的动作价值的加权最大值。\n",
    "其中\n",
    "q(s,a)=∑ \n",
    "r\n",
    "​\n",
    " p(r∣s,a)r+γ∑ \n",
    "s \n",
    "′\n",
    " \n",
    "​\n",
    " p(s \n",
    "′\n",
    " ∣s,a)v(s \n",
    "′\n",
    " )\n",
    "为动作价值。\n",
    "### 3.2 矩阵 - 向量形式（Matrix-Vector Form）\n",
    "v=max \n",
    "π\n",
    "​\n",
    " (r \n",
    "π\n",
    "​\n",
    " +γP \n",
    "π\n",
    "​\n",
    " v)\n",
    "r \n",
    "π\n",
    "​\n",
    " \n",
    "：策略\n",
    "π\n",
    "下的即时奖励向量。\n",
    "P \n",
    "π\n",
    "​\n",
    " \n",
    "：策略\n",
    "π\n",
    "下的状态转移概率矩阵。\n",
    "max \n",
    "π\n",
    "​\n",
    " \n",
    "：对每个状态独立求解最优策略，取最大值。\n",
    "## 4. 求解方法\n",
    "### 4.1 理论基础：压缩映射定理（Contraction Mapping Theorem）\n",
    "核心结论：贝尔曼最优公式中的映射\n",
    "f(v)=max \n",
    "π\n",
    "​\n",
    " (r \n",
    "π\n",
    "​\n",
    " +γP \n",
    "π\n",
    "​\n",
    " v)\n",
    "是压缩映射，因此：\n",
    "存在唯一解\n",
    "v \n",
    "∗\n",
    " \n",
    "（最优状态价值）。\n",
    "可通过迭代算法收敛到\n",
    "v \n",
    "∗\n",
    " \n",
    "。\n",
    "### 4.2 迭代求解（价值迭代算法）\n",
    "v \n",
    "k+1\n",
    "​\n",
    " =max \n",
    "π\n",
    "​\n",
    " (r \n",
    "π\n",
    "​\n",
    " +γP \n",
    "π\n",
    "​\n",
    " v \n",
    "k\n",
    "​\n",
    " )\n",
    "初始值\n",
    "v \n",
    "0\n",
    "​\n",
    " \n",
    "可任意设定（如全 0）。\n",
    "迭代至\n",
    "v \n",
    "k\n",
    "​\n",
    " \n",
    "收敛，得到\n",
    "v \n",
    "∗\n",
    " \n",
    "。\n",
    "## 5. 最优策略的性质\n",
    "确定性与贪婪性：最优策略为确定性策略，即\n",
    "π \n",
    "∗\n",
    " (a∣s)=1\n",
    "（选择动作价值最大的动作\n",
    "a \n",
    "∗\n",
    " \n",
    "），其余动作概率为 0。\n",
    "a \n",
    "∗\n",
    " (s)=argmax \n",
    "a\n",
    "​\n",
    " q(s,a)\n",
    "影响因素：\n",
    "系统模型：状态转移概率\n",
    "p(s \n",
    "′\n",
    " ∣s,a)\n",
    "和奖励概率\n",
    "p(r∣s,a)\n",
    "。\n",
    "奖励设计：即时奖励\n",
    "r\n",
    "的相对值（绝对值不影响策略）。\n",
    "折扣因子\n",
    "γ\n",
    "：\n",
    "γ\n",
    "越大，智能体越 “远视”（重视未来奖励）。\n",
    "不变性：奖励的仿射变换（\n",
    "r→ar+b\n",
    "，\n",
    "a>0\n",
    "）不改变最优策略，仅缩放状态价值。"
   ],
   "id": "cc3cdd26fbbadfe0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
