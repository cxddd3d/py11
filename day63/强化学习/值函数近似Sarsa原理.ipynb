{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 值函数近似 Sarsa 算法原理\n",
    "## 一、算法概述\n",
    "值函数近似 Sarsa 是同策略（on-policy）时序差分（TD）算法与参数化函数近似结合的强化学习方法。与表格型 Sarsa 相比，它通过参数化函数替代传统表格存储动作值，解决了表格方法在大规模或连续状态 / 动作空间中的存储瓶颈和泛化能力不足问题\n",
    "其核心特性在于：\n",
    "行为策略（生成经验的策略）与目标策略（待优化的策略）保持一致（通常为 ε- 贪婪策略）\n",
    "每步交互后立即更新参数，无需等待回合结束\n",
    "通过函数近似实现对未访问状态 - 动作对的价值估计\n",
    "## 二、核心原理\n",
    "1. 动作值函数的近似表示\n",
    "值函数近似 Sarsa 使用参数化函数表示动作值函数：\n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)≈q \n",
    "π\n",
    "​\n",
    " (s,a)\n",
    "其中：\n",
    "s\n",
    "为当前状态，\n",
    "a\n",
    "为当前动作\n",
    "w\n",
    "为可学习的参数向量\n",
    "q\n",
    "^\n",
    "​\n",
    " \n",
    "为近似动作值函数，可选择线性函数（如\n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)=ϕ(s,a) \n",
    "T\n",
    " w\n",
    "，\n",
    "ϕ\n",
    "为特征向量）或神经网络等非线性函数\n",
    "2. 参数更新公式\n",
    "算法通过时序差分误差更新参数\n",
    "w\n",
    "，核心公式为：\n",
    "w \n",
    "t+1\n",
    "​\n",
    " =w \n",
    "t\n",
    "​\n",
    " +α \n",
    "t\n",
    "​\n",
    " [r \n",
    "t+1\n",
    "​\n",
    " +γ \n",
    "q\n",
    "^\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "t+1\n",
    "​\n",
    " ,w \n",
    "t\n",
    "​\n",
    " )− \n",
    "q\n",
    "^\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " ,w \n",
    "t\n",
    "​\n",
    " )]∇ \n",
    "w\n",
    "​\n",
    "  \n",
    "q\n",
    "^\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " ,w \n",
    "t\n",
    "​\n",
    " )\n",
    "各参数含义：\n",
    "α \n",
    "t\n",
    "​\n",
    " \n",
    "：学习率，控制更新幅度\n",
    "γ\n",
    "：折扣因子，平衡即时奖励与未来奖励\n",
    "r \n",
    "t+1\n",
    "​\n",
    " \n",
    "：即时奖励，\n",
    "s \n",
    "t+1\n",
    "​\n",
    " \n",
    "：下一状态，\n",
    "a \n",
    "t+1\n",
    "​\n",
    " \n",
    "：下一动作（体现同策略特性）\n",
    "∇ \n",
    "w\n",
    "​\n",
    "  \n",
    "q\n",
    "^\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " ,w \n",
    "t\n",
    "​\n",
    " )\n",
    "：近似函数关于参数\n",
    "w\n",
    "的梯度\n",
    "括号内部分为TD 误差，衡量当前估计与目标的差距\n",
    "3. 策略评估与改进\n",
    "策略评估：通过上述更新公式优化参数\n",
    "w\n",
    "，使\n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)\n",
    "逼近当前策略\n",
    "π\n",
    "的动作值函数\n",
    "策略改进：基于当前\n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)\n",
    "采用 ε- 贪婪策略更新行为策略：\n",
    "π(a∣s)={ \n",
    "1−ε+ε/∣A(s)∣\n",
    "ε/∣A(s)∣\n",
    "​\n",
    "  \n",
    "if a=argmax \n",
    "a \n",
    "′\n",
    " \n",
    "​\n",
    "  \n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a \n",
    "′\n",
    " ,w)\n",
    "otherwise\n",
    "​\n",
    " \n",
    "\n",
    "确保行为策略与目标策略一致\n",
    "## 三、算法步骤\n",
    "初始化参数向量\n",
    "w\n",
    "（线性模型或神经网络参数），设置行为策略\n",
    "π\n",
    "为 ε- 贪婪策略（基于当前\n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)\n",
    "）\n",
    "对于每个回合（episode）：\n",
    "初始化状态\n",
    "s\n",
    "根据策略\n",
    "π\n",
    "选择初始动作\n",
    "a\n",
    "当\n",
    "s\n",
    "不是终止状态时：\n",
    "执行动作\n",
    "a\n",
    "，获得奖励\n",
    "r\n",
    "和下一状态\n",
    "s \n",
    "′\n",
    " \n",
    "根据策略\n",
    "π\n",
    "选择下一动作\n",
    "a \n",
    "′\n",
    " \n",
    "（同策略关键步骤）\n",
    "计算 TD 误差：\n",
    "δ=r+γ⋅ \n",
    "q\n",
    "^\n",
    "​\n",
    " (s \n",
    "′\n",
    " ,a \n",
    "′\n",
    " ,w)− \n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)\n",
    "计算梯度：\n",
    "∇ \n",
    "w\n",
    "​\n",
    " =∇ \n",
    "w\n",
    "​\n",
    "  \n",
    "q\n",
    "^\n",
    "​\n",
    " (s,a,w)\n",
    "更新参数：\n",
    "w=w+α⋅δ⋅∇ \n",
    "w\n",
    "​\n",
    " \n",
    "转移到下一状态和动作：\n",
    "s=s \n",
    "′\n",
    " \n",
    "，\n",
    "a=a \n",
    "′\n",
    " \n",
    "\n",
    "## 四、关键特性分析\n",
    "1. 与表格型 Sarsa 的区别\n",
    "存储效率：值函数近似通过参数化函数（如线性模型、神经网络）替代表格，大幅减少存储需求，尤其适用于高维状态空间\n",
    "泛化能力：通过函数拟合，未访问过的状态 - 动作对可通过相似状态的信息进行估计，解决表格方法的稀疏性问题\n",
    "更新方式：表格方法直接更新单个状态 - 动作对的值，而值函数近似通过更新参数影响所有相关状态 - 动作对的估计\n",
    "2. 函数近似器的选择\n",
    "线性函数：结构简单，理论性质清晰，表格型 Sarsa 可视为其特殊情况（使用 one-hot 特征）\n",
    "神经网络：可拟合复杂非线性关系，但训练难度更高，需注意梯度稳定性\n",
    "3. 同策略特性的影响\n",
    "优势：学习过程与实际执行的策略紧密结合，更适合需要在线学习的场景\n",
    "挑战：探索与利用的平衡更复杂，过度探索可能导致策略不稳定，需谨慎设置 ε- 贪婪参数\n",
    "五、总结\n",
    "值函数近似 Sarsa 通过参数化函数扩展了传统 Sarsa 的适用范围，使其能处理大规模甚至连续的状态 / 动作空间。其核心是通过时序差分误差更新函数参数，同时保持同策略特性，确保学习到的策略与实际执行策略一致。该算法为复杂强化学习问题提供了有效的解决方案，是连接传统强化学习与深度强化学习的重要桥梁"
   ],
   "id": "3d27b8be132d4601"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
