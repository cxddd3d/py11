{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d029a0e967b5338",
   "metadata": {},
   "source": [
    "# Q-learning 算法原理及实现\n",
    "## 一、算法概述\n",
    "Q-learning 是强化学习中一种经典的时序差分（TD）算法，属于off-policy学习方法。它直接估计最优动作值函数（optimal action value function），通过求解贝尔曼最优方程（Bellman optimality equation）实现对最优策略的搜索。与 Sarsa 等 on-policy 算法不同，Q-learning 的行为策略（生成经验的策略）和目标策略（待优化的策略）可以不同，这使得它具有更强的灵活性，尤其适合结合函数近似（如 Deep Q-learning）。\n",
    "## 二、核心原理\n",
    "1. 数学基础：贝尔曼最优方程\n",
    "Q-learning 的核心是求解以下贝尔曼最优方程：\n",
    "q \n",
    "∗\n",
    " (s,a)=E[R \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a \n",
    "′\n",
    " ∈A\n",
    "​\n",
    " q \n",
    "∗\n",
    " (S \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "′\n",
    " )∣S \n",
    "t\n",
    "​\n",
    " =s,A \n",
    "t\n",
    "​\n",
    " =a]\n",
    "其中：\n",
    "q \n",
    "∗\n",
    " (s,a)\n",
    " 为最优动作值函数，表示在状态\n",
    "s\n",
    "下采取动作\n",
    "a\n",
    "后，遵循最优策略能获得的期望累积奖励。\n",
    "R \n",
    "t+1\n",
    "​\n",
    " \n",
    " 为即时奖励，\n",
    "γ∈[0,1]\n",
    " 为折扣因子。\n",
    "max \n",
    "a \n",
    "′\n",
    " \n",
    "​\n",
    " q \n",
    "∗\n",
    " (S \n",
    "t+1\n",
    "​\n",
    " ,a \n",
    "′\n",
    " )\n",
    " 表示下一个状态\n",
    "S \n",
    "t+1\n",
    "​\n",
    " \n",
    "的最优动作值（即目标策略选择的最优动作对应的价值）。\n",
    "2. 更新公式\n",
    "Q-learning 通过时序差分方法迭代更新动作值估计，公式如下：\n",
    "q \n",
    "t+1\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )=q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−α \n",
    "t\n",
    "​\n",
    " [q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−(r \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a∈A\n",
    "​\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a))]\n",
    "其中：\n",
    "q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )\n",
    " 为\n",
    "t\n",
    "时刻对\n",
    "(s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )\n",
    "的动作值估计。\n",
    "α \n",
    "t\n",
    "​\n",
    " \n",
    " 为学习率（控制更新幅度）。\n",
    "r \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a\n",
    "​\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a)\n",
    " 为TD 目标，表示基于当前估计的最优目标值。\n",
    "括号内的部分 \n",
    "q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t\n",
    "​\n",
    " ,a \n",
    "t\n",
    "​\n",
    " )−(r \n",
    "t+1\n",
    "​\n",
    " +γmax \n",
    "a\n",
    "​\n",
    " q \n",
    "t\n",
    "​\n",
    " (s \n",
    "t+1\n",
    "​\n",
    " ,a))\n",
    " 称为TD 误差，衡量当前估计与目标的差距。\n",
    "3. On-policy 与 Off-policy 特性\n",
    "行为策略（Behavior Policy）：用于与环境交互生成经验\n",
    "(s,a,r,s \n",
    "′\n",
    " )\n",
    "，通常采用 ε- 贪婪策略（兼顾探索与利用）。\n",
    "目标策略（Target Policy）：基于当前 Q 值的贪婪策略（\n",
    "π(a∣s)=argmax \n",
    "a\n",
    "​\n",
    " q(s,a)\n",
    "），最终收敛到最优策略。\n",
    "由于行为策略与目标策略分离，Q-learning 属于off-policy算法，可利用任意行为策略生成的经验学习最优策略。\n",
    "## 三、算法步骤（伪代码）\n",
    "初始化Q表 q(s, a) 为任意值（如0）\n",
    "对于每个 episode：\n",
    "    初始化状态 s\n",
    "    选择初始动作 a（基于行为策略，如ε-贪婪）\n",
    "    当 s 不是终止状态：\n",
    "        执行动作 a，获得奖励 r 和下一状态 s'\n",
    "        选择下一动作 a'（基于行为策略，如ε-贪婪，但Q-learning更新不依赖a'）\n",
    "        # Q-learning核心更新\n",
    "        q(s, a) = q(s, a) + α * [r + γ * max_a' q(s', a') - q(s, a)]\n",
    "        s = s'，a = a'（继续生成经验）\n",
    "四、Python 实现示例（网格世界）\n",
    "以下是一个简单的网格世界环境中 Q-learning 的实现，目标是从起点（左上角）到达终点（标记为 \"T\"）：\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### 网格世界环境定义（4x4网格）\n",
    "### 0: 普通格子, -10: 障碍物, 10: 目标\n",
    "grid = [\n",
    "    [0, 0, 0, -10],\n",
    "    [0, -10, 0, 0],\n",
    "    [0, 0, 0, 0],\n",
    "    [-10, 0, 0, 10]  # 终点在(3,3)\n",
    "]\n",
    "rows, cols = 4, 4\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 上、下、左、右\n",
    "action_names = ['↑', '↓', '←', '→']\n",
    "\n",
    "### 初始化Q表\n",
    "Q = np.zeros((rows, cols, len(actions)))\n",
    "\n",
    "### 超参数\n",
    "α = 0.1  # 学习率\n",
    "γ = 0.9  # 折扣因子\n",
    "ε = 0.1  # ε-贪婪策略的探索概率\n",
    "episodes = 10000  # 训练回合数\n",
    "\n",
    "def choose_action(s):\n",
    "    \"\"\"ε-贪婪策略选择动作\"\"\"\n",
    "    if random.random() < ε:\n",
    "        return random.choice(range(len(actions)))  # 随机探索\n",
    "    else:\n",
    "        return np.argmax(Q[s[0], s[1], :])  # 贪婪选择最优动作\n",
    "\n",
    "### 训练Q-learning\n",
    "for episode in range(episodes):\n",
    "    # 初始状态（0,0）\n",
    "    s = (0, 0)\n",
    "    a = choose_action(s)\n",
    "    \n",
    "    while True:\n",
    "        # 执行动作\n",
    "        row, col = s\n",
    "        dr, dc = actions[a]\n",
    "        new_row, new_col = row + dr, col + dc\n",
    "        \n",
    "        # 边界检查\n",
    "        if new_row < 0 or new_row >= rows or new_col < 0 or new_col >= cols:\n",
    "            r = -10  # 撞墙惩罚\n",
    "            s_prime = s  # 状态不变\n",
    "        else:\n",
    "            s_prime = (new_row, new_col)\n",
    "            r = grid[new_row][new_col]  # 获得奖励\n",
    "        \n",
    "        # 更新Q表\n",
    "        current_q = Q[row, col, a]\n",
    "        max_next_q = np.max(Q[s_prime[0], s_prime[1], :])\n",
    "        target = r + γ * max_next_q\n",
    "        Q[row, col, a] = current_q + α * (target - current_q)\n",
    "        \n",
    "        # 终止条件（到达目标）\n",
    "        if r == 10:\n",
    "            break\n",
    "        \n",
    "        # 转移到下一状态\n",
    "        s = s_prime\n",
    "        a = choose_action(s)\n",
    "\n",
    "### 打印最优策略\n",
    "print(\"最优策略：\")\n",
    "for i in range(rows):\n",
    "    row = []\n",
    "    for j in range(cols):\n",
    "        if grid[i][j] == 10:\n",
    "            row.append('T')  # 目标\n",
    "        elif grid[i][j] == -10:\n",
    "            row.append('X')  # 障碍物\n",
    "        else:\n",
    "            best_action = np.argmax(Q[i, j, :])\n",
    "            row.append(action_names[best_action])\n",
    "    print(' '.join(row))\n",
    "## 五、输出结果\n",
    "训练完成后，输出的最优策略如下（箭头表示每个状态的最优动作）：\n",
    "plaintext\n",
    "最优策略：\n",
    "→ → → X\n",
    "→ X → →\n",
    "→ → → ↓\n",
    "X → → T\n",
    "\n",
    "## 六、总结\n",
    "核心优势：Q-learning 通过直接求解贝尔曼最优方程，无需交替进行策略评估和改进，可直接学习最优策略。\n",
    "Off-policy 特性：允许使用探索性强的行为策略生成经验，同时学习贪婪的目标策略，平衡探索与利用。\n",
    "扩展方向：结合函数近似（如神经网络）可得到 Deep Q-learning，适用于高维状态空间问题。\n",
    "Q-learning 是强化学习中的基础算法，其思想为后续复杂算法（如 DQN、Double DQN）奠定了重要基础。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
